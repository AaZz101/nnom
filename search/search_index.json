{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Neural Network on Microcontroller (NNoM) NNoM is a high-level inference Neural Network library specifically for microcontrollers. Document version 0.2.1 [Chinese Intro] Highlights Deploy Keras model to NNoM model with one line of code. User-friendly interfaces. Support complex structures; Inception, ResNet, DenseNet... High-performance backend selections. Onboard (MCU) evaluation tools; Runtime analysis, Top-k, Confusion matrix... The structure of NNoM is shown below: More detail avaialble in Development Guide Licenses NNoM is released under Apache License 2.0 since nnom-V0.2.0. License and copyright information can be found within the code. Why NNoM? The aims of NNoM is to provide a light-weight, user-friendly and flexible interface for fast deploying. Nowadays, neural networks are wider , deeper , and denser . [1] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9). [2] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). [3] Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708). After 2014, the development of Neural Networks are more focus on structure optimising to improve efficiency and performance, which is more important to the small footprint platforms such as MCUs. However, the available NN libs for MCU are too low-level which make it sooooo difficult to use with these complex strucures. Therefore, we build NNoM to help embedded developers for faster and simpler deploying NN model directly to MCU. NNoM will manage the strucutre, memory and everything else for developer. All you need is feeding your measurements then get the results. NNoM is now working closely with Keras (You can easily learn Keras in 30 seconds!). There is no need to learn TensorFlow/Lite or other libs. Documentations API manuals are available within this site. Guides 5 min to NNoM Guide Development Guide The temporary guide Porting and optimising Guide RT-Thread Guide(Chinese) RT-Thread-MNIST example (Chinese) Examples Documented examples Please check examples and choose one to start with. Most recent Examples: MNIST-DenseNet example Octave Convolution Keyword Spotting Dependencies NNoM now use the local pure C backend implementation by default. Thus, there is no special dependency needed. Optimization You can select CMSIS-NN/DSP as the backend for about 5x performance with ARM-Cortex-M4/7/33/35P. Check Porting and optimising Guide for detail. Available Operations *Notes: NNoM now supports both HWC and CHW formats. Some operation might not support both format currently. Please check the tables for the current status. * Core Layers Layers HWC CHW Layer API Comments Convolution \u2713 \u2713 Conv2D() Support 1/2D Depthwise Conv \u2713 \u2713 DW_Conv2D() Support 1/2D Fully-connected \u2713 \u2713 Dense() Lambda \u2713 \u2713 Lambda() single input / single output anonymous operation Batch Normalization \u2713 \u2713 N/A This layer is merged to the last Conv by the script Flatten \u2713 \u2713 Flatten() SoftMax \u2713 \u2713 SoftMax() Softmax only has layer API Activation \u2713 \u2713 Activation() A layer instance for activation Input/Output \u2713 \u2713 Input()/Output() Up Sampling \u2713 \u2713 UpSample() Zero Padding \u2713 \u2713 ZeroPadding() Cropping \u2713 \u2713 Cropping() RNN Layers Layers Status Layer API Comments Recurrent NN Under Dev. RNN() Under Developpment Simple RNN Under Dev. SimpleCell() Under Developpment Gated Recurrent Network (GRU) Under Dev. GRUCell() Under Developpment Activations Activation can be used by itself as layer, or can be attached to the previous layer as \"actail\" to reduce memory cost. Actrivation HWC CHW Layer API Activation API Comments ReLU \u2713 \u2713 ReLU() act_relu() TanH \u2713 \u2713 TanH() act_tanh() Sigmoid \u2713 \u2713 Sigmoid() act_sigmoid() Pooling Layers Pooling HWC CHW Layer API Comments Max Pooling \u2713 \u2713 MaxPool() Average Pooling \u2713 \u2713 AvgPool() Sum Pooling \u2713 \u2713 SumPool() Global Max Pooling \u2713 \u2713 GlobalMaxPool() Global Average Pooling \u2713 \u2713 GlobalAvgPool() Global Sum Pooling \u2713 \u2713 GlobalSumPool() A better alternative to Global average pooling in MCU before Softmax Matrix Operations Layers Matrix HWC CHW Layer API Comments Concatenate \u2713 \u2713 Concat() Concatenate through any axis Multiple \u2713 \u2713 Mult() Addition \u2713 \u2713 Add() Substraction \u2713 \u2713 Sub()","title":"Overview"},{"location":"#neural-network-on-microcontroller-nnom","text":"NNoM is a high-level inference Neural Network library specifically for microcontrollers. Document version 0.2.1 [Chinese Intro] Highlights Deploy Keras model to NNoM model with one line of code. User-friendly interfaces. Support complex structures; Inception, ResNet, DenseNet... High-performance backend selections. Onboard (MCU) evaluation tools; Runtime analysis, Top-k, Confusion matrix... The structure of NNoM is shown below: More detail avaialble in Development Guide","title":"Neural Network on Microcontroller (NNoM)"},{"location":"#licenses","text":"NNoM is released under Apache License 2.0 since nnom-V0.2.0. License and copyright information can be found within the code.","title":"Licenses"},{"location":"#why-nnom","text":"The aims of NNoM is to provide a light-weight, user-friendly and flexible interface for fast deploying. Nowadays, neural networks are wider , deeper , and denser . [1] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9). [2] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). [3] Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708). After 2014, the development of Neural Networks are more focus on structure optimising to improve efficiency and performance, which is more important to the small footprint platforms such as MCUs. However, the available NN libs for MCU are too low-level which make it sooooo difficult to use with these complex strucures. Therefore, we build NNoM to help embedded developers for faster and simpler deploying NN model directly to MCU. NNoM will manage the strucutre, memory and everything else for developer. All you need is feeding your measurements then get the results. NNoM is now working closely with Keras (You can easily learn Keras in 30 seconds!). There is no need to learn TensorFlow/Lite or other libs.","title":"Why NNoM?"},{"location":"#documentations","text":"API manuals are available within this site. Guides 5 min to NNoM Guide Development Guide The temporary guide Porting and optimising Guide RT-Thread Guide(Chinese) RT-Thread-MNIST example (Chinese)","title":"Documentations"},{"location":"#examples","text":"Documented examples Please check examples and choose one to start with. Most recent Examples: MNIST-DenseNet example Octave Convolution Keyword Spotting","title":"Examples"},{"location":"#dependencies","text":"NNoM now use the local pure C backend implementation by default. Thus, there is no special dependency needed.","title":"Dependencies"},{"location":"#optimization","text":"You can select CMSIS-NN/DSP as the backend for about 5x performance with ARM-Cortex-M4/7/33/35P. Check Porting and optimising Guide for detail.","title":"Optimization"},{"location":"#available-operations","text":"*Notes: NNoM now supports both HWC and CHW formats. Some operation might not support both format currently. Please check the tables for the current status. * Core Layers Layers HWC CHW Layer API Comments Convolution \u2713 \u2713 Conv2D() Support 1/2D Depthwise Conv \u2713 \u2713 DW_Conv2D() Support 1/2D Fully-connected \u2713 \u2713 Dense() Lambda \u2713 \u2713 Lambda() single input / single output anonymous operation Batch Normalization \u2713 \u2713 N/A This layer is merged to the last Conv by the script Flatten \u2713 \u2713 Flatten() SoftMax \u2713 \u2713 SoftMax() Softmax only has layer API Activation \u2713 \u2713 Activation() A layer instance for activation Input/Output \u2713 \u2713 Input()/Output() Up Sampling \u2713 \u2713 UpSample() Zero Padding \u2713 \u2713 ZeroPadding() Cropping \u2713 \u2713 Cropping() RNN Layers Layers Status Layer API Comments Recurrent NN Under Dev. RNN() Under Developpment Simple RNN Under Dev. SimpleCell() Under Developpment Gated Recurrent Network (GRU) Under Dev. GRUCell() Under Developpment Activations Activation can be used by itself as layer, or can be attached to the previous layer as \"actail\" to reduce memory cost. Actrivation HWC CHW Layer API Activation API Comments ReLU \u2713 \u2713 ReLU() act_relu() TanH \u2713 \u2713 TanH() act_tanh() Sigmoid \u2713 \u2713 Sigmoid() act_sigmoid() Pooling Layers Pooling HWC CHW Layer API Comments Max Pooling \u2713 \u2713 MaxPool() Average Pooling \u2713 \u2713 AvgPool() Sum Pooling \u2713 \u2713 SumPool() Global Max Pooling \u2713 \u2713 GlobalMaxPool() Global Average Pooling \u2713 \u2713 GlobalAvgPool() Global Sum Pooling \u2713 \u2713 GlobalSumPool() A better alternative to Global average pooling in MCU before Softmax Matrix Operations Layers Matrix HWC CHW Layer API Comments Concatenate \u2713 \u2713 Concat() Concatenate through any axis Multiple \u2713 \u2713 Mult() Addition \u2713 \u2713 Add() Substraction \u2713 \u2713 Sub()","title":"Available Operations"},{"location":"A_Temporary_Guide_to_NNoM/","text":"The simplest first. Deploying Deploying is much easier than before. (Thanks to @parai) Simply use generate_model(model, x_data) to generate a C header weights.h after you have trained your model in Keras. It is available in nnom_utils.py Include the weights.h in your project, then call nnom_model_create() to create and compile the model on the MCU. Finaly, call model_run() to do your prediction. Please check MNIST-DenseNet example for usage The generate_model(model, x_data) might not be updated with NNoM from time to time. For new features and customized layers, you can still use NNoM APIs to build your model. NNoM Structure NNoM uses a layer-based structure. A layer is a container. Every operation (convolution, concat...) must be wrapped into a layer. A basic layer contains a list of Input/Ouput modules (I/O). Each of I/O contains a list of Hook (similar to Nodes in Keras). Hook stores the links to an I/O (other layer's) I/O is a buffer to store input/output data of the operation. Dont be scared, check this: Those APIs listed below will help you to create layers and build the model structures. APIs Layer APIs and Construction APIs are used to build a model. Layer APIs can create and return a new layer instance, while construction APIs use layer instances to build a model. Layer APIs such as Conv2D(), Dense(), Activation() ... which you can find in nnom_layers.h Construction APIs such as model.hook(), model.merge(), model.add() ... which you can find in new_model() at nnom.c For example, to add a convolution layer into a sequencial model, use model.add() : model.add(&model, Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b)); In functional model, the links between layer are specified explicitly by using model.hook() or model.merge() x = model.hook(Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b), input_layer); x = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); NNoM currently supports HWC format. Which also called \"channel last\", where H = number of rows or y axis, W = number of column or x axis, C = number of channes. For example: In the above codes, both kernal(H, W) and stride(H, W) returns a 'shape' instance. The shape instance in format of (H, W, ?) All convolutional layers and poolings layers support both 1D / 2D data input. However, when using 1D input, the H must be set to 1. Construction APIs Construction APIs are statics functions located in nnom.c Currently are: Sequencial Construction API nnom_status_t model.add(nnom_model_t* model, nnom_layer_t *layer); Functional Construction API // hook the current layer to the input layer // this function only to connect (single output layer) to (single input layer). // return the curr (layer) instance nnom_layer_t * model.hook(nnom_layer_t* curr, nnom_layer_t *last) // merge 2 layer's output to one output by provided merging method(a mutiple input layer) // method = merging layer such as (concat(), dot(), mult(), add()) // return the method (layer) instance nnom_layer_t * model.merge(nnom_layer_t *method, nnom_layer_t *in1, nnom_layer_t *in2) // Same as model.merge() // Except it can take mutiple layers as input. // num = the number of layer // method: same as model.merge() nnom_layer_t * model.mergex(nnom_layer_t *method, int num, ...) // This api will merge the activation to the targeted layerto reduce an extra activation layer // activation such as (act_relu(), act_tanh()...) nnom_layer_t * model.active(nnom_activation_t* act, nnom_layer_t * target) For model.active() , please check Activation APIs below. Layer APIs Layers APIs are listed in nnom_layers.h Input/output layers are neccessary for a model. They are responsible to copy data from user's input buffer, and copy out to user's output buffer. // Layer APIs // input/output nnom_layer_t* Input(nnom_3d_shape_t input_shape\uff0cvoid* p_buf); nnom_layer_t* Output(nnom_3d_shape_t output_shape, void* p_buf); Pooling as they are: The sum pooling here will dynamicly change its ourput shift to avoid overflowing. It is recommened to replace the Global Average Pooling by Global Sum Pooling for better accuracy in MCU side. // Pooling, kernel, strides, padding nnom_layer_t* MaxPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); nnom_layer_t* AvgPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); nnom_layer_t* SumPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); // The Global poolings simplly do better nnom_layer_t *GlobalMaxPool(void); nnom_layer_t *GlobalAvgPool(void); nnom_layer_t *GlobalSumPool(void); // Upsampling layer / Unpooling layer nnom_layer_t* UpSample(nnom_3d_shape_t kernel); Activation's Layers API are started with capital letter. They are differed from the Activation API , which start with act_* and retrun an activation instance. Pleas check the Activation APIs below for more detail. They return a layer instance. // Activation layers take activation instance as input. nnom_layer_t* Activation(nnom_activation_t *act); // Activation's layer API. nnom_layer_t* ReLU(void); nnom_layer_t* Softmax(void); nnom_layer_t* Sigmoid(void); nnom_layer_t* TanH(void); Matrix API. These layers normally take 2 or more layer's output as their inputs. They also called \"merging method\", which must be used by model.merge(method, in1, in2) or model.mergex(method, num of input, in1, in2, 1n3 ...) // Matrix nnom_layer_t* Add(void); nnom_layer_t* Sub(void); nnom_layer_t* Mult(void); nnom_layer_t* Concat(int8_t axis); Flatten change the shapes to (x, 1, 1) // utils nnom_layer_t* Flatten(void); Stable NN layers. For more developing layers, please check the source codes. // conv1D/2d nnom_layer_t* Conv2D(uint32_t filters, nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); // depthwise_convolution 1D/2D nnom_layer_t* DW_Conv2D(uint32_t multiplier, nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); // fully connected, dense nnom_layer_t* Dense(size_t output_unit, nnom_weight_t *w, nnom_bias_t *b); // Lambda Layers // layer.run() , required // layer.oshape(), optional, call default_output_shape() if left NULL // layer.free() , optional, called while model is deleting, to free private resources // parameters , private parameters for run method, left NULL if not needed. nnom_layer_t *Lambda(nnom_status_t (*run)(nnom_layer_t *), nnom_status_t (*oshape)(nnom_layer_t *), nnom_status_t (*free)(nnom_layer_t *), void *parameters); About the missing Batch Normalization Layer Batch Normalization layer can be fused into the last convolution layer. So NNoM currently does not provide a Batch Normalization Layer. It might be implemented as a single layer in the future. However, currently, please fused it to the last layer. Further reading about fusing BN parameters to conv weights Fusing batch-norm layers Addictionlly, Activation APIs Actication APIs are not essential in the original idea. The original idea is making eveything as a layer. However, single layer instances cost huge amount of memories(100~150 Bytes), while activations are relativly simple, mostly have same input/output shape, a few/none parameter(s)... Therefore, to reduce the complexity, the \"actail\"(activation tail) is added to each layer instance. If a layer's Actail is not null, it will be called right after the layer is executed. Actail takes activation instance as input. The model API, model.active() will attach the activation to the layer's actail. // attach act to target_layer, return the target layer instance. nnom_layer_t * model.active(nnom_activation_t* act, nnom_layer_t * target_layer) The Activation APIs are listed in nnom_activations.h // Activation nnom_activation_t* act_relu(void); nnom_activation_t* act_sigmoid(void); nnom_activation_t* act_tanh(void); Model API A model instance contains the starting layer, the end layer and other neccessary info. Please refer to the examples for usage // Create or initial a new model() nnom_model_t* new_model(nnom_model_t* m); // Delete the model completely (new). void model_delete(nnom_model_t* m); // Compile a sequencial model. nnom_status_t sequencial_compile(nnom_model_t *m); // Compile a functional model with specified input layer and output layer. // if output = NULL, the output is automatic selected. nnom_status_t model_compile(nnom_model_t *m, nnom_layer_t* input, nnom_layer_t* output); // Run the model. nnom_status_t model_run(nnom_model_t *m); Known Issues Shared output buffer destroyed by single buffer layers (input-destructive) Single buffer layers (Such as most of the Activations, additionally MaxPool/AvgPool) are working directly on its input buffer. While its input buffer is shared with other parallel layers, and it is placed before other layers in a parallel structure (such as Inception), the shared buffer will be destroyed by those input-destructive before other parallel layer can access it. Additionally, although, MaxPool & AvgPool are not single buffer layers, they will destroy the input buffer as they are mentioned with input-destructive layers in CMSIS-NN. So they should be treated as same as single buffer layers. Fix plan of the issue Not planned. Possiblly, add an invisible copying layer/functions to copy data for single input layer before passing to other parallel layers. Current work around Work around 1 If the Inception has only one single buffer layer, always hook the single buffer layer at the end. For example, instead of doing MaxPool - Conv2D - Conv2D , do Conv2D - Conv2D - MaxPool // the codes are faked and simplified, please rewrite them according to corresponding APIs. // original x1 = model.hook(MaxPool(), input); // Single buffer layer, this will destroyed the buffer x2 = model.hook(Conv2D(), input); // buffer destroyed. x3 = model.hook(Conv2D(), input); // buffer destroyed. output = model.mergex(Concat(-1), 3, x1, x2, x3); // This will fixed the problem without affacting the concatenate order. // notice that the order of x1,x2,x3 will change, // the different is the order that the inception layers hooked to the input layer. x3 = model.hook(Conv2D(), input); // multiple buffers layer x2 = model.hook(Conv2D(), input); // x1 = model.hook(MaxPool(), input); // this will destroyed the buffer, but it doesnt matter now. output = model.mergex(Concat(-1), 3, x1, x2, x3); Work around 2 If there is multiple, add an extra multiple bufer layer before the single buffer layer. Such as using Lambda() layer to copy buffer. // the codes are faked and simplified, please rewrite them according to corresponding APIs. lambda_run(layer) { memcpy(layer->output, layer->input, sizeof(inputshape); } x1 = model.hook(Lambda(lambda_run), input); // add a lambda to copy data x1 = model.hook(MaxPool(), x1); // now it is only destroying Lambda's output buffer instead of the input layer's. x2 = model.hook(Lambda(lambda_run), input); // add a lambda to copy data x2 = model.hook(MaxPool(), x2); x3 = model.hook(Conv2D(), input); output = model.mergex(Concat(-1), 3, x1, x2, x3); Evaluation The evaluation methods are listed in nnom_utils.h They run the model with testing data, then evaluate the model. Includes Top-k accuracy, confusion matrix, runtime stat... Please refer to UCI HAR example for usage. // create a prediction // input model, the buf pointer to the softwmax output (Temporary, this can be extract from model) // the size of softmax output (the num of lable) // the top k that wants to record. nnom_predic_t* prediction_create(nnom_model_t* m, int8_t* buf_prediction, size_t label_num, size_t top_k_size);// currently int8_t // after a new data is set in input // feed data to prediction // input the current label, (range from 0 to total number of label -1) // (the current input data should be set by user manully to the input buffer of the model.) // return NN_ARGUMENT_ERROR if parameter error int32_t prediction_run(nnom_predic_t *pre, uint32_t label); // to mark prediction finished void prediction_end(nnom_predic_t *pre); // free all resources void predicetion_delete(nnom_predic_t *pre); // print matrix void prediction_matrix(nnom_predic_t *pre); // print top-k void prediction_top_k(nnom_predic_t *pre); // this function is to print sumarry void prediction_summary(nnom_predic_t *pre); // ------------------------------- // stand alone prediction API // this api test one set of data, return the prediction // input the model's input and output bufer // return the predicted label // return NN_ARGUMENT_ERROR if parameter error int32_t nnom_predic_one(nnom_model_t *m, int8_t *input, int8_t *output); // currently int8_t // print last runtime stat of the model void model_stat(nnom_model_t *m); Demo of Evaluation The UCI HAR example runs on RT-Thread, uses Y-Modem to receive testing dataset, uses ringbuffer to store data, and the console (msh) to print the results. The layer order, activation, output shape, operation, memory of I/O, and assigned memory block are shown. It also summarised the memory cost by neural network. Type predic , then use Y-Modem to send the data file. The model will run once enough data is received. When the file copying done, the runtime summary, Top-k and confusion matrix will be printed Optionally, the runtime stat detail of each layer can be printed by nn_stat PS: The \"runtime stat\" in the animation is not correct, due to the test chip is overclocking (STM32L476 @ 160MHz, 2x overclocking), and the timer is overclocking as well. However, the numbers in prediction summary are correct, because they are measured by system_tick timer which is not overclocking.","title":"A Temporary Guide"},{"location":"A_Temporary_Guide_to_NNoM/#deploying","text":"Deploying is much easier than before. (Thanks to @parai) Simply use generate_model(model, x_data) to generate a C header weights.h after you have trained your model in Keras. It is available in nnom_utils.py Include the weights.h in your project, then call nnom_model_create() to create and compile the model on the MCU. Finaly, call model_run() to do your prediction. Please check MNIST-DenseNet example for usage The generate_model(model, x_data) might not be updated with NNoM from time to time. For new features and customized layers, you can still use NNoM APIs to build your model.","title":"Deploying"},{"location":"A_Temporary_Guide_to_NNoM/#nnom-structure","text":"NNoM uses a layer-based structure. A layer is a container. Every operation (convolution, concat...) must be wrapped into a layer. A basic layer contains a list of Input/Ouput modules (I/O). Each of I/O contains a list of Hook (similar to Nodes in Keras). Hook stores the links to an I/O (other layer's) I/O is a buffer to store input/output data of the operation. Dont be scared, check this: Those APIs listed below will help you to create layers and build the model structures.","title":"NNoM Structure"},{"location":"A_Temporary_Guide_to_NNoM/#apis","text":"Layer APIs and Construction APIs are used to build a model. Layer APIs can create and return a new layer instance, while construction APIs use layer instances to build a model. Layer APIs such as Conv2D(), Dense(), Activation() ... which you can find in nnom_layers.h Construction APIs such as model.hook(), model.merge(), model.add() ... which you can find in new_model() at nnom.c For example, to add a convolution layer into a sequencial model, use model.add() : model.add(&model, Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b)); In functional model, the links between layer are specified explicitly by using model.hook() or model.merge() x = model.hook(Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b), input_layer); x = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); NNoM currently supports HWC format. Which also called \"channel last\", where H = number of rows or y axis, W = number of column or x axis, C = number of channes. For example: In the above codes, both kernal(H, W) and stride(H, W) returns a 'shape' instance. The shape instance in format of (H, W, ?) All convolutional layers and poolings layers support both 1D / 2D data input. However, when using 1D input, the H must be set to 1.","title":"APIs"},{"location":"A_Temporary_Guide_to_NNoM/#construction-apis","text":"Construction APIs are statics functions located in nnom.c Currently are: Sequencial Construction API nnom_status_t model.add(nnom_model_t* model, nnom_layer_t *layer); Functional Construction API // hook the current layer to the input layer // this function only to connect (single output layer) to (single input layer). // return the curr (layer) instance nnom_layer_t * model.hook(nnom_layer_t* curr, nnom_layer_t *last) // merge 2 layer's output to one output by provided merging method(a mutiple input layer) // method = merging layer such as (concat(), dot(), mult(), add()) // return the method (layer) instance nnom_layer_t * model.merge(nnom_layer_t *method, nnom_layer_t *in1, nnom_layer_t *in2) // Same as model.merge() // Except it can take mutiple layers as input. // num = the number of layer // method: same as model.merge() nnom_layer_t * model.mergex(nnom_layer_t *method, int num, ...) // This api will merge the activation to the targeted layerto reduce an extra activation layer // activation such as (act_relu(), act_tanh()...) nnom_layer_t * model.active(nnom_activation_t* act, nnom_layer_t * target) For model.active() , please check Activation APIs below.","title":"Construction APIs"},{"location":"A_Temporary_Guide_to_NNoM/#layer-apis","text":"Layers APIs are listed in nnom_layers.h Input/output layers are neccessary for a model. They are responsible to copy data from user's input buffer, and copy out to user's output buffer. // Layer APIs // input/output nnom_layer_t* Input(nnom_3d_shape_t input_shape\uff0cvoid* p_buf); nnom_layer_t* Output(nnom_3d_shape_t output_shape, void* p_buf); Pooling as they are: The sum pooling here will dynamicly change its ourput shift to avoid overflowing. It is recommened to replace the Global Average Pooling by Global Sum Pooling for better accuracy in MCU side. // Pooling, kernel, strides, padding nnom_layer_t* MaxPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); nnom_layer_t* AvgPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); nnom_layer_t* SumPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); // The Global poolings simplly do better nnom_layer_t *GlobalMaxPool(void); nnom_layer_t *GlobalAvgPool(void); nnom_layer_t *GlobalSumPool(void); // Upsampling layer / Unpooling layer nnom_layer_t* UpSample(nnom_3d_shape_t kernel); Activation's Layers API are started with capital letter. They are differed from the Activation API , which start with act_* and retrun an activation instance. Pleas check the Activation APIs below for more detail. They return a layer instance. // Activation layers take activation instance as input. nnom_layer_t* Activation(nnom_activation_t *act); // Activation's layer API. nnom_layer_t* ReLU(void); nnom_layer_t* Softmax(void); nnom_layer_t* Sigmoid(void); nnom_layer_t* TanH(void); Matrix API. These layers normally take 2 or more layer's output as their inputs. They also called \"merging method\", which must be used by model.merge(method, in1, in2) or model.mergex(method, num of input, in1, in2, 1n3 ...) // Matrix nnom_layer_t* Add(void); nnom_layer_t* Sub(void); nnom_layer_t* Mult(void); nnom_layer_t* Concat(int8_t axis); Flatten change the shapes to (x, 1, 1) // utils nnom_layer_t* Flatten(void); Stable NN layers. For more developing layers, please check the source codes. // conv1D/2d nnom_layer_t* Conv2D(uint32_t filters, nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); // depthwise_convolution 1D/2D nnom_layer_t* DW_Conv2D(uint32_t multiplier, nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); // fully connected, dense nnom_layer_t* Dense(size_t output_unit, nnom_weight_t *w, nnom_bias_t *b); // Lambda Layers // layer.run() , required // layer.oshape(), optional, call default_output_shape() if left NULL // layer.free() , optional, called while model is deleting, to free private resources // parameters , private parameters for run method, left NULL if not needed. nnom_layer_t *Lambda(nnom_status_t (*run)(nnom_layer_t *), nnom_status_t (*oshape)(nnom_layer_t *), nnom_status_t (*free)(nnom_layer_t *), void *parameters); About the missing Batch Normalization Layer Batch Normalization layer can be fused into the last convolution layer. So NNoM currently does not provide a Batch Normalization Layer. It might be implemented as a single layer in the future. However, currently, please fused it to the last layer. Further reading about fusing BN parameters to conv weights Fusing batch-norm layers","title":"Layer APIs"},{"location":"A_Temporary_Guide_to_NNoM/#addictionlly-activation-apis","text":"Actication APIs are not essential in the original idea. The original idea is making eveything as a layer. However, single layer instances cost huge amount of memories(100~150 Bytes), while activations are relativly simple, mostly have same input/output shape, a few/none parameter(s)... Therefore, to reduce the complexity, the \"actail\"(activation tail) is added to each layer instance. If a layer's Actail is not null, it will be called right after the layer is executed. Actail takes activation instance as input. The model API, model.active() will attach the activation to the layer's actail. // attach act to target_layer, return the target layer instance. nnom_layer_t * model.active(nnom_activation_t* act, nnom_layer_t * target_layer) The Activation APIs are listed in nnom_activations.h // Activation nnom_activation_t* act_relu(void); nnom_activation_t* act_sigmoid(void); nnom_activation_t* act_tanh(void);","title":"Addictionlly, Activation APIs"},{"location":"A_Temporary_Guide_to_NNoM/#model-api","text":"A model instance contains the starting layer, the end layer and other neccessary info. Please refer to the examples for usage // Create or initial a new model() nnom_model_t* new_model(nnom_model_t* m); // Delete the model completely (new). void model_delete(nnom_model_t* m); // Compile a sequencial model. nnom_status_t sequencial_compile(nnom_model_t *m); // Compile a functional model with specified input layer and output layer. // if output = NULL, the output is automatic selected. nnom_status_t model_compile(nnom_model_t *m, nnom_layer_t* input, nnom_layer_t* output); // Run the model. nnom_status_t model_run(nnom_model_t *m);","title":"Model API"},{"location":"A_Temporary_Guide_to_NNoM/#known-issues","text":"","title":"Known Issues"},{"location":"A_Temporary_Guide_to_NNoM/#shared-output-buffer-destroyed-by-single-buffer-layers-input-destructive","text":"Single buffer layers (Such as most of the Activations, additionally MaxPool/AvgPool) are working directly on its input buffer. While its input buffer is shared with other parallel layers, and it is placed before other layers in a parallel structure (such as Inception), the shared buffer will be destroyed by those input-destructive before other parallel layer can access it. Additionally, although, MaxPool & AvgPool are not single buffer layers, they will destroy the input buffer as they are mentioned with input-destructive layers in CMSIS-NN. So they should be treated as same as single buffer layers. Fix plan of the issue Not planned. Possiblly, add an invisible copying layer/functions to copy data for single input layer before passing to other parallel layers. Current work around Work around 1 If the Inception has only one single buffer layer, always hook the single buffer layer at the end. For example, instead of doing MaxPool - Conv2D - Conv2D , do Conv2D - Conv2D - MaxPool // the codes are faked and simplified, please rewrite them according to corresponding APIs. // original x1 = model.hook(MaxPool(), input); // Single buffer layer, this will destroyed the buffer x2 = model.hook(Conv2D(), input); // buffer destroyed. x3 = model.hook(Conv2D(), input); // buffer destroyed. output = model.mergex(Concat(-1), 3, x1, x2, x3); // This will fixed the problem without affacting the concatenate order. // notice that the order of x1,x2,x3 will change, // the different is the order that the inception layers hooked to the input layer. x3 = model.hook(Conv2D(), input); // multiple buffers layer x2 = model.hook(Conv2D(), input); // x1 = model.hook(MaxPool(), input); // this will destroyed the buffer, but it doesnt matter now. output = model.mergex(Concat(-1), 3, x1, x2, x3); Work around 2 If there is multiple, add an extra multiple bufer layer before the single buffer layer. Such as using Lambda() layer to copy buffer. // the codes are faked and simplified, please rewrite them according to corresponding APIs. lambda_run(layer) { memcpy(layer->output, layer->input, sizeof(inputshape); } x1 = model.hook(Lambda(lambda_run), input); // add a lambda to copy data x1 = model.hook(MaxPool(), x1); // now it is only destroying Lambda's output buffer instead of the input layer's. x2 = model.hook(Lambda(lambda_run), input); // add a lambda to copy data x2 = model.hook(MaxPool(), x2); x3 = model.hook(Conv2D(), input); output = model.mergex(Concat(-1), 3, x1, x2, x3);","title":"Shared output buffer destroyed by single buffer layers (input-destructive)"},{"location":"A_Temporary_Guide_to_NNoM/#evaluation","text":"The evaluation methods are listed in nnom_utils.h They run the model with testing data, then evaluate the model. Includes Top-k accuracy, confusion matrix, runtime stat... Please refer to UCI HAR example for usage. // create a prediction // input model, the buf pointer to the softwmax output (Temporary, this can be extract from model) // the size of softmax output (the num of lable) // the top k that wants to record. nnom_predic_t* prediction_create(nnom_model_t* m, int8_t* buf_prediction, size_t label_num, size_t top_k_size);// currently int8_t // after a new data is set in input // feed data to prediction // input the current label, (range from 0 to total number of label -1) // (the current input data should be set by user manully to the input buffer of the model.) // return NN_ARGUMENT_ERROR if parameter error int32_t prediction_run(nnom_predic_t *pre, uint32_t label); // to mark prediction finished void prediction_end(nnom_predic_t *pre); // free all resources void predicetion_delete(nnom_predic_t *pre); // print matrix void prediction_matrix(nnom_predic_t *pre); // print top-k void prediction_top_k(nnom_predic_t *pre); // this function is to print sumarry void prediction_summary(nnom_predic_t *pre); // ------------------------------- // stand alone prediction API // this api test one set of data, return the prediction // input the model's input and output bufer // return the predicted label // return NN_ARGUMENT_ERROR if parameter error int32_t nnom_predic_one(nnom_model_t *m, int8_t *input, int8_t *output); // currently int8_t // print last runtime stat of the model void model_stat(nnom_model_t *m);","title":"Evaluation"},{"location":"A_Temporary_Guide_to_NNoM/#demo-of-evaluation","text":"The UCI HAR example runs on RT-Thread, uses Y-Modem to receive testing dataset, uses ringbuffer to store data, and the console (msh) to print the results. The layer order, activation, output shape, operation, memory of I/O, and assigned memory block are shown. It also summarised the memory cost by neural network. Type predic , then use Y-Modem to send the data file. The model will run once enough data is received. When the file copying done, the runtime summary, Top-k and confusion matrix will be printed Optionally, the runtime stat detail of each layer can be printed by nn_stat PS: The \"runtime stat\" in the animation is not correct, due to the test chip is overclocking (STM32L476 @ 160MHz, 2x overclocking), and the timer is overclocking as well. However, the numbers in prediction summary are correct, because they are measured by system_tick timer which is not overclocking.","title":"Demo of Evaluation"},{"location":"Porting_and_Optimisation_Guide/","text":"Porting Porting is not necessary since NNoM is a pure C framework. If your development environment support standard C library (libc), it will run without any problem by default setting. However, porting can gain better performance in some platforms by switch the backends or to provide print-out model info and evaluation. Options Porting is simply done by modified the nnom_port.h under port/ The default setting is shown below. // memory interfaces #define nnom_malloc(n) malloc(n) #define nnom_free(p) free(p) #define nnom_memset(p,v,s) memset(p,v,s) // runtime & debuges #define nnom_us_get() 0 #define nnom_ms_get() 0 #define NNOM_LOG(...) printf(__VA_ARGS__) // NNoM configuration #define NNOM_BLOCK_NUM (8) // maximum number of memory block #define DENSE_WEIGHT_OPT (1) // if used fully connected layer optimized weights. // Formate configuration //#define NNOM_USING_CHW // using CHW format instead of default HWC //#define NNOM_USING_CMSIS_NN // uncomment if use CMSIS-NN for optimation Memory interfaces Memory interfaces are required. If your platform doesn't support std interface, please modify them according to your platform. See example for porting for RT-Thread. Runtime & debuges They are optional. Its recommented to port them if your platform has console or terminal. They will help you to validate your model. nnom_us_get() is used in runtime analysis. If presented, NNoM would be able to record the time cost for each layer and calcualte the effeciency of them. This method should return the current time in a us resolution (16/32-bit unsigned value, values can overflow). nnom_ms_get() is used in evaluation with APIs in 'nnom_utils.c'. It is used to evaluate the performance with the whole banch of testing data. LOG() is used to print model compiling info and evaluation info. NNoM configuration NNOM_BLOCK_NUM is the maximum number of memory block. The utilisation of memory block will be printed during compiling. Adjust it when needed. DENSE_WEIGHT_OPT , reorder weights for dense will gain better performance. If your model is using 'nnom_utils.py' to deploy, weights are already reordered. Format configuration There are 2 formats normally use in machine learning. HWC and CHW . HWC called channel last, CHW called channel first. The default backend will be running on HWC format , which is optimized for CPU case. Images are normally stored in the memory using HWC format. 1. HWC format NNOM_USING_CMSIS_NN : uncomment it will enable the CMSIS-NN backend for acceleration. On ARM-Cortex-M4/7/33/35P chips, the performance can be increased about 5x while you enable it. For detail please check the paper To switch the backend from local backend to the optimized CMSIS-NN/DSP, simply uncomment the line #define NNOM_USING_CMSIS_NN . Then, in your project, you must: Include the CMSIS-NN as well as CMSIS-DSP in your project. Make sure the optimisation is enable on CMSIS-NN/DSP. Notes It is required that CMSIS version above 5.5.1+ (NN version > 1.1.0, DSP version 1.6.0). Make sure your compiler is using the new version of \"arm_math.h\". There might be a few duplicated in a project, such as the STM32 HAL has its own version of \"arm_math.h\" You might also define your chip core and enable your FPU support in your pre-compile configuration if you are not able to compile. e.g. when using STM32L476, you might add the two macro in your project ' ARM_MATH_CM4, __FPU_PRESENT=1' After all, you can try to evaluate the performance using the APIs in 'nnom_utils.c' 2. CHW format NNOM_USING_CHW : uncomment it will change the whole backend format to CHW This format runs very inefficient convolution in CPU only mode. However, it is compatible with most hardware accelerations, such as KPU in K210 . The pure C implemenmtation is completed. The haredware acceleration using KPU is underdevelopment, will be available soon. Notes When enable CHW model, CMSIS-NN will be automaticly excluded. Examples Porting for RT-Thread // memory interfaces #define nnom_malloc(n) rt_malloc(n) #define nnom_free(p) rt_free(p) #define nnom_memset(p,v,s) rt_memset(p,v,s) // runtime & debuges #define nnom_us_get() 0 #define nnom_ms_get() rt_tick_get() // when tick is set to 1000 #define NNOM_LOG(...) rt_kprintf(__VA_ARGS__) // NNoM configuration #define NNOM_BLOCK_NUM (8) // maximum number of memory block #define DENSE_WEIGHT_OPT (1) // if used fully connected layer optimized weights. // Formate configuration //#define NNOM_USING_CHW // using CHW format instead of default HWC //#define NNOM_USING_CMSIS_NN // uncomment if use CMSIS-NN for optimation","title":"Porting and Optimisation Guide"},{"location":"Porting_and_Optimisation_Guide/#porting","text":"Porting is not necessary since NNoM is a pure C framework. If your development environment support standard C library (libc), it will run without any problem by default setting. However, porting can gain better performance in some platforms by switch the backends or to provide print-out model info and evaluation.","title":"Porting"},{"location":"Porting_and_Optimisation_Guide/#options","text":"Porting is simply done by modified the nnom_port.h under port/ The default setting is shown below. // memory interfaces #define nnom_malloc(n) malloc(n) #define nnom_free(p) free(p) #define nnom_memset(p,v,s) memset(p,v,s) // runtime & debuges #define nnom_us_get() 0 #define nnom_ms_get() 0 #define NNOM_LOG(...) printf(__VA_ARGS__) // NNoM configuration #define NNOM_BLOCK_NUM (8) // maximum number of memory block #define DENSE_WEIGHT_OPT (1) // if used fully connected layer optimized weights. // Formate configuration //#define NNOM_USING_CHW // using CHW format instead of default HWC //#define NNOM_USING_CMSIS_NN // uncomment if use CMSIS-NN for optimation","title":"Options"},{"location":"Porting_and_Optimisation_Guide/#memory-interfaces","text":"Memory interfaces are required. If your platform doesn't support std interface, please modify them according to your platform. See example for porting for RT-Thread.","title":"Memory interfaces"},{"location":"Porting_and_Optimisation_Guide/#runtime-debuges","text":"They are optional. Its recommented to port them if your platform has console or terminal. They will help you to validate your model. nnom_us_get() is used in runtime analysis. If presented, NNoM would be able to record the time cost for each layer and calcualte the effeciency of them. This method should return the current time in a us resolution (16/32-bit unsigned value, values can overflow). nnom_ms_get() is used in evaluation with APIs in 'nnom_utils.c'. It is used to evaluate the performance with the whole banch of testing data. LOG() is used to print model compiling info and evaluation info.","title":"Runtime & debuges"},{"location":"Porting_and_Optimisation_Guide/#nnom-configuration","text":"NNOM_BLOCK_NUM is the maximum number of memory block. The utilisation of memory block will be printed during compiling. Adjust it when needed. DENSE_WEIGHT_OPT , reorder weights for dense will gain better performance. If your model is using 'nnom_utils.py' to deploy, weights are already reordered.","title":"NNoM configuration"},{"location":"Porting_and_Optimisation_Guide/#format-configuration","text":"There are 2 formats normally use in machine learning. HWC and CHW . HWC called channel last, CHW called channel first. The default backend will be running on HWC format , which is optimized for CPU case. Images are normally stored in the memory using HWC format. 1. HWC format NNOM_USING_CMSIS_NN : uncomment it will enable the CMSIS-NN backend for acceleration. On ARM-Cortex-M4/7/33/35P chips, the performance can be increased about 5x while you enable it. For detail please check the paper To switch the backend from local backend to the optimized CMSIS-NN/DSP, simply uncomment the line #define NNOM_USING_CMSIS_NN . Then, in your project, you must: Include the CMSIS-NN as well as CMSIS-DSP in your project. Make sure the optimisation is enable on CMSIS-NN/DSP. Notes It is required that CMSIS version above 5.5.1+ (NN version > 1.1.0, DSP version 1.6.0). Make sure your compiler is using the new version of \"arm_math.h\". There might be a few duplicated in a project, such as the STM32 HAL has its own version of \"arm_math.h\" You might also define your chip core and enable your FPU support in your pre-compile configuration if you are not able to compile. e.g. when using STM32L476, you might add the two macro in your project ' ARM_MATH_CM4, __FPU_PRESENT=1' After all, you can try to evaluate the performance using the APIs in 'nnom_utils.c' 2. CHW format NNOM_USING_CHW : uncomment it will change the whole backend format to CHW This format runs very inefficient convolution in CPU only mode. However, it is compatible with most hardware accelerations, such as KPU in K210 . The pure C implemenmtation is completed. The haredware acceleration using KPU is underdevelopment, will be available soon. Notes When enable CHW model, CMSIS-NN will be automaticly excluded.","title":"Format configuration"},{"location":"Porting_and_Optimisation_Guide/#examples","text":"","title":"Examples"},{"location":"Porting_and_Optimisation_Guide/#porting-for-rt-thread","text":"// memory interfaces #define nnom_malloc(n) rt_malloc(n) #define nnom_free(p) rt_free(p) #define nnom_memset(p,v,s) rt_memset(p,v,s) // runtime & debuges #define nnom_us_get() 0 #define nnom_ms_get() rt_tick_get() // when tick is set to 1000 #define NNOM_LOG(...) rt_kprintf(__VA_ARGS__) // NNoM configuration #define NNOM_BLOCK_NUM (8) // maximum number of memory block #define DENSE_WEIGHT_OPT (1) // if used fully connected layer optimized weights. // Formate configuration //#define NNOM_USING_CHW // using CHW format instead of default HWC //#define NNOM_USING_CMSIS_NN // uncomment if use CMSIS-NN for optimation","title":"Porting for RT-Thread"},{"location":"api_activations/","text":"Activation Layers To reduce the memory footprint, activations provides both Layer APIs and Activation APIs . Layer APIs will create a layer instance for running the activation; Activation APIs will only create activation instance, which can be attached on a existing layers as 'actail'. Actail will be called after a layer has finished its job, which will modify the output of that layer. Activations' Layer APIs Activation() nnom_layer_t* Activation(nnom_activation_t *act); This is the Layer API wrapper for activations. It take activation instance as input. Arguments act: is the activation instance. Return The activation layer instance Softmax() nnom_layer_t* Softmax(void); Return The Softmax layer instance Notes Softmax only has Layer API. ReLU() nnom_layer_t* ReLU(void); Return The ReLU layer instance Notes Using layer = ReLU(); is no difference to layer = Activation(act_relu()); Sigmoid() nnom_layer_t* Sigmoid(int32_t dec_bit); This function is now perform normally. Arguments dec_bit: the decimal bit width of the data. which is the output shift of the last layer. It should be provide in the weight.h Return The Sigmoid layer instance Notes Using layer = Sigmoid(); is no difference to layer = Activation(act_sigmoid()); The output of this function will be set to 7 constantly. When dec_bit < 4, this function is performed by Heaviside step function due to the less resolutions. TanH() nnom_layer_t* TanH(int32_t dec_bit); Arguments dec_bit: the decimal bit width of the data. which is the output shift of the last layer. It should be provide in the weight.h Return The TanH layer instance This function is affacted by an issue that we are currently working on. Check issue Notes Using layer = TanH(); is no difference to layer = Activation(act_tanh()); The output of this function will be set to 7 constantly. When dec_bit < 4, this function is performed by {f(x)=1 if x>0; f(x)=0 if x=0; f(x)=-1 if x<0}. due to the loss of resolutions. Activation APIs nnom_activation_t* act_relu(void); nnom_activation_t* act_sigmoid(int32_t dec_bit); nnom_activation_t* act_tanh(int32_t dec_bit); They return the activation instance which can be passed to either model.active() or Activation() Notes Softmax does not provided activation APIs. Examples Using Layer's API Activation's layer API allows you to us them as a layer. nnom_layer_t layer; model.add(&model, Dense(10)); model.add(&model, ReLU()); nnom_layer_t layer; model.add(&model, Dense(10)); model.add(&model, Activation(act_relu())); nnom_layer_t layer; input = Input(shape(1, 10, 1), buffer); layer = model.hook(Dense(10), input); layer = model.hook(ReLU(), layer); All 3 above perform the same and take the occupide the same size of memory. Using Activation's API nnom_layer_t layer; input = Input(shape(1, 10, 1), buffer); layer = model.hook(Dense(10), input); layer = model.active(act_relu(), layer); This method perform the same but take less memory due to it uses the activation directly.","title":"Activations"},{"location":"api_activations/#activation-layers","text":"To reduce the memory footprint, activations provides both Layer APIs and Activation APIs . Layer APIs will create a layer instance for running the activation; Activation APIs will only create activation instance, which can be attached on a existing layers as 'actail'. Actail will be called after a layer has finished its job, which will modify the output of that layer.","title":"Activation Layers"},{"location":"api_activations/#activations-layer-apis","text":"","title":"Activations' Layer APIs"},{"location":"api_activations/#activation","text":"nnom_layer_t* Activation(nnom_activation_t *act); This is the Layer API wrapper for activations. It take activation instance as input. Arguments act: is the activation instance. Return The activation layer instance","title":"Activation()"},{"location":"api_activations/#softmax","text":"nnom_layer_t* Softmax(void); Return The Softmax layer instance Notes Softmax only has Layer API.","title":"Softmax()"},{"location":"api_activations/#relu","text":"nnom_layer_t* ReLU(void); Return The ReLU layer instance Notes Using layer = ReLU(); is no difference to layer = Activation(act_relu());","title":"ReLU()"},{"location":"api_activations/#sigmoid","text":"nnom_layer_t* Sigmoid(int32_t dec_bit); This function is now perform normally. Arguments dec_bit: the decimal bit width of the data. which is the output shift of the last layer. It should be provide in the weight.h Return The Sigmoid layer instance Notes Using layer = Sigmoid(); is no difference to layer = Activation(act_sigmoid()); The output of this function will be set to 7 constantly. When dec_bit < 4, this function is performed by Heaviside step function due to the less resolutions.","title":"Sigmoid()"},{"location":"api_activations/#tanh","text":"nnom_layer_t* TanH(int32_t dec_bit); Arguments dec_bit: the decimal bit width of the data. which is the output shift of the last layer. It should be provide in the weight.h Return The TanH layer instance This function is affacted by an issue that we are currently working on. Check issue Notes Using layer = TanH(); is no difference to layer = Activation(act_tanh()); The output of this function will be set to 7 constantly. When dec_bit < 4, this function is performed by {f(x)=1 if x>0; f(x)=0 if x=0; f(x)=-1 if x<0}. due to the loss of resolutions.","title":"TanH()"},{"location":"api_activations/#activation-apis","text":"nnom_activation_t* act_relu(void); nnom_activation_t* act_sigmoid(int32_t dec_bit); nnom_activation_t* act_tanh(int32_t dec_bit); They return the activation instance which can be passed to either model.active() or Activation() Notes Softmax does not provided activation APIs.","title":"Activation APIs"},{"location":"api_activations/#examples","text":"Using Layer's API Activation's layer API allows you to us them as a layer. nnom_layer_t layer; model.add(&model, Dense(10)); model.add(&model, ReLU()); nnom_layer_t layer; model.add(&model, Dense(10)); model.add(&model, Activation(act_relu())); nnom_layer_t layer; input = Input(shape(1, 10, 1), buffer); layer = model.hook(Dense(10), input); layer = model.hook(ReLU(), layer); All 3 above perform the same and take the occupide the same size of memory. Using Activation's API nnom_layer_t layer; input = Input(shape(1, 10, 1), buffer); layer = model.hook(Dense(10), input); layer = model.active(act_relu(), layer); This method perform the same but take less memory due to it uses the activation directly.","title":"Examples"},{"location":"api_construction/","text":"Constructions APIs NNoM support both Sequential model and Functional model similar to Keras. NNoM treat them equaly in compiling and running, the only difference the methods of constructions. In Sequential model , the layer are stacked one by one sequently using model.add() . So call Sequential API. In Functional model , the links between layer are specified explicitly by using model.hook() , model.merge() or model.mergex() . So call Functional APIs Sequential API model.add() nnom_status_t model.add(nnom_model_t *model, nnom_layer_t *layer); It is the only sequencial constructor. It stacks the new layer to the rear of the existing model. Arguments model: the model to stack new layer. layer: the new layer instance to stack onto the model. Return The state of the operation. Example to stack two layers on a model model.add(&model, Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b)); model.add(&model, ReLU()); ... Notes The first layer for a model must be Input layer . The last layer could be the Output layer You can stack like this whenever there are memory available :-) Functional APIs model.hook() nnom_layer_t *model.hook(nnom_layer_t *curr, nnom_layer_t *last); A functional constructor to explicitly hook two layers togethers. When two layers are hook, the previous layer's output (last) will be the input of the new later (curr). Arguments curr: the new layer for hooking to a previous built layer. last: the previous built layer instance. Return The curr layer instance. Note A layer instance can be hooked many times (act as \"last\" layer). NNoM will manage the topology and run order from them during compiling. This is very useful when many layers wants to take the same output of previous layer. The example is many towers layer share one output in Inception structure. model.merge() nnom_layer_t *model.merge(nnom_layer_t *method, nnom_layer_t *in1, nnom_layer_t *in2); A functional constructor for merge many layers' output by the specified merging methods(layer). Specificaly, this method merge 2 layer's output. Arguments method: the merging layer method. One of Concat(), Mult(), Add(), Sub() in1: the first layer instance to merge. in2: the second layer instance to merge. Return The method (layer) instance. model.mergex() nnom_layer_t *model.mergex(nnom_layer_t *method, int num, ...) A functional constructor for merge many layers' output by the specified merging methods(layer). Arguments method: the merging layer method. One of Concat(), Mult(), Add(), Sub() num: number of layer that needs to be merged. ...: the list of merge layer instances. Return The method (layer) instance. Note Currently, all \"merge methods\" support mutiple input layers, they will be processed one by one with the order provided by the list. model.active() nnom_layer_t *model.active(nnom_activation_t *act, nnom_layer_t *target) A functional constructor, it merges the activation to the targeted layer to avoid an redundant activation layer, which costs more memory. Arguments act: the activation instance, please check Activation for more detail. target: the layer which output will be activated by the provided activation. Return The targeted (layer) instance. Examples This example shows the construction of an Inception model. nnom_layer_t* input_layer, *x, *x1, *x2, *x3; input_layer = Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), nnom_input_data); // conv2d - 1 - inception x1 = model.hook(Conv2D(16, kernel(1, 5), stride(1, 1), PADDING_SAME, &c2_w, &c2_b), x); x1 = model.active(act_relu(), x1); x1 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x1); // conv2d - 2 - inception x2 = model.hook(Conv2D(16, kernel(1, 3), stride(1, 1), PADDING_SAME, &c3_w, &c3_b), x); x2 = model.active(act_relu(), x2); x2 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x2); // maxpool - 3 - inception x3 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // concatenate x = model.mergex(Concat(-1), 3, x1, x2, x3); // flatten x = model.hook(Flatten(), x); ...","title":"Construction Methods"},{"location":"api_construction/#constructions-apis","text":"NNoM support both Sequential model and Functional model similar to Keras. NNoM treat them equaly in compiling and running, the only difference the methods of constructions. In Sequential model , the layer are stacked one by one sequently using model.add() . So call Sequential API. In Functional model , the links between layer are specified explicitly by using model.hook() , model.merge() or model.mergex() . So call Functional APIs","title":"Constructions APIs"},{"location":"api_construction/#sequential-api","text":"","title":"Sequential API"},{"location":"api_construction/#modeladd","text":"nnom_status_t model.add(nnom_model_t *model, nnom_layer_t *layer); It is the only sequencial constructor. It stacks the new layer to the rear of the existing model. Arguments model: the model to stack new layer. layer: the new layer instance to stack onto the model. Return The state of the operation. Example to stack two layers on a model model.add(&model, Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b)); model.add(&model, ReLU()); ... Notes The first layer for a model must be Input layer . The last layer could be the Output layer You can stack like this whenever there are memory available :-)","title":"model.add()"},{"location":"api_construction/#functional-apis","text":"","title":"Functional APIs"},{"location":"api_construction/#modelhook","text":"nnom_layer_t *model.hook(nnom_layer_t *curr, nnom_layer_t *last); A functional constructor to explicitly hook two layers togethers. When two layers are hook, the previous layer's output (last) will be the input of the new later (curr). Arguments curr: the new layer for hooking to a previous built layer. last: the previous built layer instance. Return The curr layer instance. Note A layer instance can be hooked many times (act as \"last\" layer). NNoM will manage the topology and run order from them during compiling. This is very useful when many layers wants to take the same output of previous layer. The example is many towers layer share one output in Inception structure.","title":"model.hook()"},{"location":"api_construction/#modelmerge","text":"nnom_layer_t *model.merge(nnom_layer_t *method, nnom_layer_t *in1, nnom_layer_t *in2); A functional constructor for merge many layers' output by the specified merging methods(layer). Specificaly, this method merge 2 layer's output. Arguments method: the merging layer method. One of Concat(), Mult(), Add(), Sub() in1: the first layer instance to merge. in2: the second layer instance to merge. Return The method (layer) instance.","title":"model.merge()"},{"location":"api_construction/#modelmergex","text":"nnom_layer_t *model.mergex(nnom_layer_t *method, int num, ...) A functional constructor for merge many layers' output by the specified merging methods(layer). Arguments method: the merging layer method. One of Concat(), Mult(), Add(), Sub() num: number of layer that needs to be merged. ...: the list of merge layer instances. Return The method (layer) instance. Note Currently, all \"merge methods\" support mutiple input layers, they will be processed one by one with the order provided by the list.","title":"model.mergex()"},{"location":"api_construction/#modelactive","text":"nnom_layer_t *model.active(nnom_activation_t *act, nnom_layer_t *target) A functional constructor, it merges the activation to the targeted layer to avoid an redundant activation layer, which costs more memory. Arguments act: the activation instance, please check Activation for more detail. target: the layer which output will be activated by the provided activation. Return The targeted (layer) instance.","title":"model.active()"},{"location":"api_construction/#examples","text":"This example shows the construction of an Inception model. nnom_layer_t* input_layer, *x, *x1, *x2, *x3; input_layer = Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), nnom_input_data); // conv2d - 1 - inception x1 = model.hook(Conv2D(16, kernel(1, 5), stride(1, 1), PADDING_SAME, &c2_w, &c2_b), x); x1 = model.active(act_relu(), x1); x1 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x1); // conv2d - 2 - inception x2 = model.hook(Conv2D(16, kernel(1, 3), stride(1, 1), PADDING_SAME, &c3_w, &c3_b), x); x2 = model.active(act_relu(), x2); x2 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x2); // maxpool - 3 - inception x3 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // concatenate x = model.mergex(Concat(-1), 3, x1, x2, x3); // flatten x = model.hook(Flatten(), x); ...","title":"Examples"},{"location":"api_evaluation/","text":"Evaluation tools NNoM has provide a few evaluation interfaces. Thye can either do runtime statistic or model evaluations. These API are print though the standard printf() , thus a terminal/console is needed. All these API must not be called before the model has been compiled. model_stat() void model_stat(nnom_model_t *m); To print the runtime statistic of the last run. Check the below example for the availble statistics. Arguments m: the model to print. Notes It is recommended to run the mode once after compiling to gain these runtime statistic. Example Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8292 172800 20.83 #3 MaxPool - 5753 0 #4 Conv2D - 50095 3612672 72.11 #5 MaxPool - 3617 0 #6 Conv2D - 35893 2654208 73.94 #7 MaxPool - 1206 0 #8 Conv2D - 4412 171072 38.77 #9 GL_SumPool - 30 0 #10 Softmax - 2 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109309us Efficiency 60.47 ops/us Total Memory cost (Network and NNoM): 32876 nnom_predict() int32_t nnom_predict(nnom_model_t *m, uint32_t *label, float *prob); A standalone evaluation method, run single prodiction, return probability and top-1 label. This method is basicly model_run() + index(top-1) Arguments m: the model to run prediction (evaluation). label: the variable to store top-1 label. prob: the variable to store probability. Range from 0~1. Return The predicted label in digit. Error codes if model is failed to run. Note The input buffer of the model must be feeded before calling this method. prediction_create() nnom_predict_t *prediction_create(nnom_model_t *m, int8_t *buf_prediction, size_t label_num, size_t top_k_size); This method create a prediction instance, which record mutiple parameters in the evaluation process. Arguments m: the model to run prediction (evaluation). buf_prediction: the output buffer of the model, which should be the output of Softmax. Size equal to the size of class. label_num: the number of labels (the number of classifications). top_k_size: the Top-k that wants to evaluate. Return The prediction instance. Note Check later examples. prediction_run() nnom_status_t prediction_run(nnom_predict_t *pre, uint32_t true_label, uint32_t* predict_label, float* prob) To run a prediction with the new data (feeded by user to the input_buffer which passed to Input layer). Arguments pre: the prediction instance created by prediction_create() . true_label: true label of this data. predict_label: the predicted label of this data (top-1 results). prob: the probability of this label. Return nnom_status_t prediction_end() void prediction_end(nnom_predict_t *pre); To mark the prediction has done. Arguments pre: the prediction instance created by prediction_create() . prediction_delete() void predicetion_delete(nnom_predict_t *pre); To free all resources. Arguments pre: the prediction instance created by prediction_create() . prediction_matrix() void prediction_matrix(nnom_predict_t *pre); To print a confusion matrix when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Confusion matrix: predic 0 1 2 3 4 5 6 7 8 9 10 actual 0 | 395 1 0 0 2 0 0 0 0 0 21 | 94% 1 | 0 355 4 7 1 0 0 0 0 3 35 | 87% 2 | 0 3 325 2 1 0 7 29 3 2 53 | 76% 3 | 0 33 1 335 1 0 0 0 0 2 34 | 82% 4 | 6 0 1 0 371 3 0 0 0 0 31 | 90% 5 | 0 0 2 0 6 347 0 0 0 0 41 | 87% 6 | 0 1 5 8 0 0 322 4 0 0 56 | 81% 7 | 0 3 23 0 3 0 9 330 1 1 32 | 82% 8 | 0 0 5 2 0 0 0 0 343 4 57 | 83% 9 | 0 40 4 10 1 0 0 0 0 304 43 | 75% 10 | 4 61 16 34 28 17 14 6 12 37 6702 | 96% prediction_top_k() void prediction_top_k(nnom_predict_t *pre); To print a Top-k when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Top 1 Accuracy: 92.03% Top 2 Accuracy: 96.39% Top 3 Accuracy: 97.38% Top 4 Accuracy: 97.85% Top 5 Accuracy: 98.13% Top 6 Accuracy: 98.40% Top 7 Accuracy: 98.59% Top 8 Accuracy: 98.88% Top 9 Accuracy: 99.14% Top 10 Accuracy: 99.60% prediction_summary() void prediction_summary(nnom_predict_t *pre); To print a summary when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Prediction summary: Test frames: 11005 Test running time: 1598 sec Model running time: 1364908 ms Average prediction time: 124026 us Average effeciency: 53.30 ops/us Average frame rate: 8.0 Hz Example Evaluate a model using prediction_* APIs The model needs to be compiled before it is being evaluated. The evaluation gose through a few steps Create a instance using prediction_create() Feed the data one by one to the input buffer, then call prediction_run() with the true label. When all data has predicted, call prediction_end() Then you can use prediction_matrix() , prediction_top_k() , and prediction_summary() to see the results. In addition, you can call model_stat() to see the performance of the last prediction. After all, call prediction_delete() to release all memory. Log from Key-word Spotting Example . msh > \\ | / - RT - Thread Operating System / | \\ 4.0.0 build Mar 28 2019 2006 - 2018 Copyright by rt-thread team RTT Control Block Detection Address is 0x20000b3c msh > INFO: Start compile... Layer Activation output shape ops memory mem life-time ---------------------------------------------------------------------------------------------- Input - - ( 62, 12, 1) 0 ( 744, 744, 0) 1 - - - - - - - Conv2D - ReLU - ( 60, 10, 32) 172800 ( 744,19200, 1152) 1 1 - - - - - - MaxPool - - ( 30, 9, 32) 0 (19200, 8640, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 28, 7, 64) 3612672 ( 8640,12544, 2304) 1 1 - - - - - - MaxPool - - ( 14, 6, 64) 0 (12544, 5376, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 12, 4, 96) 2654208 ( 5376, 4608, 3456) 1 1 - - - - - - MaxPool - - ( 6, 3, 96) 0 ( 4608, 1728, 0) 1 - 1 - - - - - Conv2D - - ( 6, 3, 11) 171072 ( 1728, 198, 396) 1 1 - - - - - - GL_SumPool - - ( 1, 1, 11) 0 ( 198, 11, 44) 1 - 1 - - - - - Softmax - - ( 1, 1, 11) 0 ( 11, 11, 0) - 1 - - - - - - Output - - ( 11, 1, 1) 0 ( 11, 11, 0) 1 - - - - - - - ---------------------------------------------------------------------------------------------- INFO: memory analysis result Block0: 3456 Block1: 8640 Block2: 19200 Block3: 0 Block4: 0 Block5: 0 Block6: 0 Block7: 0 Total memory cost by network buffers: 31296 bytes msh >nn nn_stat msh >nn_stat Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8294 172800 20.83 #3 MaxPool - 5750 0 #4 Conv2D - 50089 3612672 72.12 #5 MaxPool - 3619 0 #6 Conv2D - 35890 2654208 73.95 #7 MaxPool - 1204 0 #8 Conv2D - 4411 171072 38.78 #9 GL_SumPool - 30 0 #10 Softmax - 3 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109299us Efficiency 60.48 ops/us Total Memory cost (Network and NNoM): 32876 msh >pre predict msh >predict Please select the NNoM binary test file and use Ymodem-128/1024 to send. CCCC Prediction done. Prediction summary: Test frames: 11005 Test running time: 1598 sec Model running time: 1364908 ms Average prediction time: 124026 us Average effeciency: 53.30 ops/us Average frame rate: 8.0 Hz Top 1 Accuracy: 92.03% Top 2 Accuracy: 96.39% Top 3 Accuracy: 97.38% Top 4 Accuracy: 97.85% Top 5 Accuracy: 98.13% Top 6 Accuracy: 98.40% Top 7 Accuracy: 98.59% Top 8 Accuracy: 98.88% Top 9 Accuracy: 99.14% Top 10 Accuracy: 99.60% Confusion matrix: predict 0 1 2 3 4 5 6 7 8 9 10 actual 0 | 395 1 0 0 2 0 0 0 0 0 21 | 94% 1 | 0 355 4 7 1 0 0 0 0 3 35 | 87% 2 | 0 3 325 2 1 0 7 29 3 2 53 | 76% 3 | 0 33 1 335 1 0 0 0 0 2 34 | 82% 4 | 6 0 1 0 371 3 0 0 0 0 31 | 90% 5 | 0 0 2 0 6 347 0 0 0 0 41 | 87% 6 | 0 1 5 8 0 0 322 4 0 0 56 | 81% 7 | 0 3 23 0 3 0 9 330 1 1 32 | 82% 8 | 0 0 5 2 0 0 0 0 343 4 57 | 83% 9 | 0 40 4 10 1 0 0 0 0 304 43 | 75% 10 | 4 61 16 34 28 17 14 6 12 37 6702 | 96% msh > OO: command not found. msh > msh >nn nn_stat msh >nn_stat Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8292 172800 20.83 #3 MaxPool - 5753 0 #4 Conv2D - 50095 3612672 72.11 #5 MaxPool - 3617 0 #6 Conv2D - 35893 2654208 73.94 #7 MaxPool - 1206 0 #8 Conv2D - 4412 171072 38.77 #9 GL_SumPool - 30 0 #10 Softmax - 2 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109309us Efficiency 60.47 ops/us Total Memory cost (Network and NNoM): 32876 msh >","title":"Evaluation Methods"},{"location":"api_evaluation/#evaluation-tools","text":"NNoM has provide a few evaluation interfaces. Thye can either do runtime statistic or model evaluations. These API are print though the standard printf() , thus a terminal/console is needed. All these API must not be called before the model has been compiled.","title":"Evaluation tools"},{"location":"api_evaluation/#model_stat","text":"void model_stat(nnom_model_t *m); To print the runtime statistic of the last run. Check the below example for the availble statistics. Arguments m: the model to print. Notes It is recommended to run the mode once after compiling to gain these runtime statistic. Example Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8292 172800 20.83 #3 MaxPool - 5753 0 #4 Conv2D - 50095 3612672 72.11 #5 MaxPool - 3617 0 #6 Conv2D - 35893 2654208 73.94 #7 MaxPool - 1206 0 #8 Conv2D - 4412 171072 38.77 #9 GL_SumPool - 30 0 #10 Softmax - 2 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109309us Efficiency 60.47 ops/us Total Memory cost (Network and NNoM): 32876","title":"model_stat()"},{"location":"api_evaluation/#nnom_predict","text":"int32_t nnom_predict(nnom_model_t *m, uint32_t *label, float *prob); A standalone evaluation method, run single prodiction, return probability and top-1 label. This method is basicly model_run() + index(top-1) Arguments m: the model to run prediction (evaluation). label: the variable to store top-1 label. prob: the variable to store probability. Range from 0~1. Return The predicted label in digit. Error codes if model is failed to run. Note The input buffer of the model must be feeded before calling this method.","title":"nnom_predict()"},{"location":"api_evaluation/#prediction_create","text":"nnom_predict_t *prediction_create(nnom_model_t *m, int8_t *buf_prediction, size_t label_num, size_t top_k_size); This method create a prediction instance, which record mutiple parameters in the evaluation process. Arguments m: the model to run prediction (evaluation). buf_prediction: the output buffer of the model, which should be the output of Softmax. Size equal to the size of class. label_num: the number of labels (the number of classifications). top_k_size: the Top-k that wants to evaluate. Return The prediction instance. Note Check later examples.","title":"prediction_create()"},{"location":"api_evaluation/#prediction_run","text":"nnom_status_t prediction_run(nnom_predict_t *pre, uint32_t true_label, uint32_t* predict_label, float* prob) To run a prediction with the new data (feeded by user to the input_buffer which passed to Input layer). Arguments pre: the prediction instance created by prediction_create() . true_label: true label of this data. predict_label: the predicted label of this data (top-1 results). prob: the probability of this label. Return nnom_status_t","title":"prediction_run()"},{"location":"api_evaluation/#prediction_end","text":"void prediction_end(nnom_predict_t *pre); To mark the prediction has done. Arguments pre: the prediction instance created by prediction_create() .","title":"prediction_end()"},{"location":"api_evaluation/#prediction_delete","text":"void predicetion_delete(nnom_predict_t *pre); To free all resources. Arguments pre: the prediction instance created by prediction_create() .","title":"prediction_delete()"},{"location":"api_evaluation/#prediction_matrix","text":"void prediction_matrix(nnom_predict_t *pre); To print a confusion matrix when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Confusion matrix: predic 0 1 2 3 4 5 6 7 8 9 10 actual 0 | 395 1 0 0 2 0 0 0 0 0 21 | 94% 1 | 0 355 4 7 1 0 0 0 0 3 35 | 87% 2 | 0 3 325 2 1 0 7 29 3 2 53 | 76% 3 | 0 33 1 335 1 0 0 0 0 2 34 | 82% 4 | 6 0 1 0 371 3 0 0 0 0 31 | 90% 5 | 0 0 2 0 6 347 0 0 0 0 41 | 87% 6 | 0 1 5 8 0 0 322 4 0 0 56 | 81% 7 | 0 3 23 0 3 0 9 330 1 1 32 | 82% 8 | 0 0 5 2 0 0 0 0 343 4 57 | 83% 9 | 0 40 4 10 1 0 0 0 0 304 43 | 75% 10 | 4 61 16 34 28 17 14 6 12 37 6702 | 96%","title":"prediction_matrix()"},{"location":"api_evaluation/#prediction_top_k","text":"void prediction_top_k(nnom_predict_t *pre); To print a Top-k when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Top 1 Accuracy: 92.03% Top 2 Accuracy: 96.39% Top 3 Accuracy: 97.38% Top 4 Accuracy: 97.85% Top 5 Accuracy: 98.13% Top 6 Accuracy: 98.40% Top 7 Accuracy: 98.59% Top 8 Accuracy: 98.88% Top 9 Accuracy: 99.14% Top 10 Accuracy: 99.60%","title":"prediction_top_k()"},{"location":"api_evaluation/#prediction_summary","text":"void prediction_summary(nnom_predict_t *pre); To print a summary when the prediction is done. Arguments pre: the prediction instance created by prediction_create() . Example Prediction summary: Test frames: 11005 Test running time: 1598 sec Model running time: 1364908 ms Average prediction time: 124026 us Average effeciency: 53.30 ops/us Average frame rate: 8.0 Hz","title":"prediction_summary()"},{"location":"api_evaluation/#example","text":"Evaluate a model using prediction_* APIs The model needs to be compiled before it is being evaluated. The evaluation gose through a few steps Create a instance using prediction_create() Feed the data one by one to the input buffer, then call prediction_run() with the true label. When all data has predicted, call prediction_end() Then you can use prediction_matrix() , prediction_top_k() , and prediction_summary() to see the results. In addition, you can call model_stat() to see the performance of the last prediction. After all, call prediction_delete() to release all memory. Log from Key-word Spotting Example . msh > \\ | / - RT - Thread Operating System / | \\ 4.0.0 build Mar 28 2019 2006 - 2018 Copyright by rt-thread team RTT Control Block Detection Address is 0x20000b3c msh > INFO: Start compile... Layer Activation output shape ops memory mem life-time ---------------------------------------------------------------------------------------------- Input - - ( 62, 12, 1) 0 ( 744, 744, 0) 1 - - - - - - - Conv2D - ReLU - ( 60, 10, 32) 172800 ( 744,19200, 1152) 1 1 - - - - - - MaxPool - - ( 30, 9, 32) 0 (19200, 8640, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 28, 7, 64) 3612672 ( 8640,12544, 2304) 1 1 - - - - - - MaxPool - - ( 14, 6, 64) 0 (12544, 5376, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 12, 4, 96) 2654208 ( 5376, 4608, 3456) 1 1 - - - - - - MaxPool - - ( 6, 3, 96) 0 ( 4608, 1728, 0) 1 - 1 - - - - - Conv2D - - ( 6, 3, 11) 171072 ( 1728, 198, 396) 1 1 - - - - - - GL_SumPool - - ( 1, 1, 11) 0 ( 198, 11, 44) 1 - 1 - - - - - Softmax - - ( 1, 1, 11) 0 ( 11, 11, 0) - 1 - - - - - - Output - - ( 11, 1, 1) 0 ( 11, 11, 0) 1 - - - - - - - ---------------------------------------------------------------------------------------------- INFO: memory analysis result Block0: 3456 Block1: 8640 Block2: 19200 Block3: 0 Block4: 0 Block5: 0 Block6: 0 Block7: 0 Total memory cost by network buffers: 31296 bytes msh >nn nn_stat msh >nn_stat Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8294 172800 20.83 #3 MaxPool - 5750 0 #4 Conv2D - 50089 3612672 72.12 #5 MaxPool - 3619 0 #6 Conv2D - 35890 2654208 73.95 #7 MaxPool - 1204 0 #8 Conv2D - 4411 171072 38.78 #9 GL_SumPool - 30 0 #10 Softmax - 3 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109299us Efficiency 60.48 ops/us Total Memory cost (Network and NNoM): 32876 msh >pre predict msh >predict Please select the NNoM binary test file and use Ymodem-128/1024 to send. CCCC Prediction done. Prediction summary: Test frames: 11005 Test running time: 1598 sec Model running time: 1364908 ms Average prediction time: 124026 us Average effeciency: 53.30 ops/us Average frame rate: 8.0 Hz Top 1 Accuracy: 92.03% Top 2 Accuracy: 96.39% Top 3 Accuracy: 97.38% Top 4 Accuracy: 97.85% Top 5 Accuracy: 98.13% Top 6 Accuracy: 98.40% Top 7 Accuracy: 98.59% Top 8 Accuracy: 98.88% Top 9 Accuracy: 99.14% Top 10 Accuracy: 99.60% Confusion matrix: predict 0 1 2 3 4 5 6 7 8 9 10 actual 0 | 395 1 0 0 2 0 0 0 0 0 21 | 94% 1 | 0 355 4 7 1 0 0 0 0 3 35 | 87% 2 | 0 3 325 2 1 0 7 29 3 2 53 | 76% 3 | 0 33 1 335 1 0 0 0 0 2 34 | 82% 4 | 6 0 1 0 371 3 0 0 0 0 31 | 90% 5 | 0 0 2 0 6 347 0 0 0 0 41 | 87% 6 | 0 1 5 8 0 0 322 4 0 0 56 | 81% 7 | 0 3 23 0 3 0 9 330 1 1 32 | 82% 8 | 0 0 5 2 0 0 0 0 343 4 57 | 83% 9 | 0 40 4 10 1 0 0 0 0 304 43 | 75% 10 | 4 61 16 34 28 17 14 6 12 37 6702 | 96% msh > OO: command not found. msh > msh >nn nn_stat msh >nn_stat Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 9 0 #2 Conv2D - 8292 172800 20.83 #3 MaxPool - 5753 0 #4 Conv2D - 50095 3612672 72.11 #5 MaxPool - 3617 0 #6 Conv2D - 35893 2654208 73.94 #7 MaxPool - 1206 0 #8 Conv2D - 4412 171072 38.77 #9 GL_SumPool - 30 0 #10 Softmax - 2 0 #11 Output - 0 0 Summary. Total ops (MAC): 6610752 Prediction time :109309us Efficiency 60.47 ops/us Total Memory cost (Network and NNoM): 32876 msh >","title":"Example"},{"location":"api_layers/","text":"Layer APIs Layers APIs are listed in nnom_layers.h Notes 1D/2D operations are both working with (H, W, C) format, known as \"channel last\". When working with 1D operations, the H for all the shapes must be 1 constantly. Input() nnom_layer_t* Input(nnom_3d_shape_t input_shape, * p_buf); A model must start with a Input layer to copy input data from user memory space to NNoM memory space. If NNoM is set to CHW format, the Input layer will also change the input format from HWC (regular store format for image in memory) to CHW during copying. Arguments input_shape: the shape of input data to the model. p_buf: the data buf in user space. Return The layer instance Output() nnom_layer_t* Output(nnom_3d_shape_t output_shape* p_buf); Output layer is to copy the result from NNoM memory space to user memory space. Arguments output_shape: the shape of output data. (might be deprecated later) p_buf: the data buf in user space. Return The layer instance. Conv2D() nnom_layer_t* Conv2D(uint32_t filters, nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); This funtion is for 1D or 2D, mutiple channels convolution. Arguments filters: the number of filters. or the channels of the output spaces. k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method either PADDING_SAME or PADDING_VALID w (weights) / b (bias) : weights and bias constants and shits. Generated in weights.h Return The layer instance Notes When it is used for 1D convolution, the H should be set to 1 constantly in kernel and stride. DW_Conv2D() nnom_layer_t* DW_Conv2D(uint32_t multiplier, nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); This funtion is for 1D or 2D, mutiple channels depthwise convolution. Arguments mutiplier: the number of mutiplier. Currently only support mutiplier=1 k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method either PADDING_SAME or PADDING_VALID w (weights) / b (bias) : weights and bias constants and shits. Generated in weights.h Return The layer instance Notes When it is used for 1D convolution, the H should be set to 1 constantly in kernel and stride. Dense() nnom_layer_t* Dense(size_t output_unit, nnom_weight_t *w, nnom_bias_t *b); A fully connected layer. It will flatten the data if the last output is mutiple-dimensions. Arguments output_unit: the number of output unit. w (weights) / b (bias) : weights and bias constants and shits. Return The layer instance UpSample() nnom_layer_t *UpSample(nnom_3d_shape_t kernel); A basic up sampling, using nearest interpolation Arguments kernel: a shape object returned by kernel() , the interpolation size. Return The layer instance ZeroPadding() nnom_layer_t *ZeroPadding(nnom_border_t pad); Pad zeros to the image for each edge (top, bottom, left, right) Arguments pad: a border object returned by border() , contains top, bottom, left and right padding. Return The layer instance Cropping() nnom_layer_t *Cropping(nnom_border_t pad); It crops along spatial dimensions. Arguments pad: a border object returned by border() , contains top, bottom, left and right size. Return The layer instance Lambda() // layer.run() , compulsory // layer.oshape(), optional, call default_output_shape() if left NULL // layer.free() , optional, called while model is deleting, to free private resources // parameters , private parameters for run method, left NULL if not needed. nnom_layer_t *Lambda(nnom_status_t (*run)(nnom_layer_t *), nnom_status_t (*oshape)(nnom_layer_t *), nnom_status_t (*free)(nnom_layer_t *), void *parameters); Lambda layer is an anonymous layer (interface), which allows user to do customized operation between the layer's input data and output data. Arguments (*run)(nnom_layer_t *) : or so called run method, is the method to do the customized operation. (*oshape)(nnom_layer_t *) : is to calculate the output shape according to the input shape during compiling. If this method is not presented, the input shape will be passed to the output shape. (*free)(nnom_layer_t *) : is to free the resources allocated by the users. This method will be called when the model is deleting. Leave it NULL if no resources need to be released. parameters: is the pointer to user configurations. User can access to it in all three methods above. Return The layer instance Notes All methods with type nnom_status_t must return NN_SUCCESS to allow the inference process. Any return other than that will stop the inference of the model. When oshape() is presented, please refer to examples of other similar layers. The shape passing must be handle carefully. This method is called in compiling, thus it can also do works other than calculating output shape only. An exmaple is the global_pooling_output_shape() fills in the parameters left by GlobalXXXPool() Examples Conv2D: //For 1D convolution nnom_layer_t *layer; layer = Conv2D(32, kernel(1, 5), stride(1, 2), PADDING_VALID, &conv2d_3_w, &conv2d_3_b);` DW_Conv2D: nnom_layer_t *layer; layer = DW_Conv2D(1, kernel(3, 3), stride(1, 1), PADDING_VALID, &conv2d_3_w, &conv2d_3_b);` Dense: nnom_layer_t *layer; layer = Dense(32, &dense_w, &dense_b); UpSample: nnom_layer_t *layer; layer = UpSample(kernel(2, 2)); // expend the output size by 2 times in both H and W axis. Lambda: This example shows how to use Lambda layer to copy data from the input buffer to the output buffer. nnom_status_t lambda_run(layer) { memcpy(layer->output, layer->input, sizeof(inputshape); return NN_SUCCESS; } main() { layer *x, *input; x = model.hook(Lambda(lambda_run, NULL, NULL, NULL), input); }","title":"Core Layers"},{"location":"api_layers/#layer-apis","text":"Layers APIs are listed in nnom_layers.h Notes 1D/2D operations are both working with (H, W, C) format, known as \"channel last\". When working with 1D operations, the H for all the shapes must be 1 constantly.","title":"Layer APIs"},{"location":"api_layers/#input","text":"nnom_layer_t* Input(nnom_3d_shape_t input_shape, * p_buf); A model must start with a Input layer to copy input data from user memory space to NNoM memory space. If NNoM is set to CHW format, the Input layer will also change the input format from HWC (regular store format for image in memory) to CHW during copying. Arguments input_shape: the shape of input data to the model. p_buf: the data buf in user space. Return The layer instance","title":"Input()"},{"location":"api_layers/#output","text":"nnom_layer_t* Output(nnom_3d_shape_t output_shape* p_buf); Output layer is to copy the result from NNoM memory space to user memory space. Arguments output_shape: the shape of output data. (might be deprecated later) p_buf: the data buf in user space. Return The layer instance.","title":"Output()"},{"location":"api_layers/#conv2d","text":"nnom_layer_t* Conv2D(uint32_t filters, nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); This funtion is for 1D or 2D, mutiple channels convolution. Arguments filters: the number of filters. or the channels of the output spaces. k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method either PADDING_SAME or PADDING_VALID w (weights) / b (bias) : weights and bias constants and shits. Generated in weights.h Return The layer instance Notes When it is used for 1D convolution, the H should be set to 1 constantly in kernel and stride.","title":"Conv2D()"},{"location":"api_layers/#dw_conv2d","text":"nnom_layer_t* DW_Conv2D(uint32_t multiplier, nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad, nnom_weight_t *w, nnom_bias_t *b); This funtion is for 1D or 2D, mutiple channels depthwise convolution. Arguments mutiplier: the number of mutiplier. Currently only support mutiplier=1 k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method either PADDING_SAME or PADDING_VALID w (weights) / b (bias) : weights and bias constants and shits. Generated in weights.h Return The layer instance Notes When it is used for 1D convolution, the H should be set to 1 constantly in kernel and stride.","title":"DW_Conv2D()"},{"location":"api_layers/#dense","text":"nnom_layer_t* Dense(size_t output_unit, nnom_weight_t *w, nnom_bias_t *b); A fully connected layer. It will flatten the data if the last output is mutiple-dimensions. Arguments output_unit: the number of output unit. w (weights) / b (bias) : weights and bias constants and shits. Return The layer instance","title":"Dense()"},{"location":"api_layers/#upsample","text":"nnom_layer_t *UpSample(nnom_3d_shape_t kernel); A basic up sampling, using nearest interpolation Arguments kernel: a shape object returned by kernel() , the interpolation size. Return The layer instance","title":"UpSample()"},{"location":"api_layers/#zeropadding","text":"nnom_layer_t *ZeroPadding(nnom_border_t pad); Pad zeros to the image for each edge (top, bottom, left, right) Arguments pad: a border object returned by border() , contains top, bottom, left and right padding. Return The layer instance","title":"ZeroPadding()"},{"location":"api_layers/#cropping","text":"nnom_layer_t *Cropping(nnom_border_t pad); It crops along spatial dimensions. Arguments pad: a border object returned by border() , contains top, bottom, left and right size. Return The layer instance","title":"Cropping()"},{"location":"api_layers/#lambda","text":"// layer.run() , compulsory // layer.oshape(), optional, call default_output_shape() if left NULL // layer.free() , optional, called while model is deleting, to free private resources // parameters , private parameters for run method, left NULL if not needed. nnom_layer_t *Lambda(nnom_status_t (*run)(nnom_layer_t *), nnom_status_t (*oshape)(nnom_layer_t *), nnom_status_t (*free)(nnom_layer_t *), void *parameters); Lambda layer is an anonymous layer (interface), which allows user to do customized operation between the layer's input data and output data. Arguments (*run)(nnom_layer_t *) : or so called run method, is the method to do the customized operation. (*oshape)(nnom_layer_t *) : is to calculate the output shape according to the input shape during compiling. If this method is not presented, the input shape will be passed to the output shape. (*free)(nnom_layer_t *) : is to free the resources allocated by the users. This method will be called when the model is deleting. Leave it NULL if no resources need to be released. parameters: is the pointer to user configurations. User can access to it in all three methods above. Return The layer instance Notes All methods with type nnom_status_t must return NN_SUCCESS to allow the inference process. Any return other than that will stop the inference of the model. When oshape() is presented, please refer to examples of other similar layers. The shape passing must be handle carefully. This method is called in compiling, thus it can also do works other than calculating output shape only. An exmaple is the global_pooling_output_shape() fills in the parameters left by GlobalXXXPool()","title":"Lambda()"},{"location":"api_layers/#examples","text":"Conv2D: //For 1D convolution nnom_layer_t *layer; layer = Conv2D(32, kernel(1, 5), stride(1, 2), PADDING_VALID, &conv2d_3_w, &conv2d_3_b);` DW_Conv2D: nnom_layer_t *layer; layer = DW_Conv2D(1, kernel(3, 3), stride(1, 1), PADDING_VALID, &conv2d_3_w, &conv2d_3_b);` Dense: nnom_layer_t *layer; layer = Dense(32, &dense_w, &dense_b); UpSample: nnom_layer_t *layer; layer = UpSample(kernel(2, 2)); // expend the output size by 2 times in both H and W axis. Lambda: This example shows how to use Lambda layer to copy data from the input buffer to the output buffer. nnom_status_t lambda_run(layer) { memcpy(layer->output, layer->input, sizeof(inputshape); return NN_SUCCESS; } main() { layer *x, *input; x = model.hook(Lambda(lambda_run, NULL, NULL, NULL), input); }","title":"Examples"},{"location":"api_merge/","text":"Merging Methods Merge methods (layers) are use to merge 2 or more layer's output using the methods list below. These methods are also layers which return a layer instance. However, they normally take two or more layer's output and \"merge\" them into one output. These layer instance must be passed to either model.merge(method, in1, in2) or model.mergex(method, num_of_input, in1, in2, 1n3 ...) . An example will be to concat the Inception structure. Concat() nnom_layer_t* Concat(int8_t axis); Concatenate mutiple input on the selected axis. Arguments axis: the axis number to concatenate in HWC format. The axis could be nagative, such as '-1' indicate the last one axis which is 'Channel'. Return The concat layer instance Notes The concatenated axis can be different in all input layers passed to this method. Other axes must be same. Mult() nnom_layer_t* Mult(int32_t oshift); Element wise mutiplication in all the inputs. This layer cannot use to merge more than 2 layer, which might cause overflowing problem. 2 Mult() must be used separately if willing to multiply 3 layer's output. The output shift individually of the 2 steps must be identify individually. Please check the example below for more than 2 input. Arguments oshift: the output shift of this layer. Return The mult layer instance Notes All input layers passed to this method must have same output shape. Add() nnom_layer_t* Add(int32_t oshift); Element wise addition in all the inputs. This layer cannot use to merge more than 2 layer, which might cause overflowing problem. Please refer to Mult() Arguments oshift: the output shift of this layer. Return The add layer instance Notes All input layers passed to this method must have same output shape. Sub() nnom_layer_t* Sub(int32_t oshift); Element wise substraction in all the inputs. This layer cannot use to merge more than 2 layer, which might cause overflowing problem. Please refer to Mult() Arguments oshift: the output shift of this layer. Return The sub layer instance Notes All input layers passed to this method must have same output shape. Example Channelwise concat for Inception input_layer = Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), nnom_input_data); // conv2d - 1 - inception x1 = model.hook(Conv2D(16, kernel(1, 5), stride(1, 1), PADDING_SAME, &c2_w, &c2_b), x); x1 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x1); // conv2d - 2 - inception x2 = model.hook(Conv2D(16, kernel(1, 3), stride(1, 1), PADDING_SAME, &c3_w, &c3_b), x); x2 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x2); // maxpool - 3 - inception x3 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // concatenate x = model.mergex(Concat(-1), 3, x1, x2, x3); // flatten x = model.hook(Flatten(), x); ... Mult for 3 input (or Add, Sub) In Keras #instead of x = multiply([x1,x2,x3]) # you must use this instead to allow sript to calculate oshift individually. x = multiply([x1,x2]) x = multiply([x,x3]) Then in NNoM // x = x1 * x2 * x3 x = model.merge(Mult(oshift_1), x1, x2); x = model.merge(Mult(oshift_2), x, x3);","title":"Merges Layers"},{"location":"api_merge/#merging-methods","text":"Merge methods (layers) are use to merge 2 or more layer's output using the methods list below. These methods are also layers which return a layer instance. However, they normally take two or more layer's output and \"merge\" them into one output. These layer instance must be passed to either model.merge(method, in1, in2) or model.mergex(method, num_of_input, in1, in2, 1n3 ...) . An example will be to concat the Inception structure.","title":"Merging Methods"},{"location":"api_merge/#concat","text":"nnom_layer_t* Concat(int8_t axis); Concatenate mutiple input on the selected axis. Arguments axis: the axis number to concatenate in HWC format. The axis could be nagative, such as '-1' indicate the last one axis which is 'Channel'. Return The concat layer instance Notes The concatenated axis can be different in all input layers passed to this method. Other axes must be same.","title":"Concat()"},{"location":"api_merge/#mult","text":"nnom_layer_t* Mult(int32_t oshift); Element wise mutiplication in all the inputs. This layer cannot use to merge more than 2 layer, which might cause overflowing problem. 2 Mult() must be used separately if willing to multiply 3 layer's output. The output shift individually of the 2 steps must be identify individually. Please check the example below for more than 2 input. Arguments oshift: the output shift of this layer. Return The mult layer instance Notes All input layers passed to this method must have same output shape.","title":"Mult()"},{"location":"api_merge/#add","text":"nnom_layer_t* Add(int32_t oshift); Element wise addition in all the inputs. This layer cannot use to merge more than 2 layer, which might cause overflowing problem. Please refer to Mult() Arguments oshift: the output shift of this layer. Return The add layer instance Notes All input layers passed to this method must have same output shape.","title":"Add()"},{"location":"api_merge/#sub","text":"nnom_layer_t* Sub(int32_t oshift); Element wise substraction in all the inputs. This layer cannot use to merge more than 2 layer, which might cause overflowing problem. Please refer to Mult() Arguments oshift: the output shift of this layer. Return The sub layer instance Notes All input layers passed to this method must have same output shape.","title":"Sub()"},{"location":"api_merge/#example","text":"Channelwise concat for Inception input_layer = Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), nnom_input_data); // conv2d - 1 - inception x1 = model.hook(Conv2D(16, kernel(1, 5), stride(1, 1), PADDING_SAME, &c2_w, &c2_b), x); x1 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x1); // conv2d - 2 - inception x2 = model.hook(Conv2D(16, kernel(1, 3), stride(1, 1), PADDING_SAME, &c3_w, &c3_b), x); x2 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x2); // maxpool - 3 - inception x3 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // concatenate x = model.mergex(Concat(-1), 3, x1, x2, x3); // flatten x = model.hook(Flatten(), x); ... Mult for 3 input (or Add, Sub) In Keras #instead of x = multiply([x1,x2,x3]) # you must use this instead to allow sript to calculate oshift individually. x = multiply([x1,x2]) x = multiply([x,x3]) Then in NNoM // x = x1 * x2 * x3 x = model.merge(Mult(oshift_1), x1, x2); x = model.merge(Mult(oshift_2), x, x3);","title":"Example"},{"location":"api_model/","text":"Model APIs NNoM support Sequential model and Functional model whitch are similar to Keras. Model is a minimum runable object in NNoM. Here list the Model APIs that used for create, compile and run a model. new_model() nnom_model_t *new_model(nnom_model_t *m); This method is to create or initiate a model instance. Arguments m: the model instance that need to be initiated. If NULL is passed to it, the method will create a new model instance. Return The created or the initiated model instance. model_delete() void model_delete(nnom_model_t *m); Delete and free all the resources created with the model. Arguments m: the model instance. sequencial_compile() nnom_status_t sequencial_compile(nnom_model_t *m); Compile a sequencial model which is constructed by sequencial construction APIs. Arguments m: the model instance for compile. Return Status of compiling. model_compile() nnom_status_t model_compile(nnom_model_t *m, nnom_layer_t* input, nnom_layer_t* output); Compile a functional model which is constructed by functional construction APIs. Arguments m: the model instance for compile. input: the specified input layer instance. output: the specified output layer instance. If left NULL , the all layers will be compile. Return Status of compiling. model_run() nnom_status_t model_run(nnom_model_t *m); To run all the layers inside the model. Run one prediction. Arguments m: the model instance for compile. input: the specified input layer instance. output: the specified output layer instance. If left NULL , the all layers will be compile. Return The status of layer running. Note User must fill in the input buffer which has passed to the input layer before run the model. The input layer then copy the data from user space to NNoM memory space to run the model. The results of prediction will be copy from NNoM memory space to user memory space by Output layer. model_run_to() nnom_status_t model_run_to(nnom_model_t *m, nnom_layer_t *end_layer); Same as model_run() but it only run partly to the specified layer. Arguments m: the model instance for compile. end_layer: the layer where to stop. Return The result of layer running. (*layer_callback)() nnom_status_t (*layer_callback)(nnom_model_t *m, nnom_layer_t *layer); This callback is a runtime callback, which can then be used to evaluate the performance, extract the intermediate output etc. It will be called after each layer has ran. (if a actail (activation tail) is present, this callback will be called after the actail) Arguments m: the model instance of the current model. layer: the layer instance which has just been ran. Return The result of the callback. Any result other than NN_SUCCESS will cause the model to return immediately with the error code. NOTE You should not change ANY setting inside the model instance or the layer instance unless you know what you are doing. All configurations and buffers must be read-only inside this method. This is a runtime callback which means it will affect your performance if this callback is time consuming. model_set_callback() nnom_status_t model_set_callback( nnom_model_t *m, nnom_status_t (*layer_callback)(nnom_model_t *m, nnom_layer_t *layer)); Set a callback to model. Please refer to the (*layer_callback)() . If a callback is already set but not the same one as what is given, this method will return error. You need to delete the old callback by model_delete_callback() before setting new callback. Arguments m: the model instance to be set. *layer_callback: the layer callback. Return NN_SUCCESS: when callback is set successfully NN_LENGTH_ERROR: when callback is existed in the model but not the same as the given one. model_delete_callback() void model_delete_callback(nnom_model_t *m); Delete an existing callback on the model. Arguments m: the model instance to delete the callback from. Examples This example shows a 2 layer MPL for MNIST dateset. Input shape 28 x 28 x 1 hand writing image. Please check mnist-densenet example for further reading. Sequential model /* nnom model */ int8_t input_data[784]; int8_t output_data[10]; void sequencial_model(void) { nnom_model_t model; new_model(&model); model.add(&model, Input(shape(784, 1, 1), input_data)); model.add(&model, Flatten()); model.add(&model, Dense(100, &w1, &b1)); model.add(&model, Dense(10, &w2, &b2)); model.add(&model, Softmax()) model.add(&model, Output(shape(10, 1, 1), output_data)); sequencial_compile(&model); while(1) { feed_data(&input_data); model_run(&model); // evaluate on output_data[] ... } } Functional model /* nnom model */ int8_t input_data[784]; int8_t output_data[10]; void functional_model(void) { static nnom_model_t model; nnom_layer_t *input, *x; new_model(&model); input = Input(shape(784, 1, 1), input_data); x = model.hook(Flatten(), input); x = model.hook(Dense(100, &w1, &b1), x) x = model.hook(Dense(10, &w2, &b2), x) x = model.hook(Softmax(), x) x = model.hook(Output(shape(10, 1, 1), output_data), x); // compile these layers into the model. model_compile(&model, input, x); while(1) { feed_data(&input_data); model_run(&model); // evaluate on output_data[] ... } } Layer Callback // this callback to print output size of every layer. nnom_status_t callback(nnom_model_t *m, nnom_layer_t *layer) { printf(\"layer %s, output size %d \\n\", default_layer_names[layer->type], shape_size(&layer->out->shape)); return NN_SUCCESS; } int main(void) { // using automatic tools to generate model model = nnom_model_create(); // set callback model_set_callback(model, callback); // run and see what happend. model_run(model); } Here is the console logging of the callback output: layer Input, output shape 784 layer Conv2D, output shape 9408 layer MaxPool, output shape 2352 layer UpSample, output shape 9408 layer Conv2D, output shape 2352 layer Conv2D, output shape 9408 layer Add, output shape 9408 layer MaxPool, output shape 2352 layer Conv2D, output shape 2352 ...","title":"Model"},{"location":"api_model/#model-apis","text":"NNoM support Sequential model and Functional model whitch are similar to Keras. Model is a minimum runable object in NNoM. Here list the Model APIs that used for create, compile and run a model.","title":"Model APIs"},{"location":"api_model/#new_model","text":"nnom_model_t *new_model(nnom_model_t *m); This method is to create or initiate a model instance. Arguments m: the model instance that need to be initiated. If NULL is passed to it, the method will create a new model instance. Return The created or the initiated model instance.","title":"new_model()"},{"location":"api_model/#model_delete","text":"void model_delete(nnom_model_t *m); Delete and free all the resources created with the model. Arguments m: the model instance.","title":"model_delete()"},{"location":"api_model/#sequencial_compile","text":"nnom_status_t sequencial_compile(nnom_model_t *m); Compile a sequencial model which is constructed by sequencial construction APIs. Arguments m: the model instance for compile. Return Status of compiling.","title":"sequencial_compile()"},{"location":"api_model/#model_compile","text":"nnom_status_t model_compile(nnom_model_t *m, nnom_layer_t* input, nnom_layer_t* output); Compile a functional model which is constructed by functional construction APIs. Arguments m: the model instance for compile. input: the specified input layer instance. output: the specified output layer instance. If left NULL , the all layers will be compile. Return Status of compiling.","title":"model_compile()"},{"location":"api_model/#model_run","text":"nnom_status_t model_run(nnom_model_t *m); To run all the layers inside the model. Run one prediction. Arguments m: the model instance for compile. input: the specified input layer instance. output: the specified output layer instance. If left NULL , the all layers will be compile. Return The status of layer running. Note User must fill in the input buffer which has passed to the input layer before run the model. The input layer then copy the data from user space to NNoM memory space to run the model. The results of prediction will be copy from NNoM memory space to user memory space by Output layer.","title":"model_run()"},{"location":"api_model/#model_run_to","text":"nnom_status_t model_run_to(nnom_model_t *m, nnom_layer_t *end_layer); Same as model_run() but it only run partly to the specified layer. Arguments m: the model instance for compile. end_layer: the layer where to stop. Return The result of layer running.","title":"model_run_to()"},{"location":"api_model/#layer_callback","text":"nnom_status_t (*layer_callback)(nnom_model_t *m, nnom_layer_t *layer); This callback is a runtime callback, which can then be used to evaluate the performance, extract the intermediate output etc. It will be called after each layer has ran. (if a actail (activation tail) is present, this callback will be called after the actail) Arguments m: the model instance of the current model. layer: the layer instance which has just been ran. Return The result of the callback. Any result other than NN_SUCCESS will cause the model to return immediately with the error code. NOTE You should not change ANY setting inside the model instance or the layer instance unless you know what you are doing. All configurations and buffers must be read-only inside this method. This is a runtime callback which means it will affect your performance if this callback is time consuming.","title":"(*layer_callback)()"},{"location":"api_model/#model_set_callback","text":"nnom_status_t model_set_callback( nnom_model_t *m, nnom_status_t (*layer_callback)(nnom_model_t *m, nnom_layer_t *layer)); Set a callback to model. Please refer to the (*layer_callback)() . If a callback is already set but not the same one as what is given, this method will return error. You need to delete the old callback by model_delete_callback() before setting new callback. Arguments m: the model instance to be set. *layer_callback: the layer callback. Return NN_SUCCESS: when callback is set successfully NN_LENGTH_ERROR: when callback is existed in the model but not the same as the given one.","title":"model_set_callback()"},{"location":"api_model/#model_delete_callback","text":"void model_delete_callback(nnom_model_t *m); Delete an existing callback on the model. Arguments m: the model instance to delete the callback from.","title":"model_delete_callback()"},{"location":"api_model/#examples","text":"This example shows a 2 layer MPL for MNIST dateset. Input shape 28 x 28 x 1 hand writing image. Please check mnist-densenet example for further reading. Sequential model /* nnom model */ int8_t input_data[784]; int8_t output_data[10]; void sequencial_model(void) { nnom_model_t model; new_model(&model); model.add(&model, Input(shape(784, 1, 1), input_data)); model.add(&model, Flatten()); model.add(&model, Dense(100, &w1, &b1)); model.add(&model, Dense(10, &w2, &b2)); model.add(&model, Softmax()) model.add(&model, Output(shape(10, 1, 1), output_data)); sequencial_compile(&model); while(1) { feed_data(&input_data); model_run(&model); // evaluate on output_data[] ... } } Functional model /* nnom model */ int8_t input_data[784]; int8_t output_data[10]; void functional_model(void) { static nnom_model_t model; nnom_layer_t *input, *x; new_model(&model); input = Input(shape(784, 1, 1), input_data); x = model.hook(Flatten(), input); x = model.hook(Dense(100, &w1, &b1), x) x = model.hook(Dense(10, &w2, &b2), x) x = model.hook(Softmax(), x) x = model.hook(Output(shape(10, 1, 1), output_data), x); // compile these layers into the model. model_compile(&model, input, x); while(1) { feed_data(&input_data); model_run(&model); // evaluate on output_data[] ... } } Layer Callback // this callback to print output size of every layer. nnom_status_t callback(nnom_model_t *m, nnom_layer_t *layer) { printf(\"layer %s, output size %d \\n\", default_layer_names[layer->type], shape_size(&layer->out->shape)); return NN_SUCCESS; } int main(void) { // using automatic tools to generate model model = nnom_model_create(); // set callback model_set_callback(model, callback); // run and see what happend. model_run(model); } Here is the console logging of the callback output: layer Input, output shape 784 layer Conv2D, output shape 9408 layer MaxPool, output shape 2352 layer UpSample, output shape 9408 layer Conv2D, output shape 2352 layer Conv2D, output shape 9408 layer Add, output shape 9408 layer MaxPool, output shape 2352 layer Conv2D, output shape 2352 ...","title":"Examples"},{"location":"api_nnom_utils/","text":"NNoM Utils NNoM Utils are Python scripts for deploying models. What makes NNoM easy to use is the models can be deployed to MCU automatically or manually with the help of NNoM utils. These functions are located in scripts/nnom_utils.py Tutorial is comming, before it arrives, please refer to examples for usage. API generate_model() generate_model(model, x_test, name='weights.h', format='hwc', kld=True) This is all you need This method is the most frequently used function for deployment. It firsly scans the output range of each layer's output using layers_output_ranges() Then it quantised and write the weights & bias, fused the BatchNorm parameters using generate_weights() Finally, it generate the C functions nnom_model_t* nnom_model_create(void) in weights.h Arguments model: the trained Keras model x_test: the dataset used to check calibrate the output data quantisation range of each layer. name: the name of the automatically generated c file. format: indicate the backend format, options between 'hwc' and 'chw' . See notes kld: True , use KLD method for activation quantisation (saturated). False , use min-max method (nonsaturated). Notes This method might not be updated from time to time with new features in NNoM. Currently, only support single input, single output models. The default backend format is set to 'hwc', also call 'channel last', which is the same format as CMSIS-NN. This format is optimal for CPU. 'chw' format, call 'channel first', is for MCU with hardware AI accelerator (such as Kendryte K210 ). This setting only affects the format in the backend. the frontend will always use 'HWC' for data shape. About activation quantisat method options, check TensorRT notes for detail. layers_output_ranges() layers_output_ranges(model, x_test, kld=True, calibrate_size=1000) This function is to check the output range and generate the output shifting list of each layer. It will automatically distinguish whether a layer can change its output Q format or not. Arguments model: the trained Keras model x_test: the dataset for calibrating quantisation. kld: True , use KLD method for activation quantisation (saturated). False , use min-max method (nonsaturated). calibrate_size: how many data for calibration. TensorRT suggest 1000 is enough. If x_test is longger than this value, it will randomly pick the lenght from the x_test . Return The shifting list. Notes Checking output range of each layer is essential in deploying. It is a part of the quantisation process. generate_weights() generate_weights(model, name='weights.h', format='hwc', shift_list=None) Scans all the layer which includes weights, quantise the weights and put them into the c header. Arguments model: the trained Keras model name: the c file name to store weigths. shift_list: the shift list returned by layers_output_ranges(model, x_test) format: indicate the backend format, options between 'hwc' and 'chw' . See notes in generate_model() Notes Use function individually when willing to use none-supported operation by generate_model() evaluate_model() def evaluate_model(model, x_test, y_test, running_time=False, to_file='evaluation.txt'): Evaluate the model after training. It do running time check, Top-k(k=1,2) accuracy, and confusion matrix. Arguments model: the trained Keras model x_test: the dataset for testing (one hot format) y_test: the label for testing dataset running_time: check running time for one prediction to_file: save above metrics to the file . generate_test_bin() generate_test_bin(x, y, name='test_data_with_label.bin') This is to generate a binary file for MCU side for model validation. The format of the file is shown below. Each batch size 128 started with 128 label, each label has converted from one-hot to number. The 'n' is the size of one data, such as 28x28=784 for mnist, or 32x32x3=3072 for cifar. Label(0~127) Data0 Data1 ... Data127 Label(128) Data128... 128-byte n-byte n-byte ... n-byte 128-byte n-bytes... Arguments x: the quantised dataset for testing y: the label for testing dataset(one hot format) name: the label for testing dataset Output - the binary file for testing on MCU. Notes The data must be quantised to the fixed-point range. For example, MNIST range 0~255 whcih can be converted into 0~1 using ((float)mnist/255) for training. After training, it should be converted back to 0~127 for binary file because MCU only recognised q7 format. fake_clip() fake_clip(frac_bit=0, bit=8) (deprecated) This function is used for inseart a fake_quantitation using tensorflow's method tf.fake_quant_with_min_max_vars() , for similating quantised output of fixed-point model (the deployed model) during training. It should be inserted to the model after every 'conv' or 'dense' layer. However, without simulating the quantisation, most of the models are still performing well. Manually insert these layers will take huge effort for training-configuring cycle result in only a slitly better accuracy. Arguments frac_bit: the fraction bit in Q-format bit: the quantisation bitwidth. Examples x = Conv2D(k, kernel_size=(3, 3), strides=(1,1), padding=\"same\")(x) x = fake_clip(frac_bit=7, bit=8)(x) # quantise range to [-1~1) with 256 level. x = ReLU()(x) fake_clip_min_max() fake_clip_min_max(min=0, max=1, bit=8) (deprecated) a max-min version of fake_clip() , check fake_clip() for details. Arguments min: the min value to be clipped. max: the max value to be clipped. bit: the quantisation bitwidth. Examples This code snips shows training using above functions. Please check examples for real-life utilisation. # load data (x_train, y_train), (x_test, y_test) = mnist.load_data() # convert class vectors to one-hot y_train = to_categorical(y_train, num_classes) y_test = to_categorical(y_test, num_classes) # quantize the range to 0~1 x_test = x_test.astype('float32')/255 x_train = x_train.astype('float32')/255 # (NNoM utils) generate the binary file for testing on MCU. generate_test_bin(x_test*127, y_test, name='test_data.bin') # Train train(x_train,y_train, x_test, y_test, batch_size=128, epochs=epochs) # (NNoM utils) evaluate evaluate_model(model, x_test, y_test) # (NNoM utils) Automatically deploying, use 100 pices for output range generate_model(model, x_test[:100], name=weights)","title":"Utils (Python)"},{"location":"api_nnom_utils/#nnom-utils","text":"NNoM Utils are Python scripts for deploying models. What makes NNoM easy to use is the models can be deployed to MCU automatically or manually with the help of NNoM utils. These functions are located in scripts/nnom_utils.py Tutorial is comming, before it arrives, please refer to examples for usage.","title":"NNoM Utils"},{"location":"api_nnom_utils/#api","text":"","title":"API"},{"location":"api_nnom_utils/#generate_model","text":"generate_model(model, x_test, name='weights.h', format='hwc', kld=True) This is all you need This method is the most frequently used function for deployment. It firsly scans the output range of each layer's output using layers_output_ranges() Then it quantised and write the weights & bias, fused the BatchNorm parameters using generate_weights() Finally, it generate the C functions nnom_model_t* nnom_model_create(void) in weights.h Arguments model: the trained Keras model x_test: the dataset used to check calibrate the output data quantisation range of each layer. name: the name of the automatically generated c file. format: indicate the backend format, options between 'hwc' and 'chw' . See notes kld: True , use KLD method for activation quantisation (saturated). False , use min-max method (nonsaturated). Notes This method might not be updated from time to time with new features in NNoM. Currently, only support single input, single output models. The default backend format is set to 'hwc', also call 'channel last', which is the same format as CMSIS-NN. This format is optimal for CPU. 'chw' format, call 'channel first', is for MCU with hardware AI accelerator (such as Kendryte K210 ). This setting only affects the format in the backend. the frontend will always use 'HWC' for data shape. About activation quantisat method options, check TensorRT notes for detail.","title":"generate_model()"},{"location":"api_nnom_utils/#layers_output_ranges","text":"layers_output_ranges(model, x_test, kld=True, calibrate_size=1000) This function is to check the output range and generate the output shifting list of each layer. It will automatically distinguish whether a layer can change its output Q format or not. Arguments model: the trained Keras model x_test: the dataset for calibrating quantisation. kld: True , use KLD method for activation quantisation (saturated). False , use min-max method (nonsaturated). calibrate_size: how many data for calibration. TensorRT suggest 1000 is enough. If x_test is longger than this value, it will randomly pick the lenght from the x_test . Return The shifting list. Notes Checking output range of each layer is essential in deploying. It is a part of the quantisation process.","title":"layers_output_ranges()"},{"location":"api_nnom_utils/#generate_weights","text":"generate_weights(model, name='weights.h', format='hwc', shift_list=None) Scans all the layer which includes weights, quantise the weights and put them into the c header. Arguments model: the trained Keras model name: the c file name to store weigths. shift_list: the shift list returned by layers_output_ranges(model, x_test) format: indicate the backend format, options between 'hwc' and 'chw' . See notes in generate_model() Notes Use function individually when willing to use none-supported operation by generate_model()","title":"generate_weights()"},{"location":"api_nnom_utils/#evaluate_model","text":"def evaluate_model(model, x_test, y_test, running_time=False, to_file='evaluation.txt'): Evaluate the model after training. It do running time check, Top-k(k=1,2) accuracy, and confusion matrix. Arguments model: the trained Keras model x_test: the dataset for testing (one hot format) y_test: the label for testing dataset running_time: check running time for one prediction to_file: save above metrics to the file .","title":"evaluate_model()"},{"location":"api_nnom_utils/#generate_test_bin","text":"generate_test_bin(x, y, name='test_data_with_label.bin') This is to generate a binary file for MCU side for model validation. The format of the file is shown below. Each batch size 128 started with 128 label, each label has converted from one-hot to number. The 'n' is the size of one data, such as 28x28=784 for mnist, or 32x32x3=3072 for cifar. Label(0~127) Data0 Data1 ... Data127 Label(128) Data128... 128-byte n-byte n-byte ... n-byte 128-byte n-bytes... Arguments x: the quantised dataset for testing y: the label for testing dataset(one hot format) name: the label for testing dataset Output - the binary file for testing on MCU. Notes The data must be quantised to the fixed-point range. For example, MNIST range 0~255 whcih can be converted into 0~1 using ((float)mnist/255) for training. After training, it should be converted back to 0~127 for binary file because MCU only recognised q7 format.","title":"generate_test_bin()"},{"location":"api_nnom_utils/#fake_clip","text":"fake_clip(frac_bit=0, bit=8) (deprecated) This function is used for inseart a fake_quantitation using tensorflow's method tf.fake_quant_with_min_max_vars() , for similating quantised output of fixed-point model (the deployed model) during training. It should be inserted to the model after every 'conv' or 'dense' layer. However, without simulating the quantisation, most of the models are still performing well. Manually insert these layers will take huge effort for training-configuring cycle result in only a slitly better accuracy. Arguments frac_bit: the fraction bit in Q-format bit: the quantisation bitwidth. Examples x = Conv2D(k, kernel_size=(3, 3), strides=(1,1), padding=\"same\")(x) x = fake_clip(frac_bit=7, bit=8)(x) # quantise range to [-1~1) with 256 level. x = ReLU()(x)","title":"fake_clip()"},{"location":"api_nnom_utils/#fake_clip_min_max","text":"fake_clip_min_max(min=0, max=1, bit=8) (deprecated) a max-min version of fake_clip() , check fake_clip() for details. Arguments min: the min value to be clipped. max: the max value to be clipped. bit: the quantisation bitwidth.","title":"fake_clip_min_max()"},{"location":"api_nnom_utils/#examples","text":"This code snips shows training using above functions. Please check examples for real-life utilisation. # load data (x_train, y_train), (x_test, y_test) = mnist.load_data() # convert class vectors to one-hot y_train = to_categorical(y_train, num_classes) y_test = to_categorical(y_test, num_classes) # quantize the range to 0~1 x_test = x_test.astype('float32')/255 x_train = x_train.astype('float32')/255 # (NNoM utils) generate the binary file for testing on MCU. generate_test_bin(x_test*127, y_test, name='test_data.bin') # Train train(x_train,y_train, x_test, y_test, batch_size=128, epochs=epochs) # (NNoM utils) evaluate evaluate_model(model, x_test, y_test) # (NNoM utils) Automatically deploying, use 100 pices for output range generate_model(model, x_test[:100], name=weights)","title":"Examples"},{"location":"api_pooling/","text":"Pooling Layers Pooling Layers are listed in nnom_layers.h 1D/2D operations are both working with (H, W, C) format, known as \"channel last\". When working with 1D operations, the H for all the shapes must be 1 constantly. MaxPool() nnom_layer_t* MaxPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels max pooling. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID Return The layer instance AvgPool() nnom_layer_t* AvgPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels average pooling. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID Return The layer instance Notes Average pooling is not recommended to use with fixed-point model (such as here). Small values will be vanished when the sum is devided. (CMSIS-NN currently does not support changing the output shifting in average pooling from input to output.) However, if the average pooling is the second last layer right before softmax layer, you can still use average pooling for training and then replaced by sumpooling in MCU. the only different is sumpooling (in NNoM) will not divide the sum directly but looking for a best shift (dynamic shifting) to cover the largest number. SumPool() nnom_layer_t* SumPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels Sum pooling. This is a better alternative to average pooling WHEN deploy the trained model into NNoM. The output shift for the sumpool in NNoM is dynamic, means that this pooling can only place before softmax layer. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID Return The layer instance GlobalMaxPool() nnom_layer_t *GlobalMaxPool(void); Global Max Pooling Return The layer instance GlobalAvgPool() nnom_layer_t *GlobalAvgPool(void); Global Average Pooling. Due to the same reason as discussed in Average Pooling, it is recommended to replace this layer by GlobalSumPool() when the layer it she second last layer, and before the softmax layer. If you used generate_model() to deploy your keras model to NNoM, this layer will be automaticly replaced by GlobalSumPool() when above conditions has met. Return The layer instance GlobalSumPool() nnom_layer_t *GlobalSumPool(void); Global sum pooling. Return The layer instance","title":"Pooling Layers"},{"location":"api_pooling/#pooling-layers","text":"Pooling Layers are listed in nnom_layers.h 1D/2D operations are both working with (H, W, C) format, known as \"channel last\". When working with 1D operations, the H for all the shapes must be 1 constantly.","title":"Pooling Layers"},{"location":"api_pooling/#maxpool","text":"nnom_layer_t* MaxPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels max pooling. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID Return The layer instance","title":"MaxPool()"},{"location":"api_pooling/#avgpool","text":"nnom_layer_t* AvgPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels average pooling. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID Return The layer instance Notes Average pooling is not recommended to use with fixed-point model (such as here). Small values will be vanished when the sum is devided. (CMSIS-NN currently does not support changing the output shifting in average pooling from input to output.) However, if the average pooling is the second last layer right before softmax layer, you can still use average pooling for training and then replaced by sumpooling in MCU. the only different is sumpooling (in NNoM) will not divide the sum directly but looking for a best shift (dynamic shifting) to cover the largest number.","title":"AvgPool()"},{"location":"api_pooling/#sumpool","text":"nnom_layer_t* SumPool(nnom_3d_shape_t k, nnom_3d_shape_t s, nnom_padding_t pad); This funtion is for 1D or 2D, mutiple channels Sum pooling. This is a better alternative to average pooling WHEN deploy the trained model into NNoM. The output shift for the sumpool in NNoM is dynamic, means that this pooling can only place before softmax layer. Arguments k (kernel): the kernel shape, which is returned by kernel() s (stride): the stride shape, which is returned by stride() pad (padding): the padding method PADDING_SAME or PADDING_VALID Return The layer instance","title":"SumPool()"},{"location":"api_pooling/#globalmaxpool","text":"nnom_layer_t *GlobalMaxPool(void); Global Max Pooling Return The layer instance","title":"GlobalMaxPool()"},{"location":"api_pooling/#globalavgpool","text":"nnom_layer_t *GlobalAvgPool(void); Global Average Pooling. Due to the same reason as discussed in Average Pooling, it is recommended to replace this layer by GlobalSumPool() when the layer it she second last layer, and before the softmax layer. If you used generate_model() to deploy your keras model to NNoM, this layer will be automaticly replaced by GlobalSumPool() when above conditions has met. Return The layer instance","title":"GlobalAvgPool()"},{"location":"api_pooling/#globalsumpool","text":"nnom_layer_t *GlobalSumPool(void); Global sum pooling. Return The layer instance","title":"GlobalSumPool()"},{"location":"api_properties/","text":"Properties Properties include some basic properties such as shape of the data buffer, Q-format of the data. Typedef #define nnom_shape_data_t uint16_t typedef struct _nnom_shape { nnom_shape_data_t h, w, c; } nnom_3d_shape_t; typedef struct _nnom_weights { const void *p_value; size_t shift; // the right shift for output } nnom_weight_t; typedef struct _nnom_bias { const void *p_value; size_t shift; // the left shift for bias } nnom_bias_t; typedef struct _nnom_qformat { int8_t n, m; } nnom_qformat_t; typedef struct _nnom_border_t { nnom_shape_data_t top, bottom, left, right; } nnom_border_t; Methods shape() nnom_3d_shape_t shape(size_t h, size_t w, size_t c); Arguments h: size of H, or number of row, or y axis in image. w: size of W, or number of column, or x axis in image. c: size of channel. Return A shape instance. kernel() nnom_3d_shape_t kernel(size_t h, size_t w); Use in pooling or convolutional layer to specified the kernel size. Arguments h: size of kernel in H, or number of row, or y axis in image. w: size of kernel in W, or number of column, or x axis in image. Return A shape instance. stride() nnom_3d_shape_t stride(size_t h, size_t w); Use in pooling or convolutional layer to specified the stride size. Arguments h: size of stride in H, or number of row, or y axis in image. w: size of stride in W, or number of column, or x axis in image. Return A shape instance. border() nnom_border_t border(size_t top, size_t bottom, size_t left, size_t right); It pack the 4 padding/cropping value to a border object. Arguments top: the padding/cropping at the top edge of the image. bottom: the padding/cropping at the bottom edge of the image. left: the padding/cropping at the left edge of the image. right: the padding/cropping at the right edge of the image. Return A shape instance. qformat() nnom_qformat_t qformat(int8_t m, int8_t n); Arguments m: the integer bitwidth. n: the fractional bitwidth. Return A nnom_qformat_t inistance. Notes The Q-format within model is currently handled by Python script nnom_utils.py . This function will be deprecated. shape_size() size_t shape_size(nnom_3d_shape_t *s); Calculate the size from a shape. size = s.h * s.w * s.c; Arguments s: the shape to calculate. Return The total size of the shape.","title":"Properties"},{"location":"api_properties/#properties","text":"Properties include some basic properties such as shape of the data buffer, Q-format of the data.","title":"Properties"},{"location":"api_properties/#typedef","text":"#define nnom_shape_data_t uint16_t typedef struct _nnom_shape { nnom_shape_data_t h, w, c; } nnom_3d_shape_t; typedef struct _nnom_weights { const void *p_value; size_t shift; // the right shift for output } nnom_weight_t; typedef struct _nnom_bias { const void *p_value; size_t shift; // the left shift for bias } nnom_bias_t; typedef struct _nnom_qformat { int8_t n, m; } nnom_qformat_t; typedef struct _nnom_border_t { nnom_shape_data_t top, bottom, left, right; } nnom_border_t;","title":"Typedef"},{"location":"api_properties/#methods","text":"","title":"Methods"},{"location":"api_properties/#shape","text":"nnom_3d_shape_t shape(size_t h, size_t w, size_t c); Arguments h: size of H, or number of row, or y axis in image. w: size of W, or number of column, or x axis in image. c: size of channel. Return A shape instance.","title":"shape()"},{"location":"api_properties/#kernel","text":"nnom_3d_shape_t kernel(size_t h, size_t w); Use in pooling or convolutional layer to specified the kernel size. Arguments h: size of kernel in H, or number of row, or y axis in image. w: size of kernel in W, or number of column, or x axis in image. Return A shape instance.","title":"kernel()"},{"location":"api_properties/#stride","text":"nnom_3d_shape_t stride(size_t h, size_t w); Use in pooling or convolutional layer to specified the stride size. Arguments h: size of stride in H, or number of row, or y axis in image. w: size of stride in W, or number of column, or x axis in image. Return A shape instance.","title":"stride()"},{"location":"api_properties/#border","text":"nnom_border_t border(size_t top, size_t bottom, size_t left, size_t right); It pack the 4 padding/cropping value to a border object. Arguments top: the padding/cropping at the top edge of the image. bottom: the padding/cropping at the bottom edge of the image. left: the padding/cropping at the left edge of the image. right: the padding/cropping at the right edge of the image. Return A shape instance.","title":"border()"},{"location":"api_properties/#qformat","text":"nnom_qformat_t qformat(int8_t m, int8_t n); Arguments m: the integer bitwidth. n: the fractional bitwidth. Return A nnom_qformat_t inistance. Notes The Q-format within model is currently handled by Python script nnom_utils.py . This function will be deprecated.","title":"qformat()"},{"location":"api_properties/#shape_size","text":"size_t shape_size(nnom_3d_shape_t *s); Calculate the size from a shape. size = s.h * s.w * s.c; Arguments s: the shape to calculate. Return The total size of the shape.","title":"shape_size()"},{"location":"example_mnist_simple_cn/","text":"MNIST-SIMPLE MNIST \u662f\u4e00\u4e2a\u624b\u5199\u6570\u5b57\u5e93\uff0c\u7531250\u4e2a\u4eba\u7684\u624b\u5199\u6570\u5b57\u7ec4\u6210\u3002\u6bcf\u4e2a\u6570\u5b57\u88ab\u88c1\u526a\u6210 28 * 28 \u7684\u7070\u5ea6\u56fe\u7247\u3002 MNIST \u7ecf\u5e38\u88ab\u7528\u6765\u505a\u4e3a\u5206\u7c7b\u4efb\u52a1\u7684\u5165\u95e8\u6570\u636e\u5e93\u4f7f\u7528\u3002\u5728\u8fd9\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u91cc\u9762\uff0c\u6211\u4eec\u4e5f\u7528\u5b83\u6765\u8bd5\u8bd5\u6570\u636e\u5f52\u7c7b\u3002 \u8fd9\u4e2a\u4f8b\u5b50\u5efa\u9020\u4e86\u4e00\u4e2a\u5377\u79ef\u7f51\u7edc\uff0c\u7ed3\u6784\u5982\u4e0a\u56fe\u3002 \u8fd9\u4e2a\u7f51\u7edc\u4e3b\u8981\u5305\u62ec\u5377\u79ef\u5c42\uff08Convolution layer\uff09\u548c\u5168\u8fde\u63a5\u5c42 (Densely-connected\uff0c\u6216\u8005\u4e5f\u53eb Fully-connected layer). \u6a21\u578b\u4ece\u5de6\u5f80\u53f3\u8fd0\u884c\uff0c\u6700\u5de6\u8fb9\u7684\u662f\u539f\u59cb\u56fe\u50cf\uff0c\u957f\u5bbd28x28\uff0c\u5355\u901a\u9053\u7070\u5ea6\u56fe\u50cf\u3002\u7136\u540e\u7ecf\u8fc73\u4e2a\u5377\u79ef\u5c42\u548c2\u4e2a\u5168\u94fe\u63a5\u5c42\u5b8c\u6210\u5206\u7c7b\u4efb\u52a1\uff08\u8bc6\u522b10\u4e2a\u6570\u5b57\uff09\u3002 \u6bcf\u4e00\u5c42\u901a\u8fc7\u4e00\u5b9a\u5927\u5c0f\u7684\u5377\u79ef\u6838\u4ece\u4e0a\u4e00\u5c42\u63d0\u53d6\u4e00\u5b9a\u7684\u7279\u5f81\u3002 \u8fd9\u4e9b\u7279\u5f81\u6700\u540e\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u9009\u62e9\u540e\uff0c\u4f1a\u9009\u62e9\u6027\u6fc0\u6d3b10\u4e2a\u7c7b\u578b\u4e2d\u67d0\u4e2a\u7c7b\u578b\u7684\u6807\u7b7e\u3002 \u8fd9\u4e9b\u6807\u7b7e\u5bf9\u5e94\u7684\u6570\u503c\u8d8a\u5927\uff0c\u53ef\u80fd\u6027\u8d8a\u5927\u3002 1. \u4e0b\u8f7d\u5e76\u542f\u7528NNoM \u5728 RT-Thread \u7684\u5305\u7ba1\u7406\u4e2d RT-Thread online packages ---> miscellaneous packages ---> [*] NNoM: A Higher-level Nerual Network ... ---> *\u9009\u62e9 latest \u7248\u672c *\u9700\u8981\u6253\u5f00 msh \u652f\u6301 \u6b64\u5916\uff0c\u9ed8\u8ba4\u79fb\u690d\u6587\u4ef6\u548c\u672c\u4f8b\u5b50\u4f9d\u8d56 libc\uff0c \u6240\u4ee5\u9700\u8981\u6253\u5f00 RT-Thread \u7684 libc \u652f\u6301: RT-Thread Components ---> POSIX layer and C standard library ---> [*] Enable libc APIs from toolchain ... ---> \u5982\u679c\u9047\u5230\u95ee\u9898\u6216\u8005\u73af\u5883\u4e0d\u5141\u8bb8\u4f7f\u7528 libc\uff0c\u8bf7\u770b Appendix \u6e90\u7801\u8bf7\u5230 GitHub 2. \u590d\u5236\u4f8b\u5b50\u6587\u4ef6 \u628a packages/nnom-latest/examples/mnist-simple/mcu \u76ee\u5f55\u4e0b\u7684\u4e09\u4e2a\u6587\u4ef6 image.h , weights.h \u548c main.c \u590d\u5236\u5230\u5de5\u7a0b\u76ee\u5f55\u7684 application/ \u3002\u66ff\u6362\u6389\u9ed8\u8ba4\u7684 main.c \u3002\u5148\u4e0d\u7528\u7ba1\u8fd9\u4e09\u4e2a\u6587\u4ef6\u7684\u5185\u5bb9\u3002 \uff08\u5982\u679c\u4f60\u662f\u597d\u5947\u5b9d\u5b9d\uff1a\uff09 image.h \u91cc\u9762\u653e\u7f6e\u4e86 10 \u5f20\u4ece MNIST \u6570\u636e\u96c6\u91cc\u9762\u968f\u673a\u6311\u9009\u7684\u56fe\u7247\u3002 weights.h \u662f NNoM \u7684\u5de5\u5177\u811a\u672c\u81ea\u52a8\u751f\u6210\u7684\u6a21\u578b\u53c2\u6570\u3002 main.c \u5305\u542b\u4e86\u6700\u7b80\u5355\u7684\u6a21\u578b\u521d\u59cb\u5316\u548c msh \u4ea4\u4e92\u547d\u4ee4\u3002 3. \u8dd1\u8d77\u6765 \u7528\u4f60\u559c\u6b22\u7684\u65b9\u5f0f\uff0c\u7f16\u8bd1\uff0c\u4e0b\u8f7d\uff0c\u8fd0\u884c 3.1 \u6a21\u578b\u7f16\u8bd1 RT-Thread \u542f\u52a8\u540e\uff0c\u63a5\u7740\u4f1a\u5728 main() \u51fd\u6570\u91cc\u9762\u8c03\u7528 model = nnom_model_create(); \u3002 \u8fd9\u6761\u8bed\u53e5\u5c06\u4f1a\u8f7d\u5165\u6211\u4eec\u85cf\u5728 weights.h \u91cc\u9762\u7684\u6a21\u578b\uff0c\u5c06\u5b83\u7f16\u8bd1\u5e76\u628a\u4fe1\u606f\u6253\u5370\u51fa\u6765\u3002 \\ | / - RT - Thread Operating System / | \\ 4.0.0 build Mar 29 2019 2006 - 2018 Copyright by rt-thread team RTT Control Block Detection Address is 0x20000a8c msh > INFO: Start compile... Layer Activation output shape ops memory mem life-time ---------------------------------------------------------------------------------------------- Input - - ( 28, 28, 1) 0 ( 784, 784, 0) 1 - - - - - - - Conv2D - ReLU - ( 28, 28, 12) 84672 ( 784, 9408, 432) 1 1 - - - - - - MaxPool - - ( 14, 14, 12) 0 ( 9408, 2352, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 14, 14, 24) 508032 ( 2352, 4704, 864) 1 1 - - - - - - MaxPool - - ( 7, 7, 24) 0 ( 4704, 1176, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 7, 7, 48) 508032 ( 1176, 2352, 1728) 1 1 - - - - - - MaxPool - - ( 4, 4, 48) 0 ( 2352, 768, 0) 1 - 1 - - - - - Dense - ReLU - ( 96, 1, 1) 73728 ( 768, 96, 768) 1 1 - - - - - - Dense - - ( 10, 1, 1) 960 ( 96, 10, 96) 1 - 1 - - - - - Softmax - - ( 10, 1, 1) 0 ( 10, 10, 0) - 1 - - - - - - Output - - ( 10, 1, 1) 0 ( 10, 10, 0) 1 - - - - - - - ---------------------------------------------------------------------------------------------- INFO: memory analysis result Block0: 1728 Block1: 2352 Block2: 9408 Block3: 0 Block4: 0 Block5: 0 Block6: 0 Block7: 0 Total memory cost by network buffers: 13488 bytes \u8fd9\u91cc\u9762\u7684\u4fe1\u606f\u6709\uff1a \u6a21\u578b\u6709\u4e09\u4e2a\u5377\u79ef\u5c42\u7ec4\u6210\uff0c\u6bcf\u4e2a\u5377\u79ef\u5c42\u90fd\u4f7f\u7528 ReLU \u8fdb\u884c\u6fc0\u6d3b \uff08ReLU\uff1a \u5927\u4e8e0\u7684\u6570\u503c\u4e0d\u53d8\uff0c\u5c0f\u4e8e0\u7684\u6570\u503c\u91cd\u65b0\u8d4b\u503c\u4e3a0\uff09\u3002 \u4e09\u4e2a\u5377\u79ef\u540e\u9762\u8ddf\u7740\u4e24\u4e2a Dense \u5c42 \uff08Densely-connected\uff0c\u6216\u8005\u4e5f\u53eb fully-connected \u5168\u8fde\u63a5\u5c42\uff09\u3002 \u6700\u540e\u6a21\u578b\u901a\u8fc7 Softmax \u5c42\u6765\u8f93\u51fa \uff08\u5c06\u6570\u503c\u8f6c\u6362\u6210\u6982\u7387\u503c\uff09 \u5404\u5c42\u7684\u5185\u5b58\u4fe1\u606f\uff0c\u8f93\u51fa\u7684\u6570\u636e\uff0c\u8ba1\u7b97\u91cf \uff08\u5b9a\u70b9\u4e58\u52a0\u64cd\u4f5c\uff1aMAC-OPS\uff09 \u603b\u7f51\u7edc\u5185\u5b58\u5360\u7528 13488 bytes 3.2 \u8dd1\u4e2a\u6a21\u578b \u4e4b\u524d\u6211\u4eec\u4ecb\u7ecd\u8fc7 image.h \u91cc\u9762\u85cf\u6709\u5341\u5f20\u56fe\u7247\u3002\u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7 mnist \u8fd9\u4e2a\u81ea\u5b9a\u4e49\u7684 MSH \u547d\u4ee4\u6765\u9884\u6d4b\u4e00\u4e0b\u8fd9\u5341\u5f20\u56fe\u3002 \u547d\u4ee4\u4f7f\u7528\u65b9\u6cd5\u5982\u4e0b, num \u662f 0~9 \u91cc\u9762\u7684\u4efb\u610f\u6570\u5b57\u3002\u4ee3\u8868\u5341\u5f20\u56fe\u7247\u91cc\u9762\u7684\u7b2c\u51e0\u4e2a\u56fe\u7247\uff08\u6ce8\u610f\uff1a\u8f93\u5165\u7684\u6570\u5b57\u5e76\u975e\u6307\u56fe\u7247\u7684\u6570\u5b57\uff0c\u56fe\u7247\u662f\u968f\u673a\u62c9\u53d6\u7684\uff09\u3002 mnist num \u6211\u8f93\u5165\u4e86 msh >mnist 6 \uff0c\u6211\u8981\u6d4b\u8bd5\u7b2c\u516d\u5f20\u56fe\u7247\u3002 msh >mnist 6 prediction start.. ..]] ((ZZOO))^^ ``//qq&&)) kkBB@@@@ff \"\">>\\\\pp%%ZZ,, [[%%@@BB11 ^^}}MM@@@@oo{{ rr@@@@OO<< nn@@@@aajj.. ++dd@@88nn'' \\\\%%@@hh!! ++88@@oo:: !!%%@@kk>> ;;88@@oo:: ))@@@@<< ^^pp@@oo:: ::oo@@WWzzll!!bb@@bb'' ttBB@@@@%%WW@@**,, ll}}LL%%@@@@@@bbtt'' ``&&@@MMCC&&%%hh[[ ((@@@@(( II**@@nn'' ??@@##`` QQ@@>> ((@@@@^^ [[@@pp [[@@@@^^ nn@@jj ..aa@@[[ ZZ%%++ __@@**,, xx@@OO {{&&**jj00@@aa:: ^^YYpppp||,, Time: 62 tick Truth label: 8 Predicted label: 8 Probability: 100% \u989d\uff0c\u5982\u679c\u6076\u5fc3\u5230\u4f60\u4e86\uff0c\u90a3\u6211\u9053\u6b49... \u4e0d\u8981\u6000\u7591\uff0c\u4e0a\u9762\u90a3\u4e00\u5768\u662f ASCII \u7801\u8868\u793a\u7684 28 * 28 \u7684\u624b\u5199\u56fe\u7247... \u8f93\u51fa\u7684\u4fe1\u606f\u91cc\u9762\u8bb0\u5f55\u4e86 \u6b64\u6b21\u9884\u6d4b\u7684\u65f6\u95f4\uff0c\u8fd9\u91cc\u7528\u4e86 62 tick \uff0c\u6211\u8fd9\u662f\u76f8\u5f53\u4e8e 62ms \u8fd9\u5f20\u56fe\u7247\u7684\u771f\u5b9e\u6570\u5b57\u662f 8 \u7f51\u7edc\u8ba1\u7b97\u7684\u8fd9\u5f20\u7167\u7247\u7684\u6570\u5b57 8 \u53ef\u80fd\u6027\u662f100% \u8d76\u5feb\u53bb\u8bd5\u8bd5\uff0c\u5176\u4ed6\u7684 9 \u5f20\u56fe\u7247\u5427\u3002 \u7b80\u5355\u7684\u4f53\u9a8c\u5c31\u5230\u8fd9\u3002 4 \u5efa\u7acb\u81ea\u5df1\u7684\u6a21\u578b \u5bf9\u4e8e\u6ca1\u6709\u673a\u5668\u5b66\u4e60\u57fa\u7840\u7684\u540c\u5b66\uff0c\u60f3\u8981\u5728 MCU \u4e0a\u8dd1\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u9700\u8981\u5148\u5b66\u4f1a\u5728 Keras \u91cc\u9762\u5efa\u7acb\u4e00\u4e2a\u6a21\u578b\u3002 \u5728\u8fd9\u91cc\u53ef\u4ee5\u53c2\u7167\u7f51\u7edc\u4e0a Keras \u7684\u6559\u7a0b\u6765\u4fee\u6539\u8fd9\u4e2a\u4f8b\u5b50\u91cc\u9762\u7684\u6a21\u578b\u3002 \u8fd9\u4e2a\u4f8b\u5b50\u7684\u6a21\u578b\u5728 nnom/example/mnist-simple/model \u91cc\u9762\u7684 mnist_simple.py \uff0c\u8bf7\u81ea\u884c\u5b9e\u8df5\u3002 *\u9700\u8981\u628a nnom/scripts \u4e0b\u7684\u51e0\u4e2a python \u811a\u672c\u6587\u4ef6\u590d\u5236\u5230\u4ee5\u4e0a\u76ee\u5f55\u3002 \u73af\u5883\u662f Python3 + Keras + Tensorflow\u3002\u63a8\u8350\u4f7f\u7528 Anaconda \u6765\u5b89\u88c5 python \u73af\u5883\u800c\u4e0d\u662f pip\u3002 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u4f1a\u751f\u6210 weights.h \u8fd8\u4f1a\u751f\u6210\u968f\u673a\u56fe\u7247\u6587\u4ef6 image.h \u3002 \u63a5\u4e0b\u6765\u6309\u7167\u4e0a\u9762\u7684\u64cd\u4f5c\u4ece\u5934\u6765\u4e00\u904d\u5c31\u597d\u3002 5 \u7ed3\u8bed \u4f7f\u7528 NNoM \u6765\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u771f\u7684\u5f88\u7b80\u5355\u3002\u57fa\u7840\u7684\u4ee3\u7801\u4e0d\u8fc7\u4e24\u4e09\u884c\uff0cNNoM \u80fd\u8f7b\u677e\u8ba9\u4f60\u7684 MCU \u4e5f\u795e\u7ecf\u4e00\u628a~ \u53d8\u6210\u771f\u6b63\u7684 Edge AI \u8bbe\u5907\u3002 \u8fd9\u4e2a\u4f8b\u5b50\u4ec5\u4f7f\u7528\u4e86\u6700\u7b80\u5355\u7684 API \u642d\u5efa\u4e86\u6700\u57fa\u7840\u7684\u5377\u79ef\u6a21\u578b\u3002 \u9ad8\u7ea7\u7528\u6cd5\u548c\u66f4\u591a\u4f8b\u5b50\u8bf7\u67e5\u770b API \u6587\u6863 \u548c \u5176\u4ed6\u4f8b\u5b50 Appendix \u5982\u679c\u65e0\u6cd5\u6253\u5f00 libc \u652f\u6301\uff0c\u6216\u8005\u4f60\u7684\u5de5\u7a0b\u4e0d\u5141\u8bb8\u6253\u5f00 libc \u652f\u6301\uff0c\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u79fb\u690d\u5b8c\u5168\u53bb\u6389 nnom \u5bf9 libc \u4f9d\u8d56\u3002 \u9996\u5148\u628a application/main.c \u91cc\u9762\u6240\u6709\u7684 printf \u66ff\u6362\u6210 RTT \u5bf9\u5e94\u7684\u7248\u672c rt_kprinf . \u7136\u540e\uff0c\u4fee\u6539\u79fb\u690d\u6587\u4ef6 packages/nnom-latest/port \uff0c \u628a\u91cc\u9762\u7684c\u6807\u51c6\u63a5\u53e3\u6539\u4e3artt\u5bf9\u5e94\u7684\u63a5\u53e3\u3002\uff08\u4e5f\u5c31\u662f\u5728 malloc(), free(), memset(), printf() \u524d\u9762\u52a0\u4e0a rt_ \u524d\u7f00\u3002\u8bb0\u5f97\u5728\u4ed6\u4eec\u4e4b\u524d\u52a0\u5165rtt\u7684\u5934\u6587\u4ef6 rtthread.h \u3002\uff09 \u89c9\u5f97\u9ebb\u70e6\u53ef\u4ee5\u76f4\u63a5\u590d\u5236\u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u66ff\u6362\u6389\u5bf9\u5e94\u7684\u90e8\u5206\u3002 #include \"rtthread.h\" // memory interfaces #define nnom_malloc(n) rt_malloc(n) #define nnom_free(p) rt_free(p) #define nnom_memset(p,v,s) rt_memset(p,v,s) // runtime & debuges #define nnom_us_get() 0 #define nnom_ms_get() 0 #define NNOM_LOG(...) rt_kprintf(__VA_ARGS__) \u5230\u6b64\uff0c\u8fd9\u4e2a\u4f8b\u5b50\u548c nnom \u5c31\u5b8c\u5168\u8131\u79bb libc \u4f9d\u8d56\u5566\u3002\u73b0\u5728\u53ef\u4ee5\u5c1d\u8bd5\u91cd\u65b0\u7f16\u8bd1\u8fd0\u884c\u4e86\u3002","title":"MNIST-Simple"},{"location":"example_mnist_simple_cn/#mnist-simple","text":"MNIST \u662f\u4e00\u4e2a\u624b\u5199\u6570\u5b57\u5e93\uff0c\u7531250\u4e2a\u4eba\u7684\u624b\u5199\u6570\u5b57\u7ec4\u6210\u3002\u6bcf\u4e2a\u6570\u5b57\u88ab\u88c1\u526a\u6210 28 * 28 \u7684\u7070\u5ea6\u56fe\u7247\u3002 MNIST \u7ecf\u5e38\u88ab\u7528\u6765\u505a\u4e3a\u5206\u7c7b\u4efb\u52a1\u7684\u5165\u95e8\u6570\u636e\u5e93\u4f7f\u7528\u3002\u5728\u8fd9\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\u91cc\u9762\uff0c\u6211\u4eec\u4e5f\u7528\u5b83\u6765\u8bd5\u8bd5\u6570\u636e\u5f52\u7c7b\u3002 \u8fd9\u4e2a\u4f8b\u5b50\u5efa\u9020\u4e86\u4e00\u4e2a\u5377\u79ef\u7f51\u7edc\uff0c\u7ed3\u6784\u5982\u4e0a\u56fe\u3002 \u8fd9\u4e2a\u7f51\u7edc\u4e3b\u8981\u5305\u62ec\u5377\u79ef\u5c42\uff08Convolution layer\uff09\u548c\u5168\u8fde\u63a5\u5c42 (Densely-connected\uff0c\u6216\u8005\u4e5f\u53eb Fully-connected layer). \u6a21\u578b\u4ece\u5de6\u5f80\u53f3\u8fd0\u884c\uff0c\u6700\u5de6\u8fb9\u7684\u662f\u539f\u59cb\u56fe\u50cf\uff0c\u957f\u5bbd28x28\uff0c\u5355\u901a\u9053\u7070\u5ea6\u56fe\u50cf\u3002\u7136\u540e\u7ecf\u8fc73\u4e2a\u5377\u79ef\u5c42\u548c2\u4e2a\u5168\u94fe\u63a5\u5c42\u5b8c\u6210\u5206\u7c7b\u4efb\u52a1\uff08\u8bc6\u522b10\u4e2a\u6570\u5b57\uff09\u3002 \u6bcf\u4e00\u5c42\u901a\u8fc7\u4e00\u5b9a\u5927\u5c0f\u7684\u5377\u79ef\u6838\u4ece\u4e0a\u4e00\u5c42\u63d0\u53d6\u4e00\u5b9a\u7684\u7279\u5f81\u3002 \u8fd9\u4e9b\u7279\u5f81\u6700\u540e\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u9009\u62e9\u540e\uff0c\u4f1a\u9009\u62e9\u6027\u6fc0\u6d3b10\u4e2a\u7c7b\u578b\u4e2d\u67d0\u4e2a\u7c7b\u578b\u7684\u6807\u7b7e\u3002 \u8fd9\u4e9b\u6807\u7b7e\u5bf9\u5e94\u7684\u6570\u503c\u8d8a\u5927\uff0c\u53ef\u80fd\u6027\u8d8a\u5927\u3002","title":"MNIST-SIMPLE"},{"location":"example_mnist_simple_cn/#1-nnom","text":"\u5728 RT-Thread \u7684\u5305\u7ba1\u7406\u4e2d RT-Thread online packages ---> miscellaneous packages ---> [*] NNoM: A Higher-level Nerual Network ... ---> *\u9009\u62e9 latest \u7248\u672c *\u9700\u8981\u6253\u5f00 msh \u652f\u6301 \u6b64\u5916\uff0c\u9ed8\u8ba4\u79fb\u690d\u6587\u4ef6\u548c\u672c\u4f8b\u5b50\u4f9d\u8d56 libc\uff0c \u6240\u4ee5\u9700\u8981\u6253\u5f00 RT-Thread \u7684 libc \u652f\u6301: RT-Thread Components ---> POSIX layer and C standard library ---> [*] Enable libc APIs from toolchain ... ---> \u5982\u679c\u9047\u5230\u95ee\u9898\u6216\u8005\u73af\u5883\u4e0d\u5141\u8bb8\u4f7f\u7528 libc\uff0c\u8bf7\u770b Appendix \u6e90\u7801\u8bf7\u5230 GitHub","title":"1. \u4e0b\u8f7d\u5e76\u542f\u7528NNoM"},{"location":"example_mnist_simple_cn/#2","text":"\u628a packages/nnom-latest/examples/mnist-simple/mcu \u76ee\u5f55\u4e0b\u7684\u4e09\u4e2a\u6587\u4ef6 image.h , weights.h \u548c main.c \u590d\u5236\u5230\u5de5\u7a0b\u76ee\u5f55\u7684 application/ \u3002\u66ff\u6362\u6389\u9ed8\u8ba4\u7684 main.c \u3002\u5148\u4e0d\u7528\u7ba1\u8fd9\u4e09\u4e2a\u6587\u4ef6\u7684\u5185\u5bb9\u3002 \uff08\u5982\u679c\u4f60\u662f\u597d\u5947\u5b9d\u5b9d\uff1a\uff09 image.h \u91cc\u9762\u653e\u7f6e\u4e86 10 \u5f20\u4ece MNIST \u6570\u636e\u96c6\u91cc\u9762\u968f\u673a\u6311\u9009\u7684\u56fe\u7247\u3002 weights.h \u662f NNoM \u7684\u5de5\u5177\u811a\u672c\u81ea\u52a8\u751f\u6210\u7684\u6a21\u578b\u53c2\u6570\u3002 main.c \u5305\u542b\u4e86\u6700\u7b80\u5355\u7684\u6a21\u578b\u521d\u59cb\u5316\u548c msh \u4ea4\u4e92\u547d\u4ee4\u3002","title":"2. \u590d\u5236\u4f8b\u5b50\u6587\u4ef6"},{"location":"example_mnist_simple_cn/#3","text":"\u7528\u4f60\u559c\u6b22\u7684\u65b9\u5f0f\uff0c\u7f16\u8bd1\uff0c\u4e0b\u8f7d\uff0c\u8fd0\u884c","title":"3. \u8dd1\u8d77\u6765"},{"location":"example_mnist_simple_cn/#31","text":"RT-Thread \u542f\u52a8\u540e\uff0c\u63a5\u7740\u4f1a\u5728 main() \u51fd\u6570\u91cc\u9762\u8c03\u7528 model = nnom_model_create(); \u3002 \u8fd9\u6761\u8bed\u53e5\u5c06\u4f1a\u8f7d\u5165\u6211\u4eec\u85cf\u5728 weights.h \u91cc\u9762\u7684\u6a21\u578b\uff0c\u5c06\u5b83\u7f16\u8bd1\u5e76\u628a\u4fe1\u606f\u6253\u5370\u51fa\u6765\u3002 \\ | / - RT - Thread Operating System / | \\ 4.0.0 build Mar 29 2019 2006 - 2018 Copyright by rt-thread team RTT Control Block Detection Address is 0x20000a8c msh > INFO: Start compile... Layer Activation output shape ops memory mem life-time ---------------------------------------------------------------------------------------------- Input - - ( 28, 28, 1) 0 ( 784, 784, 0) 1 - - - - - - - Conv2D - ReLU - ( 28, 28, 12) 84672 ( 784, 9408, 432) 1 1 - - - - - - MaxPool - - ( 14, 14, 12) 0 ( 9408, 2352, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 14, 14, 24) 508032 ( 2352, 4704, 864) 1 1 - - - - - - MaxPool - - ( 7, 7, 24) 0 ( 4704, 1176, 0) 1 - 1 - - - - - Conv2D - ReLU - ( 7, 7, 48) 508032 ( 1176, 2352, 1728) 1 1 - - - - - - MaxPool - - ( 4, 4, 48) 0 ( 2352, 768, 0) 1 - 1 - - - - - Dense - ReLU - ( 96, 1, 1) 73728 ( 768, 96, 768) 1 1 - - - - - - Dense - - ( 10, 1, 1) 960 ( 96, 10, 96) 1 - 1 - - - - - Softmax - - ( 10, 1, 1) 0 ( 10, 10, 0) - 1 - - - - - - Output - - ( 10, 1, 1) 0 ( 10, 10, 0) 1 - - - - - - - ---------------------------------------------------------------------------------------------- INFO: memory analysis result Block0: 1728 Block1: 2352 Block2: 9408 Block3: 0 Block4: 0 Block5: 0 Block6: 0 Block7: 0 Total memory cost by network buffers: 13488 bytes \u8fd9\u91cc\u9762\u7684\u4fe1\u606f\u6709\uff1a \u6a21\u578b\u6709\u4e09\u4e2a\u5377\u79ef\u5c42\u7ec4\u6210\uff0c\u6bcf\u4e2a\u5377\u79ef\u5c42\u90fd\u4f7f\u7528 ReLU \u8fdb\u884c\u6fc0\u6d3b \uff08ReLU\uff1a \u5927\u4e8e0\u7684\u6570\u503c\u4e0d\u53d8\uff0c\u5c0f\u4e8e0\u7684\u6570\u503c\u91cd\u65b0\u8d4b\u503c\u4e3a0\uff09\u3002 \u4e09\u4e2a\u5377\u79ef\u540e\u9762\u8ddf\u7740\u4e24\u4e2a Dense \u5c42 \uff08Densely-connected\uff0c\u6216\u8005\u4e5f\u53eb fully-connected \u5168\u8fde\u63a5\u5c42\uff09\u3002 \u6700\u540e\u6a21\u578b\u901a\u8fc7 Softmax \u5c42\u6765\u8f93\u51fa \uff08\u5c06\u6570\u503c\u8f6c\u6362\u6210\u6982\u7387\u503c\uff09 \u5404\u5c42\u7684\u5185\u5b58\u4fe1\u606f\uff0c\u8f93\u51fa\u7684\u6570\u636e\uff0c\u8ba1\u7b97\u91cf \uff08\u5b9a\u70b9\u4e58\u52a0\u64cd\u4f5c\uff1aMAC-OPS\uff09 \u603b\u7f51\u7edc\u5185\u5b58\u5360\u7528 13488 bytes","title":"3.1 \u6a21\u578b\u7f16\u8bd1"},{"location":"example_mnist_simple_cn/#32","text":"\u4e4b\u524d\u6211\u4eec\u4ecb\u7ecd\u8fc7 image.h \u91cc\u9762\u85cf\u6709\u5341\u5f20\u56fe\u7247\u3002\u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7 mnist \u8fd9\u4e2a\u81ea\u5b9a\u4e49\u7684 MSH \u547d\u4ee4\u6765\u9884\u6d4b\u4e00\u4e0b\u8fd9\u5341\u5f20\u56fe\u3002 \u547d\u4ee4\u4f7f\u7528\u65b9\u6cd5\u5982\u4e0b, num \u662f 0~9 \u91cc\u9762\u7684\u4efb\u610f\u6570\u5b57\u3002\u4ee3\u8868\u5341\u5f20\u56fe\u7247\u91cc\u9762\u7684\u7b2c\u51e0\u4e2a\u56fe\u7247\uff08\u6ce8\u610f\uff1a\u8f93\u5165\u7684\u6570\u5b57\u5e76\u975e\u6307\u56fe\u7247\u7684\u6570\u5b57\uff0c\u56fe\u7247\u662f\u968f\u673a\u62c9\u53d6\u7684\uff09\u3002 mnist num \u6211\u8f93\u5165\u4e86 msh >mnist 6 \uff0c\u6211\u8981\u6d4b\u8bd5\u7b2c\u516d\u5f20\u56fe\u7247\u3002 msh >mnist 6 prediction start.. ..]] ((ZZOO))^^ ``//qq&&)) kkBB@@@@ff \"\">>\\\\pp%%ZZ,, [[%%@@BB11 ^^}}MM@@@@oo{{ rr@@@@OO<< nn@@@@aajj.. ++dd@@88nn'' \\\\%%@@hh!! ++88@@oo:: !!%%@@kk>> ;;88@@oo:: ))@@@@<< ^^pp@@oo:: ::oo@@WWzzll!!bb@@bb'' ttBB@@@@%%WW@@**,, ll}}LL%%@@@@@@bbtt'' ``&&@@MMCC&&%%hh[[ ((@@@@(( II**@@nn'' ??@@##`` QQ@@>> ((@@@@^^ [[@@pp [[@@@@^^ nn@@jj ..aa@@[[ ZZ%%++ __@@**,, xx@@OO {{&&**jj00@@aa:: ^^YYpppp||,, Time: 62 tick Truth label: 8 Predicted label: 8 Probability: 100% \u989d\uff0c\u5982\u679c\u6076\u5fc3\u5230\u4f60\u4e86\uff0c\u90a3\u6211\u9053\u6b49... \u4e0d\u8981\u6000\u7591\uff0c\u4e0a\u9762\u90a3\u4e00\u5768\u662f ASCII \u7801\u8868\u793a\u7684 28 * 28 \u7684\u624b\u5199\u56fe\u7247... \u8f93\u51fa\u7684\u4fe1\u606f\u91cc\u9762\u8bb0\u5f55\u4e86 \u6b64\u6b21\u9884\u6d4b\u7684\u65f6\u95f4\uff0c\u8fd9\u91cc\u7528\u4e86 62 tick \uff0c\u6211\u8fd9\u662f\u76f8\u5f53\u4e8e 62ms \u8fd9\u5f20\u56fe\u7247\u7684\u771f\u5b9e\u6570\u5b57\u662f 8 \u7f51\u7edc\u8ba1\u7b97\u7684\u8fd9\u5f20\u7167\u7247\u7684\u6570\u5b57 8 \u53ef\u80fd\u6027\u662f100% \u8d76\u5feb\u53bb\u8bd5\u8bd5\uff0c\u5176\u4ed6\u7684 9 \u5f20\u56fe\u7247\u5427\u3002 \u7b80\u5355\u7684\u4f53\u9a8c\u5c31\u5230\u8fd9\u3002","title":"3.2 \u8dd1\u4e2a\u6a21\u578b"},{"location":"example_mnist_simple_cn/#4","text":"\u5bf9\u4e8e\u6ca1\u6709\u673a\u5668\u5b66\u4e60\u57fa\u7840\u7684\u540c\u5b66\uff0c\u60f3\u8981\u5728 MCU \u4e0a\u8dd1\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u9700\u8981\u5148\u5b66\u4f1a\u5728 Keras \u91cc\u9762\u5efa\u7acb\u4e00\u4e2a\u6a21\u578b\u3002 \u5728\u8fd9\u91cc\u53ef\u4ee5\u53c2\u7167\u7f51\u7edc\u4e0a Keras \u7684\u6559\u7a0b\u6765\u4fee\u6539\u8fd9\u4e2a\u4f8b\u5b50\u91cc\u9762\u7684\u6a21\u578b\u3002 \u8fd9\u4e2a\u4f8b\u5b50\u7684\u6a21\u578b\u5728 nnom/example/mnist-simple/model \u91cc\u9762\u7684 mnist_simple.py \uff0c\u8bf7\u81ea\u884c\u5b9e\u8df5\u3002 *\u9700\u8981\u628a nnom/scripts \u4e0b\u7684\u51e0\u4e2a python \u811a\u672c\u6587\u4ef6\u590d\u5236\u5230\u4ee5\u4e0a\u76ee\u5f55\u3002 \u73af\u5883\u662f Python3 + Keras + Tensorflow\u3002\u63a8\u8350\u4f7f\u7528 Anaconda \u6765\u5b89\u88c5 python \u73af\u5883\u800c\u4e0d\u662f pip\u3002 \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u4f1a\u751f\u6210 weights.h \u8fd8\u4f1a\u751f\u6210\u968f\u673a\u56fe\u7247\u6587\u4ef6 image.h \u3002 \u63a5\u4e0b\u6765\u6309\u7167\u4e0a\u9762\u7684\u64cd\u4f5c\u4ece\u5934\u6765\u4e00\u904d\u5c31\u597d\u3002","title":"4 \u5efa\u7acb\u81ea\u5df1\u7684\u6a21\u578b"},{"location":"example_mnist_simple_cn/#5","text":"\u4f7f\u7528 NNoM \u6765\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u771f\u7684\u5f88\u7b80\u5355\u3002\u57fa\u7840\u7684\u4ee3\u7801\u4e0d\u8fc7\u4e24\u4e09\u884c\uff0cNNoM \u80fd\u8f7b\u677e\u8ba9\u4f60\u7684 MCU \u4e5f\u795e\u7ecf\u4e00\u628a~ \u53d8\u6210\u771f\u6b63\u7684 Edge AI \u8bbe\u5907\u3002 \u8fd9\u4e2a\u4f8b\u5b50\u4ec5\u4f7f\u7528\u4e86\u6700\u7b80\u5355\u7684 API \u642d\u5efa\u4e86\u6700\u57fa\u7840\u7684\u5377\u79ef\u6a21\u578b\u3002 \u9ad8\u7ea7\u7528\u6cd5\u548c\u66f4\u591a\u4f8b\u5b50\u8bf7\u67e5\u770b API \u6587\u6863 \u548c \u5176\u4ed6\u4f8b\u5b50","title":"5 \u7ed3\u8bed"},{"location":"example_mnist_simple_cn/#appendix","text":"\u5982\u679c\u65e0\u6cd5\u6253\u5f00 libc \u652f\u6301\uff0c\u6216\u8005\u4f60\u7684\u5de5\u7a0b\u4e0d\u5141\u8bb8\u6253\u5f00 libc \u652f\u6301\uff0c\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u79fb\u690d\u5b8c\u5168\u53bb\u6389 nnom \u5bf9 libc \u4f9d\u8d56\u3002 \u9996\u5148\u628a application/main.c \u91cc\u9762\u6240\u6709\u7684 printf \u66ff\u6362\u6210 RTT \u5bf9\u5e94\u7684\u7248\u672c rt_kprinf . \u7136\u540e\uff0c\u4fee\u6539\u79fb\u690d\u6587\u4ef6 packages/nnom-latest/port \uff0c \u628a\u91cc\u9762\u7684c\u6807\u51c6\u63a5\u53e3\u6539\u4e3artt\u5bf9\u5e94\u7684\u63a5\u53e3\u3002\uff08\u4e5f\u5c31\u662f\u5728 malloc(), free(), memset(), printf() \u524d\u9762\u52a0\u4e0a rt_ \u524d\u7f00\u3002\u8bb0\u5f97\u5728\u4ed6\u4eec\u4e4b\u524d\u52a0\u5165rtt\u7684\u5934\u6587\u4ef6 rtthread.h \u3002\uff09 \u89c9\u5f97\u9ebb\u70e6\u53ef\u4ee5\u76f4\u63a5\u590d\u5236\u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u66ff\u6362\u6389\u5bf9\u5e94\u7684\u90e8\u5206\u3002 #include \"rtthread.h\" // memory interfaces #define nnom_malloc(n) rt_malloc(n) #define nnom_free(p) rt_free(p) #define nnom_memset(p,v,s) rt_memset(p,v,s) // runtime & debuges #define nnom_us_get() 0 #define nnom_ms_get() 0 #define NNOM_LOG(...) rt_kprintf(__VA_ARGS__) \u5230\u6b64\uff0c\u8fd9\u4e2a\u4f8b\u5b50\u548c nnom \u5c31\u5b8c\u5168\u8131\u79bb libc \u4f9d\u8d56\u5566\u3002\u73b0\u5728\u53ef\u4ee5\u5c1d\u8bd5\u91cd\u65b0\u7f16\u8bd1\u8fd0\u884c\u4e86\u3002","title":"Appendix"},{"location":"guide_5_min_to_nnom/","text":"5 min Guide to NNoM The aim for NNoM is to help Embedded Engineers to develop and deploy Neural Network models onto the MCUs . NNoM is working closely with Keras . If you dont know Keras yet Getting started: 30 seconds to Keras This guide will show you how to use NNoM for your very first step from an embedded engineer perspective. Backgrouds Checking You should: know C language and your target MCU enviroment. know a bit of python. You must NOT : be a pro in TensorFlow / lite :-) Neural Network with Keras If you know nothing about Keras, you must check Getting started: 30 seconds to Keras first. Lets say if we want to classify the MNIST hand writing dataset. This is what you normally do with Keras. model = Sequential() model.add(Dense(32, input_dim=784)) model.add(Activation('relu')) model.add(Dense(10)) Each operation in Keras are defined by \"Layer\", same as we did in NNoM. The terms are different from Tensorflow (That is why you must not be a PRO in Tensorflow >_<). This model is with an input dimension 784, a hidden fully connected layer including 32 units and outputing 10 units(which is the number of classification(number 0~9)). The hidden layer is activated by ReLU activation (which keep all possitive values while set all nagtive values to 0). After you have trained this model using the method in the Keras' guide, the model can now do prediction. If you feed new image to it, it will tell you what is the wrtten number. Please try to run a example in Keras or NNoM if you are still confusing. Deployed using NNoM After the model is trained, the weights and parameters are already functional. We can now convert it to C language files then put it in your MCU project. The result of this step is a single weights.h file, which contains everything you need. To conver the model, NNoM has provided an simple API generate_model() API to automaticly do the job. Simply pass the model and the test dataset to it. It will do all the magics for you. generate_model(model, x_test, name='weights.h') When the conversion is finished, you will find a new weights.h under your working folder. Simply copy the file to your MCU project, and call model = nnom_model_create(); inside you main() . Below is what you should do in practice. #include \"nnom.h\" #include \"weights.h\" int main(void) { nnom_model_t *model; model = nnom_model_create(); model_run(model); } Then, your model is now running on you MCU. If you have supported printf on your MCU, you should see the compiling info on your consoles. Compiling logging similar to this: Start compiling model... Layer(#) Activation output shape ops(MAC) mem(in, out, buf) mem blk lifetime ------------------------------------------------------------------------------------------------- #1 Input - - ( 28, 28, 1) ( 784, 784, 0) 1 - - - - - - - #2 Conv2D - ReLU - ( 28, 28, 12) 84k ( 784, 9408, 36) 1 1 3 - - - - - #3 MaxPool - - ( 14, 14, 12) ( 9408, 2352, 0) 1 2 3 - - - - - #4 UpSample - - ( 28, 28, 12) ( 2352, 9408, 0) 1 2 2 - - - - - #5 Conv2D - - ( 14, 14, 12) 254k ( 2352, 2352, 432) 1 1 2 1 1 - - - #6 Conv2D - - ( 28, 28, 12) 1.01M ( 9408, 9408, 432) 1 1 2 1 1 - - - #7 Add - - ( 28, 28, 12) ( 9408, 9408, 0) 1 1 1 1 1 - - - #8 MaxPool - - ( 14, 14, 12) ( 9408, 2352, 0) 1 1 1 2 1 - - - #9 Conv2D - - ( 14, 14, 12) 254k ( 2352, 2352, 432) 1 1 1 2 1 - - - #10 AvgPool - - ( 7, 7, 12) ( 2352, 588, 168) 1 1 1 1 1 1 - - #11 AvgPool - - ( 14, 14, 12) ( 9408, 2352, 336) 1 1 1 1 1 1 - - #12 Add - - ( 14, 14, 12) ( 2352, 2352, 0) 1 1 - 1 1 1 - - #13 MaxPool - - ( 7, 7, 12) ( 2352, 588, 0) 1 1 1 2 - 1 - - #14 UpSample - - ( 14, 14, 12) ( 588, 2352, 0) 1 1 - 2 - 1 - - #15 Add - - ( 14, 14, 12) ( 2352, 2352, 0) 1 1 1 1 - 1 - - #16 MaxPool - - ( 7, 7, 12) ( 2352, 588, 0) 1 1 1 1 - 1 - - #17 Conv2D - - ( 7, 7, 12) 63k ( 588, 588, 432) 1 1 1 1 - 1 - - #18 Add - - ( 7, 7, 12) ( 588, 588, 0) 1 1 1 - - 1 - - #19 Concat - - ( 7, 7, 24) ( 1176, 1176, 0) 1 1 1 - - - - - #20 Dense - ReLU - ( 96, 1, 1) 112k ( 1176, 96, 2352) 1 1 1 - - - - - #21 Dense - - ( 10, 1, 1) 960 ( 96, 10, 192) 1 1 1 - - - - - #22 Softmax - - ( 10, 1, 1) ( 10, 10, 0) 1 - 1 - - - - - #23 Output - - ( 10, 1, 1) ( 10, 10, 0) 1 - - - - - - - ------------------------------------------------------------------------------------------------- Memory cost by each block: blk_0:9408 blk_1:9408 blk_2:9408 blk_3:9408 blk_4:2352 blk_5:588 blk_6:0 blk_7:0 Total memory cost by network buffers: 40572 bytes Compling done in 76 ms You can now use the model to predict your data. Firstly, filling the input buffer nnom_input_buffer[] with your own data(image, signals) which is defined in weights.h . Secondly, call model_run(model); to do your prediction. Thirdly, read your result from nnom_output_buffer[] . The maximum number is the results. Now, please do check NNoM examples for more fancy methods. What's More? To be continue..","title":"5 min to NNoM"},{"location":"guide_5_min_to_nnom/#5-min-guide-to-nnom","text":"The aim for NNoM is to help Embedded Engineers to develop and deploy Neural Network models onto the MCUs . NNoM is working closely with Keras . If you dont know Keras yet Getting started: 30 seconds to Keras This guide will show you how to use NNoM for your very first step from an embedded engineer perspective.","title":"5 min Guide to NNoM"},{"location":"guide_5_min_to_nnom/#backgrouds-checking","text":"You should: know C language and your target MCU enviroment. know a bit of python. You must NOT : be a pro in TensorFlow / lite :-)","title":"Backgrouds Checking"},{"location":"guide_5_min_to_nnom/#neural-network-with-keras","text":"If you know nothing about Keras, you must check Getting started: 30 seconds to Keras first. Lets say if we want to classify the MNIST hand writing dataset. This is what you normally do with Keras. model = Sequential() model.add(Dense(32, input_dim=784)) model.add(Activation('relu')) model.add(Dense(10)) Each operation in Keras are defined by \"Layer\", same as we did in NNoM. The terms are different from Tensorflow (That is why you must not be a PRO in Tensorflow >_<). This model is with an input dimension 784, a hidden fully connected layer including 32 units and outputing 10 units(which is the number of classification(number 0~9)). The hidden layer is activated by ReLU activation (which keep all possitive values while set all nagtive values to 0). After you have trained this model using the method in the Keras' guide, the model can now do prediction. If you feed new image to it, it will tell you what is the wrtten number. Please try to run a example in Keras or NNoM if you are still confusing.","title":"Neural Network with Keras"},{"location":"guide_5_min_to_nnom/#deployed-using-nnom","text":"After the model is trained, the weights and parameters are already functional. We can now convert it to C language files then put it in your MCU project. The result of this step is a single weights.h file, which contains everything you need. To conver the model, NNoM has provided an simple API generate_model() API to automaticly do the job. Simply pass the model and the test dataset to it. It will do all the magics for you. generate_model(model, x_test, name='weights.h') When the conversion is finished, you will find a new weights.h under your working folder. Simply copy the file to your MCU project, and call model = nnom_model_create(); inside you main() . Below is what you should do in practice. #include \"nnom.h\" #include \"weights.h\" int main(void) { nnom_model_t *model; model = nnom_model_create(); model_run(model); } Then, your model is now running on you MCU. If you have supported printf on your MCU, you should see the compiling info on your consoles. Compiling logging similar to this: Start compiling model... Layer(#) Activation output shape ops(MAC) mem(in, out, buf) mem blk lifetime ------------------------------------------------------------------------------------------------- #1 Input - - ( 28, 28, 1) ( 784, 784, 0) 1 - - - - - - - #2 Conv2D - ReLU - ( 28, 28, 12) 84k ( 784, 9408, 36) 1 1 3 - - - - - #3 MaxPool - - ( 14, 14, 12) ( 9408, 2352, 0) 1 2 3 - - - - - #4 UpSample - - ( 28, 28, 12) ( 2352, 9408, 0) 1 2 2 - - - - - #5 Conv2D - - ( 14, 14, 12) 254k ( 2352, 2352, 432) 1 1 2 1 1 - - - #6 Conv2D - - ( 28, 28, 12) 1.01M ( 9408, 9408, 432) 1 1 2 1 1 - - - #7 Add - - ( 28, 28, 12) ( 9408, 9408, 0) 1 1 1 1 1 - - - #8 MaxPool - - ( 14, 14, 12) ( 9408, 2352, 0) 1 1 1 2 1 - - - #9 Conv2D - - ( 14, 14, 12) 254k ( 2352, 2352, 432) 1 1 1 2 1 - - - #10 AvgPool - - ( 7, 7, 12) ( 2352, 588, 168) 1 1 1 1 1 1 - - #11 AvgPool - - ( 14, 14, 12) ( 9408, 2352, 336) 1 1 1 1 1 1 - - #12 Add - - ( 14, 14, 12) ( 2352, 2352, 0) 1 1 - 1 1 1 - - #13 MaxPool - - ( 7, 7, 12) ( 2352, 588, 0) 1 1 1 2 - 1 - - #14 UpSample - - ( 14, 14, 12) ( 588, 2352, 0) 1 1 - 2 - 1 - - #15 Add - - ( 14, 14, 12) ( 2352, 2352, 0) 1 1 1 1 - 1 - - #16 MaxPool - - ( 7, 7, 12) ( 2352, 588, 0) 1 1 1 1 - 1 - - #17 Conv2D - - ( 7, 7, 12) 63k ( 588, 588, 432) 1 1 1 1 - 1 - - #18 Add - - ( 7, 7, 12) ( 588, 588, 0) 1 1 1 - - 1 - - #19 Concat - - ( 7, 7, 24) ( 1176, 1176, 0) 1 1 1 - - - - - #20 Dense - ReLU - ( 96, 1, 1) 112k ( 1176, 96, 2352) 1 1 1 - - - - - #21 Dense - - ( 10, 1, 1) 960 ( 96, 10, 192) 1 1 1 - - - - - #22 Softmax - - ( 10, 1, 1) ( 10, 10, 0) 1 - 1 - - - - - #23 Output - - ( 10, 1, 1) ( 10, 10, 0) 1 - - - - - - - ------------------------------------------------------------------------------------------------- Memory cost by each block: blk_0:9408 blk_1:9408 blk_2:9408 blk_3:9408 blk_4:2352 blk_5:588 blk_6:0 blk_7:0 Total memory cost by network buffers: 40572 bytes Compling done in 76 ms You can now use the model to predict your data. Firstly, filling the input buffer nnom_input_buffer[] with your own data(image, signals) which is defined in weights.h . Secondly, call model_run(model); to do your prediction. Thirdly, read your result from nnom_output_buffer[] . The maximum number is the results. Now, please do check NNoM examples for more fancy methods.","title":"Deployed using NNoM"},{"location":"guide_5_min_to_nnom/#whats-more","text":"To be continue..","title":"What's More?"},{"location":"guide_development/","text":"Development Guide Currently, it is not yet a \"guide\". At least, it provides some further information. Frequent Questions and Answers (Q&A) What is NNoM different from others? NNoM is a higher-level inference framework. The most obvious feature is the human understandable interface. It is also a layer-based framework, instead of operator-based. A layer might contain a few operators. It natively supports complex model structure. High-efficiency network always benefited from complex structure. It provides layer-to-layer analysis to help developer optimize their models. Should I develop an ad-hoc model or use a pre-trained model? The famous pre-trained models are more for the image processing side. They are efficient on such mobile phones. But they are still too bulky if the MCU doesn't provide at least 250K RAM and a hardware Neural Network Accelerator. MobileNet V1 model with depth multi-plier (0.25x) ... STM32 F746 ... CMSIS-NN kernels to program the depthwise and pointwise convolutions ... approximately 0.75 frames/sec Source: Visual Wake Words Dataset In most cases, MCUs should not really do image processing without hardware accelerator. The data they normally process a few channels of time sequence measurement. For example, the accelerometer data consist of 3-axis (channel) measurement per timestamp. Dealing with these data, building the ad-hoc models for each application is the only option. Building an ad-hoc model is sooo easy with NNoM since most of the codes are automatically generated. What can NNoM provide to embedded engineers? It provides an easy to use and easy to evaluate inference tools for fast neural network development. As embedded engineers, we might not know well how does neural network work and how can we optimize it for the MCU. NNoM together with Keras can help you to start practising within half an hour. There is no need to learn other ML libs from scratch. Deployment can be done with one line of python code after you have trained a model using Keras. Other than building a model, NNoM also provides a set of evaluation methods. These evaluation methods will give the developer a layer-to-layer performance evaluation of the model. Developers can then modify the ad-hoc model to increase efficiency or to lower the memory cost. (Please check the following Performance sections for detail.) NNoM Structure As mentioned in many other docs, NNoM uses a layer-based structure. The most benefit is the model structure can seem directly from the codes. It also makes the model conversion from other layer-based libs (Keras, TensorLayer, Caffe) to NNoM model very straight forward. When use generate_model(model, x_test, name='weights.h') to generate NNoM model, it simply read the configuration out and rewrite it to C codes. Structure: NNoM uses a compiler to manage the layer structure and other resources. After compiling, all layers inside the model will be put into a shortcut list per the running order. Besides that, arguments will be filled in and the memory will be allocated to each layer (Memory are reused in between layers). Therefore, no memory allocation performed in the runtime, performance is the same as running backend function directly. The NNoM is more on managing the higher-level structure, context argument and memory. The actual arithmetics are done by the backend functions. Currently, NNoM supports a pure C backend and CMSIS-NN backend. The CMSIS-NN is a highly optimized low-level NN core for ARM-Cortex-M microcontroller. Please check the optimization guide for utilisation. Quantisation NNoM currently only support 8bit weights and 8bit activations. The model will be quantised through model conversion generate_model(model, x_test, name='weights.h') . The input data (activations) will need to be quantised then feed to the model. Please see any example for quantising the input data. Optimization The CMSIS-NN can provide up to 5 times performance compared to the pure C backend on Cortex-M MCUs. It maximises the performance by using SIMD and other instructions(__SSAT, ...). These optimizations come with different constraints. This is why CMSIS-NN provides many variances to one operator (such as 1x1 convolution, RGB convolution, none-square/square, they are all convolution only with different routines). NNoM will automatically select the best operator for the layer when it is available. Sometimes, it is not possible to use CMSIS-NN because the condition is not met. CMSIS-NN provides a subset operator to the local pure C backend. When it is not possible to use CMSIS-NN, NNoM will run the layer using the C backend end instead. It varies from layer to layer whether to use CMSIS-NN or C backend. The example condition for convolutions are list below: Operation Input Channel Output Channel Convolution multiple of 4 multiple of 2 Pointwise Convolution multiple of 4 multiple of 2 Depthwise Convolution multiple of 2 multiple of 2 The full details can be found in CMSIS-NN's source code and documentation . Some of them can be further optimized by square shape, however, the optimization is less significant. Trick, if you keep the channel size is a multiple of 4, it should work in most of the case. If you are not sure whether the optimization is working, simply use the model_stat() in Evaluation API to print the performance of each layer. The comparison will be shown in the following sections. Fully connected layers and pooling layers are less constrained. Performance Performances vary from chip to chip. Efficiencies are more constant. We can use Multiply\u2013accumulate operation (MAC) per Hz (MACops/Hz) to evaluate the efficiency. It simply means how many MAC can be done in one cycle. Currently, NNoM only count MAC operations on Convolution layers and Dense layers since other layers (pooling, padding) are much lesser. Running a model on CMSIS-NN and NNoM will have the same performance when a model is fully compliant with CMSIS-NN and running on Cortex-M4/7/33/35P. (\"compliant\" means it meets the optimization condition in the above discussion). For example, in CMSIS-NN paper , the authors used an STM32F746@216MHz to run a model with 24.7M(MACops) took 99.1ms in total. The runtime of each layer was recorded. What hasn't been shown in the paper is this table. (refer to Table 1 in the paper) Layer Input ch output ch Ops Runtime Efficiency (MACops/Hz) Layer 1 Conv 3 32 4.9M 31.4ms 0.36 Layer 3 Conv 32 32 13.1M 42.8ms 0.71 Layer 5 Conv 32 64 6.6M 22.6ms 0.68 Layer 7 Dense 1024 10 20k 0.1ms 0.93 Total 24.7M 99.1ms 0.58 ops = 2 x MACops, total is less due to other layers such as activation and pooling, please check the paper for full table In the table, layer 3 and 5 are both Convolution layer with input and output channel size equal to a multiple of 4. Layer 1 with input channel = 3. You can already see the efficiency difference. When input channel = 3, the convolution is performed by arm_convolve_HWC_q7_RGB() . This method is partially optimized since the input channel is not a multiple of 4, While Layer 3 and layer 5 are fully optimized. The efficiency difference is already huge ( 0.36 vs 0.71/0.68 ). To achieve high efficiency, you should keep both input channel is a multiple of 4 and output is a multiple of 2. What does this number mean? You can use this number to estimate the best size of the model to fit the targeting MCU. In typical applications: Use motion sensor to recognise human activity . A model takes 9 channels time sequence data, 0.67M MACops , STM32F746 will take around 0.67M/0.58/216MHz = 5.3ms to do one inference. Use microphone to spot key-word commands . A model takes 63 x 12 x 1 MFCC data, 2.09M MACops , STM32F746 will take around 2.09M/0.58/216MHz = 16.7ms to do one inference. Notes, MACops/Hz in NNoM is less than the CMSIS-NN in the paper, this is because NNoM considers the operator and its following activation as one single layer. For example, the running time cost by the convolution layer is the time cost by operator(Conv) + the time cost by activation(ReLU) . Evaluations Evaluation is equally important to building the model. In NNoM, we provide a few different methods to evaluate the model. The details are list in Evaluation Methods . If your system support print through a console (such as serial port), the evaluation can be printed on the console. Firstly, the model structure is printed during compiling in model_compile() , which is normally called in nnom_model_create() . Secondly, the runtime performance is printed by model_stat() . Thirdly, there is a set of prediction_*() APIs to validate a set of testing data and print out Top-K accuracy, confusion matrix and other info. An NNoM model This is what a typical model looks like in the weights.h or model.h or whatever you name it. These codes are generated by the script. In user's main() , call nnom_model_create() will create and compile the model. ... /* nnom model */ static int8_t nnom_input_data[784]; static int8_t nnom_output_data[10]; static nnom_model_t* nnom_model_create(void) { static nnom_model_t model; nnom_layer_t* layer[20]; new_model(&model); layer[0] = Input(shape(28, 28, 1), nnom_input_data); layer[1] = model.hook(Conv2D(12, kernel(3, 3), stride(1, 1), PADDING_SAME, &conv2d_1_w, &conv2d_1_b), layer[0]); layer[2] = model.active(act_relu(), layer[1]); layer[3] = model.hook(MaxPool(kernel(2, 2), stride(2, 2), PADDING_SAME), layer[2]); layer[4] = model.hook(Cropping(border(1,2,3,4)), layer[3]); layer[5] = model.hook(Conv2D(24, kernel(3, 3), stride(1, 1), PADDING_SAME, &conv2d_2_w, &conv2d_2_b), layer[4]); layer[6] = model.active(act_relu(), layer[5]); layer[7] = model.hook(MaxPool(kernel(4, 4), stride(4, 4), PADDING_SAME), layer[6]); layer[8] = model.hook(ZeroPadding(border(1,2,3,4)), layer[7]); layer[9] = model.hook(Conv2D(24, kernel(3, 3), stride(1, 1), PADDING_SAME, &conv2d_3_w, &conv2d_3_b), layer[8]); layer[10] = model.active(act_relu(), layer[9]); layer[11] = model.hook(UpSample(kernel(2, 2)), layer[10]); layer[12] = model.hook(Conv2D(48, kernel(3, 3), stride(1, 1), PADDING_SAME, &conv2d_4_w, &conv2d_4_b), layer[11]); layer[13] = model.active(act_relu(), layer[12]); layer[14] = model.hook(MaxPool(kernel(2, 2), stride(2, 2), PADDING_SAME), layer[13]); layer[15] = model.hook(Dense(64, &dense_1_w, &dense_1_b), layer[14]); layer[16] = model.active(act_relu(), layer[15]); layer[17] = model.hook(Dense(10, &dense_2_w, &dense_2_b), layer[16]); layer[18] = model.hook(Softmax(), layer[17]); layer[19] = model.hook(Output(shape(10,1,1), nnom_output_data), layer[18]); model_compile(&model, layer[0], layer[19]); return &model; } Model info, memory This is an example printed by model_compile() , which is normally called by nnom_model_create() . Start compiling model... Layer(#) Activation output shape ops(MAC) mem(in, out, buf) mem blk lifetime ------------------------------------------------------------------------------------------------- #1 Input - - ( 28, 28, 1) ( 784, 784, 0) 1 - - - - - - - #2 Conv2D - ReLU - ( 28, 28, 12) 84k ( 784, 9408, 36) 1 1 1 - - - - - #3 MaxPool - - ( 14, 14, 12) ( 9408, 2352, 0) 1 1 1 - - - - - #4 Cropping - - ( 11, 7, 12) ( 2352, 924, 0) 1 1 - - - - - - #5 Conv2D - ReLU - ( 11, 7, 24) 199k ( 924, 1848, 432) 1 1 1 - - - - - #6 MaxPool - - ( 3, 2, 24) ( 1848, 144, 0) 1 1 1 - - - - - #7 ZeroPad - - ( 6, 9, 24) ( 144, 1296, 0) 1 1 - - - - - - #8 Conv2D - ReLU - ( 6, 9, 24) 279k ( 1296, 1296, 864) 1 1 1 - - - - - #9 UpSample - - ( 12, 18, 24) ( 1296, 5184, 0) 1 - 1 - - - - - #10 Conv2D - ReLU - ( 12, 18, 48) 2.23M ( 5184, 10368, 864) 1 1 1 - - - - - #11 MaxPool - - ( 6, 9, 48) ( 10368, 2592, 0) 1 1 1 - - - - - #12 Dense - ReLU - ( 64, 1, 1) 165k ( 2592, 64, 5184) 1 1 1 - - - - - #13 Dense - - ( 10, 1, 1) 640 ( 64, 10, 128) 1 1 1 - - - - - #14 Softmax - - ( 10, 1, 1) ( 10, 10, 0) 1 1 - - - - - - #15 Output - - ( 10, 1, 1) ( 10, 10, 0) 1 - - - - - - - ------------------------------------------------------------------------------------------------- Memory cost by each block: blk_0:5184 blk_1:2592 blk_2:10368 blk_3:0 blk_4:0 blk_5:0 blk_6:0 blk_7:0 Total memory cost by network buffers: 18144 bytes Compling done in 179 ms It shows the run order, Layer names, activations, the output shape of the layer, the operation counts, the buffer size, and the memory block assignments. Later, it prints the maximum memory cost for each memory block. Since the memory block is shared between layers, the model only e 3 memory blocks, altogether gives a sum memory cost by 18144 Bytes . Runtime statistices This is an example printed by model_stat() . This method requires a microsecond timestamp porting, check porting guide Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 11 #2 Conv2D - 5848 84k 14.47 #3 MaxPool - 698 #4 Cropping - 16 #5 Conv2D - 3367 199k 59.27 #6 MaxPool - 346 #7 ZeroPad - 36 #8 Conv2D - 4400 279k 63.62 #9 UpSample - 116 #10 Conv2D - 33563 2.23M 66.72 #11 MaxPool - 2137 #12 Dense - 2881 165k 57.58 #13 Dense - 16 640 40.00 #14 Softmax - 3 #15 Output - 1 Summary: Total ops (MAC): 2970208(2.97M) Prediction time :53439us Efficiency 55.58 ops/us NNOM: Total Mem: 20236 Calling this method will print out the time cost for each layer, and the efficiency in (MACops/us) of this layer. This is very important when designing your ad-hoc model. For example, #2 layer has only 14.47 MACops/us , while #5, #8 and #10 are around 60 MACops/us . This is due to the input channel of #2 layer is 1, which cannot fulfil the optimisation conditions of CMSIS-NN. One simple optimization strategy is to minimize the complexity in #2 layer by reducing the output channel size. Others Memeory management in NNoM As mention, NNoM will allocate memory to the layer during the compiling phase. Memory block is a minimum unit for a layer to apply. For example, convolution layers normally apply one block for input data, one block for output data and one block for the intermediate data buffer. Layer(#) Activation output shape ops(MAC) mem(in, out, buf) mem blk lifetime ------------------------------------------------------------------------------------------------- #2 Conv2D - ReLU - ( 28, 28, 12) 84k ( 784, 9408, 36) 1 1 1 - - - - - The example shows input buffer size 784 , output buffer size 9408 , intermediate buffer size 36 . The following mem blk lifetime means how long does the memory block last. All three block last only one step, they will be freed after the layer. In NNoM, the output memory will be pass directly to the next layer(s) as input buffer, so there is no memory copy cost and memory allocation in between layers.","title":"Development Guide"},{"location":"guide_development/#development-guide","text":"Currently, it is not yet a \"guide\". At least, it provides some further information.","title":"Development Guide"},{"location":"guide_development/#frequent-questions-and-answers-qa","text":"","title":"Frequent Questions and Answers (Q&A)"},{"location":"guide_development/#what-is-nnom-different-from-others","text":"NNoM is a higher-level inference framework. The most obvious feature is the human understandable interface. It is also a layer-based framework, instead of operator-based. A layer might contain a few operators. It natively supports complex model structure. High-efficiency network always benefited from complex structure. It provides layer-to-layer analysis to help developer optimize their models.","title":"What is NNoM different from others?"},{"location":"guide_development/#should-i-develop-an-ad-hoc-model-or-use-a-pre-trained-model","text":"The famous pre-trained models are more for the image processing side. They are efficient on such mobile phones. But they are still too bulky if the MCU doesn't provide at least 250K RAM and a hardware Neural Network Accelerator. MobileNet V1 model with depth multi-plier (0.25x) ... STM32 F746 ... CMSIS-NN kernels to program the depthwise and pointwise convolutions ... approximately 0.75 frames/sec Source: Visual Wake Words Dataset In most cases, MCUs should not really do image processing without hardware accelerator. The data they normally process a few channels of time sequence measurement. For example, the accelerometer data consist of 3-axis (channel) measurement per timestamp. Dealing with these data, building the ad-hoc models for each application is the only option. Building an ad-hoc model is sooo easy with NNoM since most of the codes are automatically generated.","title":"Should I develop an ad-hoc model or use a pre-trained model?"},{"location":"guide_development/#what-can-nnom-provide-to-embedded-engineers","text":"It provides an easy to use and easy to evaluate inference tools for fast neural network development. As embedded engineers, we might not know well how does neural network work and how can we optimize it for the MCU. NNoM together with Keras can help you to start practising within half an hour. There is no need to learn other ML libs from scratch. Deployment can be done with one line of python code after you have trained a model using Keras. Other than building a model, NNoM also provides a set of evaluation methods. These evaluation methods will give the developer a layer-to-layer performance evaluation of the model. Developers can then modify the ad-hoc model to increase efficiency or to lower the memory cost. (Please check the following Performance sections for detail.)","title":"What can NNoM provide to embedded engineers?"},{"location":"guide_development/#nnom-structure","text":"As mentioned in many other docs, NNoM uses a layer-based structure. The most benefit is the model structure can seem directly from the codes. It also makes the model conversion from other layer-based libs (Keras, TensorLayer, Caffe) to NNoM model very straight forward. When use generate_model(model, x_test, name='weights.h') to generate NNoM model, it simply read the configuration out and rewrite it to C codes. Structure: NNoM uses a compiler to manage the layer structure and other resources. After compiling, all layers inside the model will be put into a shortcut list per the running order. Besides that, arguments will be filled in and the memory will be allocated to each layer (Memory are reused in between layers). Therefore, no memory allocation performed in the runtime, performance is the same as running backend function directly. The NNoM is more on managing the higher-level structure, context argument and memory. The actual arithmetics are done by the backend functions. Currently, NNoM supports a pure C backend and CMSIS-NN backend. The CMSIS-NN is a highly optimized low-level NN core for ARM-Cortex-M microcontroller. Please check the optimization guide for utilisation.","title":"NNoM Structure"},{"location":"guide_development/#quantisation","text":"NNoM currently only support 8bit weights and 8bit activations. The model will be quantised through model conversion generate_model(model, x_test, name='weights.h') . The input data (activations) will need to be quantised then feed to the model. Please see any example for quantising the input data.","title":"Quantisation"},{"location":"guide_development/#optimization","text":"The CMSIS-NN can provide up to 5 times performance compared to the pure C backend on Cortex-M MCUs. It maximises the performance by using SIMD and other instructions(__SSAT, ...). These optimizations come with different constraints. This is why CMSIS-NN provides many variances to one operator (such as 1x1 convolution, RGB convolution, none-square/square, they are all convolution only with different routines). NNoM will automatically select the best operator for the layer when it is available. Sometimes, it is not possible to use CMSIS-NN because the condition is not met. CMSIS-NN provides a subset operator to the local pure C backend. When it is not possible to use CMSIS-NN, NNoM will run the layer using the C backend end instead. It varies from layer to layer whether to use CMSIS-NN or C backend. The example condition for convolutions are list below: Operation Input Channel Output Channel Convolution multiple of 4 multiple of 2 Pointwise Convolution multiple of 4 multiple of 2 Depthwise Convolution multiple of 2 multiple of 2 The full details can be found in CMSIS-NN's source code and documentation . Some of them can be further optimized by square shape, however, the optimization is less significant. Trick, if you keep the channel size is a multiple of 4, it should work in most of the case. If you are not sure whether the optimization is working, simply use the model_stat() in Evaluation API to print the performance of each layer. The comparison will be shown in the following sections. Fully connected layers and pooling layers are less constrained.","title":"Optimization"},{"location":"guide_development/#performance","text":"Performances vary from chip to chip. Efficiencies are more constant. We can use Multiply\u2013accumulate operation (MAC) per Hz (MACops/Hz) to evaluate the efficiency. It simply means how many MAC can be done in one cycle. Currently, NNoM only count MAC operations on Convolution layers and Dense layers since other layers (pooling, padding) are much lesser. Running a model on CMSIS-NN and NNoM will have the same performance when a model is fully compliant with CMSIS-NN and running on Cortex-M4/7/33/35P. (\"compliant\" means it meets the optimization condition in the above discussion). For example, in CMSIS-NN paper , the authors used an STM32F746@216MHz to run a model with 24.7M(MACops) took 99.1ms in total. The runtime of each layer was recorded. What hasn't been shown in the paper is this table. (refer to Table 1 in the paper) Layer Input ch output ch Ops Runtime Efficiency (MACops/Hz) Layer 1 Conv 3 32 4.9M 31.4ms 0.36 Layer 3 Conv 32 32 13.1M 42.8ms 0.71 Layer 5 Conv 32 64 6.6M 22.6ms 0.68 Layer 7 Dense 1024 10 20k 0.1ms 0.93 Total 24.7M 99.1ms 0.58 ops = 2 x MACops, total is less due to other layers such as activation and pooling, please check the paper for full table In the table, layer 3 and 5 are both Convolution layer with input and output channel size equal to a multiple of 4. Layer 1 with input channel = 3. You can already see the efficiency difference. When input channel = 3, the convolution is performed by arm_convolve_HWC_q7_RGB() . This method is partially optimized since the input channel is not a multiple of 4, While Layer 3 and layer 5 are fully optimized. The efficiency difference is already huge ( 0.36 vs 0.71/0.68 ). To achieve high efficiency, you should keep both input channel is a multiple of 4 and output is a multiple of 2. What does this number mean? You can use this number to estimate the best size of the model to fit the targeting MCU. In typical applications: Use motion sensor to recognise human activity . A model takes 9 channels time sequence data, 0.67M MACops , STM32F746 will take around 0.67M/0.58/216MHz = 5.3ms to do one inference. Use microphone to spot key-word commands . A model takes 63 x 12 x 1 MFCC data, 2.09M MACops , STM32F746 will take around 2.09M/0.58/216MHz = 16.7ms to do one inference. Notes, MACops/Hz in NNoM is less than the CMSIS-NN in the paper, this is because NNoM considers the operator and its following activation as one single layer. For example, the running time cost by the convolution layer is the time cost by operator(Conv) + the time cost by activation(ReLU) .","title":"Performance"},{"location":"guide_development/#evaluations","text":"Evaluation is equally important to building the model. In NNoM, we provide a few different methods to evaluate the model. The details are list in Evaluation Methods . If your system support print through a console (such as serial port), the evaluation can be printed on the console. Firstly, the model structure is printed during compiling in model_compile() , which is normally called in nnom_model_create() . Secondly, the runtime performance is printed by model_stat() . Thirdly, there is a set of prediction_*() APIs to validate a set of testing data and print out Top-K accuracy, confusion matrix and other info.","title":"Evaluations"},{"location":"guide_development/#an-nnom-model","text":"This is what a typical model looks like in the weights.h or model.h or whatever you name it. These codes are generated by the script. In user's main() , call nnom_model_create() will create and compile the model. ... /* nnom model */ static int8_t nnom_input_data[784]; static int8_t nnom_output_data[10]; static nnom_model_t* nnom_model_create(void) { static nnom_model_t model; nnom_layer_t* layer[20]; new_model(&model); layer[0] = Input(shape(28, 28, 1), nnom_input_data); layer[1] = model.hook(Conv2D(12, kernel(3, 3), stride(1, 1), PADDING_SAME, &conv2d_1_w, &conv2d_1_b), layer[0]); layer[2] = model.active(act_relu(), layer[1]); layer[3] = model.hook(MaxPool(kernel(2, 2), stride(2, 2), PADDING_SAME), layer[2]); layer[4] = model.hook(Cropping(border(1,2,3,4)), layer[3]); layer[5] = model.hook(Conv2D(24, kernel(3, 3), stride(1, 1), PADDING_SAME, &conv2d_2_w, &conv2d_2_b), layer[4]); layer[6] = model.active(act_relu(), layer[5]); layer[7] = model.hook(MaxPool(kernel(4, 4), stride(4, 4), PADDING_SAME), layer[6]); layer[8] = model.hook(ZeroPadding(border(1,2,3,4)), layer[7]); layer[9] = model.hook(Conv2D(24, kernel(3, 3), stride(1, 1), PADDING_SAME, &conv2d_3_w, &conv2d_3_b), layer[8]); layer[10] = model.active(act_relu(), layer[9]); layer[11] = model.hook(UpSample(kernel(2, 2)), layer[10]); layer[12] = model.hook(Conv2D(48, kernel(3, 3), stride(1, 1), PADDING_SAME, &conv2d_4_w, &conv2d_4_b), layer[11]); layer[13] = model.active(act_relu(), layer[12]); layer[14] = model.hook(MaxPool(kernel(2, 2), stride(2, 2), PADDING_SAME), layer[13]); layer[15] = model.hook(Dense(64, &dense_1_w, &dense_1_b), layer[14]); layer[16] = model.active(act_relu(), layer[15]); layer[17] = model.hook(Dense(10, &dense_2_w, &dense_2_b), layer[16]); layer[18] = model.hook(Softmax(), layer[17]); layer[19] = model.hook(Output(shape(10,1,1), nnom_output_data), layer[18]); model_compile(&model, layer[0], layer[19]); return &model; }","title":"An NNoM model"},{"location":"guide_development/#model-info-memory","text":"This is an example printed by model_compile() , which is normally called by nnom_model_create() . Start compiling model... Layer(#) Activation output shape ops(MAC) mem(in, out, buf) mem blk lifetime ------------------------------------------------------------------------------------------------- #1 Input - - ( 28, 28, 1) ( 784, 784, 0) 1 - - - - - - - #2 Conv2D - ReLU - ( 28, 28, 12) 84k ( 784, 9408, 36) 1 1 1 - - - - - #3 MaxPool - - ( 14, 14, 12) ( 9408, 2352, 0) 1 1 1 - - - - - #4 Cropping - - ( 11, 7, 12) ( 2352, 924, 0) 1 1 - - - - - - #5 Conv2D - ReLU - ( 11, 7, 24) 199k ( 924, 1848, 432) 1 1 1 - - - - - #6 MaxPool - - ( 3, 2, 24) ( 1848, 144, 0) 1 1 1 - - - - - #7 ZeroPad - - ( 6, 9, 24) ( 144, 1296, 0) 1 1 - - - - - - #8 Conv2D - ReLU - ( 6, 9, 24) 279k ( 1296, 1296, 864) 1 1 1 - - - - - #9 UpSample - - ( 12, 18, 24) ( 1296, 5184, 0) 1 - 1 - - - - - #10 Conv2D - ReLU - ( 12, 18, 48) 2.23M ( 5184, 10368, 864) 1 1 1 - - - - - #11 MaxPool - - ( 6, 9, 48) ( 10368, 2592, 0) 1 1 1 - - - - - #12 Dense - ReLU - ( 64, 1, 1) 165k ( 2592, 64, 5184) 1 1 1 - - - - - #13 Dense - - ( 10, 1, 1) 640 ( 64, 10, 128) 1 1 1 - - - - - #14 Softmax - - ( 10, 1, 1) ( 10, 10, 0) 1 1 - - - - - - #15 Output - - ( 10, 1, 1) ( 10, 10, 0) 1 - - - - - - - ------------------------------------------------------------------------------------------------- Memory cost by each block: blk_0:5184 blk_1:2592 blk_2:10368 blk_3:0 blk_4:0 blk_5:0 blk_6:0 blk_7:0 Total memory cost by network buffers: 18144 bytes Compling done in 179 ms It shows the run order, Layer names, activations, the output shape of the layer, the operation counts, the buffer size, and the memory block assignments. Later, it prints the maximum memory cost for each memory block. Since the memory block is shared between layers, the model only e 3 memory blocks, altogether gives a sum memory cost by 18144 Bytes .","title":"Model info, memory"},{"location":"guide_development/#runtime-statistices","text":"This is an example printed by model_stat() . This method requires a microsecond timestamp porting, check porting guide Print running stat.. Layer(#) - Time(us) ops(MACs) ops/us -------------------------------------------------------- #1 Input - 11 #2 Conv2D - 5848 84k 14.47 #3 MaxPool - 698 #4 Cropping - 16 #5 Conv2D - 3367 199k 59.27 #6 MaxPool - 346 #7 ZeroPad - 36 #8 Conv2D - 4400 279k 63.62 #9 UpSample - 116 #10 Conv2D - 33563 2.23M 66.72 #11 MaxPool - 2137 #12 Dense - 2881 165k 57.58 #13 Dense - 16 640 40.00 #14 Softmax - 3 #15 Output - 1 Summary: Total ops (MAC): 2970208(2.97M) Prediction time :53439us Efficiency 55.58 ops/us NNOM: Total Mem: 20236 Calling this method will print out the time cost for each layer, and the efficiency in (MACops/us) of this layer. This is very important when designing your ad-hoc model. For example, #2 layer has only 14.47 MACops/us , while #5, #8 and #10 are around 60 MACops/us . This is due to the input channel of #2 layer is 1, which cannot fulfil the optimisation conditions of CMSIS-NN. One simple optimization strategy is to minimize the complexity in #2 layer by reducing the output channel size.","title":"Runtime statistices"},{"location":"guide_development/#others","text":"","title":"Others"},{"location":"guide_development/#memeory-management-in-nnom","text":"As mention, NNoM will allocate memory to the layer during the compiling phase. Memory block is a minimum unit for a layer to apply. For example, convolution layers normally apply one block for input data, one block for output data and one block for the intermediate data buffer. Layer(#) Activation output shape ops(MAC) mem(in, out, buf) mem blk lifetime ------------------------------------------------------------------------------------------------- #2 Conv2D - ReLU - ( 28, 28, 12) 84k ( 784, 9408, 36) 1 1 1 - - - - - The example shows input buffer size 784 , output buffer size 9408 , intermediate buffer size 36 . The following mem blk lifetime means how long does the memory block last. All three block last only one step, they will be freed after the layer. In NNoM, the output memory will be pass directly to the next layer(s) as input buffer, so there is no memory copy cost and memory allocation in between layers.","title":"Memeory management in NNoM"},{"location":"legacy_README/","text":"Neural Network on Microcontroller (NNoM) NNoM is a higher-level layer-based Neural Network library specifically for microcontrollers. NNoM is released under LGPL-V3.0, please check the license file for detail. The Temporary Guide Porting and Optimising Guide Dependencies NNoM now use the local pure C backend implementation by default. Thus, there is no special dependency needed. However, you can still select CMSIS-NN/DSP as the backend for about 5x performance with ARM-Cortex-M4/7/33/35P. Simply #define NNOM_USING_CMSIS_NN in nnom_port.h and include CMSIS-NN in your project. Check Porting and Optimising Guide for detail. Why NNoM? The aims of NNoM is to provide a light-weight, user-friendly and flexible interface for fast deploying. Nowadays, neural networks are wider , deeper , and denser . [1] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9). [2] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). [3] Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708). If you would like to try those more up-to-date, decent and complex structures on MCU NNoM can help you to build them with only a few lines of C codes , same as you did with Python in Keras Inception example: uci-inception DenseNet example: mnist-densenet A simple example: #define INPUT_HIGHT 1 #define INPUT_WIDTH 128 #define INPUT_CH 9 new_model(&model); model.add(&model, Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), qformat(7, 0), input_buf)); model.add(&model, Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b)); // c1_w, c1_b are weights and bias model.add(&model, ReLU()); model.add(&model, MaxPool(kernel(1, 4), stride(1, 4), PADDING_VALID)); model.add(&model, Dense(128, &ip1_w, &ip1_b)); model.add(&model, ReLU()); model.add(&model, Dense(6, &ip2_w, &ip2_b)); model.add(&model, Softmax()); model.add(&model, Output(shape(6, 1, 1), qformat(7, 0), output_buf)); sequencial_compile(&model); while(1){ feed_input(&input_buf) model_run(&model); } It supports both sequential and functional API. The above codes shows how a sequential model is built, compiled, and ran. Functional Model Functional APIs are much more flexible. It allows developer to build complex structures in MCU, such as Inception and ResNet . The below codes shows an Inception structures with 3 parallel subpathes. #define INPUT_HIGHT 1 #define INPUT_WIDTH 128 #define INPUT_CH 9 nnom_layer_t *input_layer, *x, *x1, *x2, *x3; input_layer = Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), qformat(7, 0), input_buf); // conv2d x = model.hook(Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b), input_layer); x = model.active(act_relu(), x); x = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // parallel Inception 1 - conv2d x1 = model.hook(Conv2D(16, kernel(1, 5), stride(1, 1), PADDING_SAME, &c2_w, &c2_b), x); // hooked to x x1 = model.active(act_relu(), x1); x1 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x1); // parallel Inception 2 - conv2d x2 = model.hook(Conv2D(16, kernel(1, 3), stride(1, 1), PADDING_SAME, &c3_w, &c3_b), x); // hooked to x x2 = model.active(act_relu(), x2); x2 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x2); // parallel Inception 3 - maxpool x3 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // hooked to x // concatenate 3 parallel. x = model.mergex(Concat(-1), 3, x1, x2, x3); // new merge API. // flatten & dense x = model.hook(Flatten(), x); x = model.hook(Dense(128, &ip1_w, &ip1_b), x); x = model.active(act_relu(), x); x = model.hook(Dense(6, &ip2_w, &ip2_b), x); x = model.hook(Softmax(), x); x = model.hook(Output(shape(6,1,1), qformat(7, 0), output_buf), x); // compile and check model_compile(&model, input_layer, x); while(1){ feed_input(&input_buf) model_run(&model); } Please check A brief manual for more API details. Available Operations Layers Layers Status Layer API Comments Convolution Beta Conv2D() Support 1/2D Depthwise Conv Beta DW_Conv2D() Support 1/2D Fully-connected Beta Dense() Lambda Alpha Lambda() single input / single output anonymous operation Input/Output Beta Input()/Output() Recurrent NN Under Dev. RNN() Under Developpment Simple RNN Under Dev. SimpleCell() Under Developpment Gated Recurrent Network (GRU) Under Dev. GRUCell() Under Developpment Flatten Beta Flatten() SoftMax Beta SoftMax() Softmax only has layer API Activation Beta Activation() A layer instance for activation Activations Activation can be used by itself as layer, or can be attached to the previous layer as \"actail\" to reduce memory cost. Actrivation Status Layer API Activation API Comments ReLU Beta ReLU() act_relu() TanH Beta TanH() act_tanh() Sigmoid Beta Sigmoid() act_sigmoid() Pooling Layers Pooling Status Layer API Comments Max Pooling Beta MaxPool() Average Pooling Beta AvgPool() Sum Pooling Beta SumPool() Global Max Pooling Beta GlobalMaxPool() Global Average Pooling Beta GlobalAvgPool() Global Sum Pooling Beta GlobalSumPool() A better alternative to Global average pooling in MCU before Softmax Up Sampling Beta UpSample() or Unpooling Matrix Operations Layers Matrix Status Layer API Comments Multiple Beta Mult() Addition Beta Add() Substraction Beta Sub() Dot Under Dev. Memory requirements NNoM requires dynamic memory allocating during model building and compiling. No memory allocating in running the model. RAM requirement is about 100 to 200 bytes per layer for NNoM instance, plus the maximum data buf cost. The sequential example above includes 9 layer instances. So, the memory cost for instances is 130 x 9 = 1170 Bytes. The maximum data buffer is in the convolutional layer. It costs 1 x 128 x 9 = 1152 Bytes as input, 1 x 64 x 16 = 1024 Bytes as output, and 576 Bytes as intermedium buffer (img2col). The total memory cost of the model is around 1170 (instance) + (1152+1024+576)(network) = ~3922 Bytes. In NNoM, we dont analysis memory cost manually like above. Memory analysis will be printed when compiling the model. Deploying Keras model to NNoM You can now use generate_model(model, x_data) to deploy your model to weights.h directly. Then simply call nnom_model_create() in your main() to compile the model on your platform. Please check A brief manual and MNIST-DenseNet example. Porting Simply modify the nnom_port.h according to your platform. To optimise for ARM chips, it is required to include the CMSIS-NN lib in your projects. Then define #define NNOM_USE_CMSIS_NN in the nnom_port.h Please check Porting and Optimising Guide for detial. Current Critical Limitations Support 8-bit quantisation only. TODO Support RNN types layers. ~~Support mutiple Q-formats~~\uff08Done, by @parai\uff09 ~~support memory releasing.~~\uff08Done\uff09 Contacts Jianjia Ma J.Ma2@lboro.ac.uk or majianjia@live.com Citation Required Please contact us using above details.","title":"Old Readme"},{"location":"legacy_README/#neural-network-on-microcontroller-nnom","text":"NNoM is a higher-level layer-based Neural Network library specifically for microcontrollers. NNoM is released under LGPL-V3.0, please check the license file for detail. The Temporary Guide Porting and Optimising Guide","title":"Neural Network on Microcontroller (NNoM)"},{"location":"legacy_README/#dependencies","text":"NNoM now use the local pure C backend implementation by default. Thus, there is no special dependency needed. However, you can still select CMSIS-NN/DSP as the backend for about 5x performance with ARM-Cortex-M4/7/33/35P. Simply #define NNOM_USING_CMSIS_NN in nnom_port.h and include CMSIS-NN in your project. Check Porting and Optimising Guide for detail.","title":"Dependencies"},{"location":"legacy_README/#why-nnom","text":"The aims of NNoM is to provide a light-weight, user-friendly and flexible interface for fast deploying. Nowadays, neural networks are wider , deeper , and denser . [1] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9). [2] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). [3] Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708). If you would like to try those more up-to-date, decent and complex structures on MCU NNoM can help you to build them with only a few lines of C codes , same as you did with Python in Keras Inception example: uci-inception DenseNet example: mnist-densenet A simple example: #define INPUT_HIGHT 1 #define INPUT_WIDTH 128 #define INPUT_CH 9 new_model(&model); model.add(&model, Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), qformat(7, 0), input_buf)); model.add(&model, Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b)); // c1_w, c1_b are weights and bias model.add(&model, ReLU()); model.add(&model, MaxPool(kernel(1, 4), stride(1, 4), PADDING_VALID)); model.add(&model, Dense(128, &ip1_w, &ip1_b)); model.add(&model, ReLU()); model.add(&model, Dense(6, &ip2_w, &ip2_b)); model.add(&model, Softmax()); model.add(&model, Output(shape(6, 1, 1), qformat(7, 0), output_buf)); sequencial_compile(&model); while(1){ feed_input(&input_buf) model_run(&model); } It supports both sequential and functional API. The above codes shows how a sequential model is built, compiled, and ran.","title":"Why NNoM?"},{"location":"legacy_README/#functional-model","text":"Functional APIs are much more flexible. It allows developer to build complex structures in MCU, such as Inception and ResNet . The below codes shows an Inception structures with 3 parallel subpathes. #define INPUT_HIGHT 1 #define INPUT_WIDTH 128 #define INPUT_CH 9 nnom_layer_t *input_layer, *x, *x1, *x2, *x3; input_layer = Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), qformat(7, 0), input_buf); // conv2d x = model.hook(Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &c1_w, &c1_b), input_layer); x = model.active(act_relu(), x); x = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // parallel Inception 1 - conv2d x1 = model.hook(Conv2D(16, kernel(1, 5), stride(1, 1), PADDING_SAME, &c2_w, &c2_b), x); // hooked to x x1 = model.active(act_relu(), x1); x1 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x1); // parallel Inception 2 - conv2d x2 = model.hook(Conv2D(16, kernel(1, 3), stride(1, 1), PADDING_SAME, &c3_w, &c3_b), x); // hooked to x x2 = model.active(act_relu(), x2); x2 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x2); // parallel Inception 3 - maxpool x3 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // hooked to x // concatenate 3 parallel. x = model.mergex(Concat(-1), 3, x1, x2, x3); // new merge API. // flatten & dense x = model.hook(Flatten(), x); x = model.hook(Dense(128, &ip1_w, &ip1_b), x); x = model.active(act_relu(), x); x = model.hook(Dense(6, &ip2_w, &ip2_b), x); x = model.hook(Softmax(), x); x = model.hook(Output(shape(6,1,1), qformat(7, 0), output_buf), x); // compile and check model_compile(&model, input_layer, x); while(1){ feed_input(&input_buf) model_run(&model); } Please check A brief manual for more API details.","title":"Functional Model"},{"location":"legacy_README/#available-operations","text":"Layers Layers Status Layer API Comments Convolution Beta Conv2D() Support 1/2D Depthwise Conv Beta DW_Conv2D() Support 1/2D Fully-connected Beta Dense() Lambda Alpha Lambda() single input / single output anonymous operation Input/Output Beta Input()/Output() Recurrent NN Under Dev. RNN() Under Developpment Simple RNN Under Dev. SimpleCell() Under Developpment Gated Recurrent Network (GRU) Under Dev. GRUCell() Under Developpment Flatten Beta Flatten() SoftMax Beta SoftMax() Softmax only has layer API Activation Beta Activation() A layer instance for activation Activations Activation can be used by itself as layer, or can be attached to the previous layer as \"actail\" to reduce memory cost. Actrivation Status Layer API Activation API Comments ReLU Beta ReLU() act_relu() TanH Beta TanH() act_tanh() Sigmoid Beta Sigmoid() act_sigmoid() Pooling Layers Pooling Status Layer API Comments Max Pooling Beta MaxPool() Average Pooling Beta AvgPool() Sum Pooling Beta SumPool() Global Max Pooling Beta GlobalMaxPool() Global Average Pooling Beta GlobalAvgPool() Global Sum Pooling Beta GlobalSumPool() A better alternative to Global average pooling in MCU before Softmax Up Sampling Beta UpSample() or Unpooling Matrix Operations Layers Matrix Status Layer API Comments Multiple Beta Mult() Addition Beta Add() Substraction Beta Sub() Dot Under Dev.","title":"Available Operations"},{"location":"legacy_README/#memory-requirements","text":"NNoM requires dynamic memory allocating during model building and compiling. No memory allocating in running the model. RAM requirement is about 100 to 200 bytes per layer for NNoM instance, plus the maximum data buf cost. The sequential example above includes 9 layer instances. So, the memory cost for instances is 130 x 9 = 1170 Bytes. The maximum data buffer is in the convolutional layer. It costs 1 x 128 x 9 = 1152 Bytes as input, 1 x 64 x 16 = 1024 Bytes as output, and 576 Bytes as intermedium buffer (img2col). The total memory cost of the model is around 1170 (instance) + (1152+1024+576)(network) = ~3922 Bytes. In NNoM, we dont analysis memory cost manually like above. Memory analysis will be printed when compiling the model.","title":"Memory requirements"},{"location":"legacy_README/#deploying-keras-model-to-nnom","text":"You can now use generate_model(model, x_data) to deploy your model to weights.h directly. Then simply call nnom_model_create() in your main() to compile the model on your platform. Please check A brief manual and MNIST-DenseNet example.","title":"Deploying Keras model to NNoM"},{"location":"legacy_README/#porting","text":"Simply modify the nnom_port.h according to your platform. To optimise for ARM chips, it is required to include the CMSIS-NN lib in your projects. Then define #define NNOM_USE_CMSIS_NN in the nnom_port.h Please check Porting and Optimising Guide for detial.","title":"Porting"},{"location":"legacy_README/#current-critical-limitations","text":"Support 8-bit quantisation only.","title":"Current Critical Limitations"},{"location":"legacy_README/#todo","text":"Support RNN types layers. ~~Support mutiple Q-formats~~\uff08Done, by @parai\uff09 ~~support memory releasing.~~\uff08Done\uff09","title":"TODO"},{"location":"legacy_README/#contacts","text":"Jianjia Ma J.Ma2@lboro.ac.uk or majianjia@live.com","title":"Contacts"},{"location":"legacy_README/#citation-required","text":"Please contact us using above details.","title":"Citation Required"},{"location":"rt-thread_guide/","text":"NNoM NNoM (Neural Network on Microcontroller) 1. \u7b80\u4ecb NNoM\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u4e86\u795e\u7ecf\u7f51\u7edc\u5728 MCU \u4e0a\u8fd0\u884c\u7684\u6846\u67b6\u3002 NNoM \u662f\u4e00\u4e2a\u5b9a\u70b9\u795e\u7ecf\u7f51\u7edc\u5e93\uff0c \u73b0\u5728\u652f\u6301 8-bit\u5b9a\u70b9\u683c\u5f0f\u3002 \u5f53\u524d NNoM \u652f\u6301\u6570\u5341\u79cd\u64cd\u4f5c\uff0c\u5377\u79ef\uff0c\u6c60\u5316\uff0c\u6fc0\u6d3b\uff0c\u77e9\u9635\u8ba1\u7b97\u7b49\u7b49\u3002 \u6b64\u5916\u8fd8\u63d0\u4f9b\u591a\u79cd\u5728\u7ebf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u65f6\u95f4\u7edf\u8ba1\uff0c\u5185\u5b58\u7edf\u8ba1\uff0cTop-K \u51c6\u786e\u5ea6\uff0cConfusion Matrix \u7b49\u7b49\u3002 \u5b83\u62e5\u6709\u4ee5\u4e0b\u4f18\u70b9\uff1a \u5feb\u901f\u5f00\u53d1 \uff1a \u7535\u8111\u4e0a\u8bad\u7ec3\u7684 Keras \u6a21\u578b\u76f4\u63a5\u8f6c\u6362\u6210 C \u6587\u4ef6\u3002 \u9ed8\u8ba4\u7eafC\u540e\u7aef \uff1a \u79fb\u690d\u548c\u4f7f\u7528\u7545\u901a\u65e0\u963b\uff0c\u652f\u630132/64bit\uff0cMCU/PC\u3002 \u652f\u6301\u590d\u6742\u7ed3\u6784 \uff1a \u652f\u6301\u591a\u79cd\u590d\u6742\u7684\u7f51\u7edc\u6a21\u578b\u7ed3\u6784\u3002 \u5b8c\u5584\u7684\u6587\u6863 \uff1a \u62e5\u6709 API \u6587\u6863\uff0c\u5165\u95e8\u6307\u5357\uff0c\u4f18\u5316\u6307\u5357\u3002 \u5165\u95e8\u7b80\u5355 \uff1a \u591a\u4e2a\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u4f8b\u5b50\uff0c\u5b8c\u5168\u5f00\u6e90\u3002 MCU \u4e0a\u7684\u795e\u7ecf\u7f51\u7edc\u80fd\u505a\u4ec0\u4e48\uff1f \u8bed\u97f3\u5173\u952e\u8bcd\u8bc6\u522b \uff08KeyWord Spotting\uff09 \u4f7f\u7528\u8fd0\u52a8\u4f20\u611f\u5668\u8bc6\u522b\u6d3b\u52a8\u72b6\u6001 \uff08Human Activity Recognition\uff09 \u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7cfb\u7edf \uff08\u66ff\u4ee3PID\u7b49\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\uff09 \u56fe\u50cf\u5904\u7406 \uff08\u5e26\u4e13\u7528\u52a0\u901f\u5668\u7684 MCU\uff09 ... \u4e3a\u4ec0\u4e48\u9700\u8981 NNoM 2014\u5e74\u540e\u7684\u7f51\u7edc\uff0c\u66f4\u9ad8\u6548\uff0c\u4e5f\u66f4\u590d\u6742\u3002 CMSIS-NN \u4e4b\u7c7b\u7684\u5e93\u592a\u5e95\u5c42\uff0c\u9700\u8981\u8bbe\u7f6e\u7684\u53c2\u6570\u4f17\u591a\uff0c\u4e0d\u591f\u7075\u6d3b\uff0c\u53ea\u80fd\u5e94\u7528\u5728\u6bd4\u8f83\u4f20\u7edf\u7684\u5355\u8def\u5f84\u7f51\u7edc\u4e0a\u3002 \u800c\u65b0\u7684\u7f51\u7edc\u503e\u5411\u4e8e\u4ece\u7ed3\u6784\u4e0a\u505a\u4f18\u5316\uff0c\u4e0d\u53ef\u907f\u514d\u5730\u9700\u8981\u5728\u7ed3\u6784\u4e0a\u505a\u8bbe\u8ba1\u548c\u4f18\u5316\u3002 \u590d\u6742\u7684\u7f51\u7edc\u7ed3\u6784\u5bfc\u81f4\u975e\u5e38\u96be\u4ee5\u4f7f\u7528\u4f20\u7edf\u7684\u5e95\u5c42\u5e93\u8fdb\u884c\u90e8\u7f72\u3002 \u6700\u521d\uff0cNNoM \u5728 CMSIS-NN \u4e0a\u5c01\u88c5\u4e86\u4e00\u5c42\u7ed3\u6784\u5c42\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u7684\u7f51\u7edc\u8def\u5f84\u5e76\u4e14\u7b80\u5316\u4e86\u53c2\u6570\u8ba1\u7b97\u3002 \u540e\u6765 NNoM \u6709\u4e86\u81ea\u5df1\u7684\u811a\u672c\u53ef\u4ee5\u4e00\u952e\u751f\u6210\u76f8\u5e94\u7684 C \u6587\u4ef6\uff0c\u66f4\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u4f7f\u7528\u6027\u3002 \u65b0\u7684\u7eaf C \u540e\u7aef\u4e5f\u4f7f\u5f97\u4f7f\u7528 NNoM \u540e\uff0c\u7f51\u7edc\u7684\u90e8\u7f72\u51e0\u4e4e\u6ca1\u6709\u5e73\u53f0\u7684\u9650\u5236\u3002 1.1 \u76ee\u5f55\u7ed3\u6784 nnom \u251c\u2500\u2500\u2500docs \u2502 \u251c\u2500\u2500\u2500figures // \u6587\u6863\u56fe\u7247 \u2502 \u2514\u2500\u2500\u2500*.md // \u6587\u6863 \u251c\u2500\u2500\u2500examples // \u4f8b\u5b50 \u251c\u2500\u2500\u2500inc // \u5934\u6587\u4ef6 \u251c\u2500\u2500\u2500port // \u79fb\u690d\u6587\u4ef6 \u251c\u2500\u2500\u2500scripts // \u811a\u672c\u5de5\u5177\uff0c\u6a21\u578b\u8f6c\u6362\u5de5\u5177 \u251c\u2500\u2500\u2500src // \u6e90\u4ee3\u7801 \u2502 LICENSE // \u8f6f\u4ef6\u5305\u8bb8\u53ef\u8bc1 \u2502 README.md // \u8f6f\u4ef6\u5305\u7b80\u4ecb \u2514\u2500\u2500\u2500SConscript // \u6784\u5efa\u811a\u672c \u5728\u7ebf\u6587\u6863 https://majianjia.github.io/nnom/ 1.2 \u8bb8\u53ef\u8bc1 NNoM \u4f7f\u7528 Apache License 2.0 \u8bb8\u53ef\u8bc1\uff0c\u8be6\u89c1LICENSE\u6587\u4ef6\u3002 1.3 \u4f9d\u8d56 RT-Thread 3.0+ 2. \u83b7\u53d6\u8f6f\u4ef6\u5305 RT-Thread online packages ---> miscellaneous packages ---> [*] NNoM: A Higher-level Nerual Network ... ---> \u63a8\u8350\u9009\u62e9 latest \u7248\u672c\u3002 \u9000\u51fa menuconfig \u540e\uff0c\u9700\u8981\u4f7f\u7528 pkgs --update \u547d\u4ee4\u4e0b\u8f7d\u8f6f\u4ef6\u5305\u3002 3. \u4f7f\u7528\u8f6f\u4ef6\u5305 \u4ece [\u4f8b\u5b50] \u5f00\u59cb\uff0c\u4f8b\u5b50\u4ece\u6d45\u5230\u6df1\uff0c\u9002\u5408\u76f4\u63a5\u786c\u5e72\u7684\u73a9\u5bb6\u3002 \u4ece [\u5728\u7ebf\u6587\u6863] \u5f00\u59cb\uff0c\u9002\u5408\u89c4\u89c4\u77e9\u77e9\u7684\u5927\u4f6c\u3002 3.1 \u80cc\u666f\u77e5\u8bc6 Python 3 \u73af\u5883\uff0c\u63a8\u8350 Anaconda \u795e\u7ecf\u7f51\u7edc\u548c\u673a\u5668\u5b66\u4e60\u7684\u57fa\u672c\u77e5\u8bc6 \u5728 MCU \u4e0a\u505a\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u4f7f\u7528\u8005\u5177\u5907\u4e00\u4e9b\u7b80\u5355\u7684\u673a\u5668\u5b66\u4e60\u7684\u6982\u5ff5\uff0c\u6bd4\u5982\u5377\u79ef\u5c42\uff0c\u5168\u8fde\u63a5\u5c42\uff0c \u548c Keras \u6216\u662f\u5176\u4ed6\u5de5\u5177\u7684\u521d\u6b65\u6280\u5de7\u3002 NNoM \u53ef\u4ee5\u4f7f\u7528\u81ea\u5e26\u7684 python \u811a\u672c\u5f88\u597d\u5730\u914d\u5408 Keras \u6765\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u5230 MCU\u3002\u6240\u4ee5\u63a8\u8350\u4f7f\u7528 Keras \u6765\u5b66\u4e60\u3002 \u5982\u679c\u4f60\u5bf9\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u5b8c\u5168\u964c\u751f\u4e5f\u6ca1\u6709\u5173\u7cfb\uff0c Keras \u53f7\u79f030\u79d2\u5165\u95e8\u3002 3.2 \u4f8b\u5b50 /examples \u4e0b\u6709\u51e0\u4e2a\u4e0d\u540c\u7684\u4f8b\u5b50\u3002 [mnist-simple] \u624b\u5199\u6570\u5b57\u8bc6\u522b (\u8d85\u7ea7\u7b80\u5355\uff0cMsh \u4ea4\u4e92) [uci-inception] \u4f7f\u75286\u8f74\u8fd0\u52a8\u4f20\u611f\u5668\u8fdb\u884c\u4eba\u4f53\u8fd0\u52a8\u8bc6\u522b(Inception \u7ed3\u6784\uff0c CMSIS-NN\u52a0\u901f\uff0c\u4f7f\u7528Y-modem\u53d1\u9001\u6d4b\u8bd5\u6570\u636e\uff0c\u652f\u6301 Msh\u547d\u4ee4\u884c\u4ea4\u4e92) [mnist-densenet] \u624b\u5199\u6570\u5b57\u8bc6\u522b (DenseNet \u7ed3\u6784) \u66f4\u591a\u4f8b\u5b50\u6b63\u5728\u8def\u4e0a... \u5173\u952e\u8bcd\u8bc6\u522b\u4f8b\u5b50(KeyWord Spotting) 3.3 \u4f18\u5316 NNoM \u9ed8\u8ba4\u4f7f\u7528\u7eaf C \u540e\u7aef\uff0c \u540c\u65f6\u652f\u6301 CMSIS-NN/DSP \u540e\u7aef\u3002\u9009\u62e9 CMSIS \u540e\u7aef\u540e\uff0c\u4f1a\u67095\u500d\u5de6\u53f3\u7684\u6027\u80fd\u63d0\u5347. \u5f00\u542f\u4f18\u5316\u7684\u65b9\u5f0f\u53ef\u4ee5\u67e5\u770b Porting and optimising Guide 4. \u8054\u7cfb\u65b9\u5f0f \u963f\u5609 \uff08Jianjia Ma\uff09 majianjia@live.com \u611f\u8c22 Wearable Bio-Robotics Group (WBR), Loughborough University","title":"RT-Thread"},{"location":"rt-thread_guide/#nnom","text":"NNoM (Neural Network on Microcontroller)","title":"NNoM"},{"location":"rt-thread_guide/#1","text":"NNoM\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u4e86\u795e\u7ecf\u7f51\u7edc\u5728 MCU \u4e0a\u8fd0\u884c\u7684\u6846\u67b6\u3002 NNoM \u662f\u4e00\u4e2a\u5b9a\u70b9\u795e\u7ecf\u7f51\u7edc\u5e93\uff0c \u73b0\u5728\u652f\u6301 8-bit\u5b9a\u70b9\u683c\u5f0f\u3002 \u5f53\u524d NNoM \u652f\u6301\u6570\u5341\u79cd\u64cd\u4f5c\uff0c\u5377\u79ef\uff0c\u6c60\u5316\uff0c\u6fc0\u6d3b\uff0c\u77e9\u9635\u8ba1\u7b97\u7b49\u7b49\u3002 \u6b64\u5916\u8fd8\u63d0\u4f9b\u591a\u79cd\u5728\u7ebf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u65f6\u95f4\u7edf\u8ba1\uff0c\u5185\u5b58\u7edf\u8ba1\uff0cTop-K \u51c6\u786e\u5ea6\uff0cConfusion Matrix \u7b49\u7b49\u3002 \u5b83\u62e5\u6709\u4ee5\u4e0b\u4f18\u70b9\uff1a \u5feb\u901f\u5f00\u53d1 \uff1a \u7535\u8111\u4e0a\u8bad\u7ec3\u7684 Keras \u6a21\u578b\u76f4\u63a5\u8f6c\u6362\u6210 C \u6587\u4ef6\u3002 \u9ed8\u8ba4\u7eafC\u540e\u7aef \uff1a \u79fb\u690d\u548c\u4f7f\u7528\u7545\u901a\u65e0\u963b\uff0c\u652f\u630132/64bit\uff0cMCU/PC\u3002 \u652f\u6301\u590d\u6742\u7ed3\u6784 \uff1a \u652f\u6301\u591a\u79cd\u590d\u6742\u7684\u7f51\u7edc\u6a21\u578b\u7ed3\u6784\u3002 \u5b8c\u5584\u7684\u6587\u6863 \uff1a \u62e5\u6709 API \u6587\u6863\uff0c\u5165\u95e8\u6307\u5357\uff0c\u4f18\u5316\u6307\u5357\u3002 \u5165\u95e8\u7b80\u5355 \uff1a \u591a\u4e2a\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u4f8b\u5b50\uff0c\u5b8c\u5168\u5f00\u6e90\u3002 MCU \u4e0a\u7684\u795e\u7ecf\u7f51\u7edc\u80fd\u505a\u4ec0\u4e48\uff1f \u8bed\u97f3\u5173\u952e\u8bcd\u8bc6\u522b \uff08KeyWord Spotting\uff09 \u4f7f\u7528\u8fd0\u52a8\u4f20\u611f\u5668\u8bc6\u522b\u6d3b\u52a8\u72b6\u6001 \uff08Human Activity Recognition\uff09 \u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7cfb\u7edf \uff08\u66ff\u4ee3PID\u7b49\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\uff09 \u56fe\u50cf\u5904\u7406 \uff08\u5e26\u4e13\u7528\u52a0\u901f\u5668\u7684 MCU\uff09 ... \u4e3a\u4ec0\u4e48\u9700\u8981 NNoM 2014\u5e74\u540e\u7684\u7f51\u7edc\uff0c\u66f4\u9ad8\u6548\uff0c\u4e5f\u66f4\u590d\u6742\u3002 CMSIS-NN \u4e4b\u7c7b\u7684\u5e93\u592a\u5e95\u5c42\uff0c\u9700\u8981\u8bbe\u7f6e\u7684\u53c2\u6570\u4f17\u591a\uff0c\u4e0d\u591f\u7075\u6d3b\uff0c\u53ea\u80fd\u5e94\u7528\u5728\u6bd4\u8f83\u4f20\u7edf\u7684\u5355\u8def\u5f84\u7f51\u7edc\u4e0a\u3002 \u800c\u65b0\u7684\u7f51\u7edc\u503e\u5411\u4e8e\u4ece\u7ed3\u6784\u4e0a\u505a\u4f18\u5316\uff0c\u4e0d\u53ef\u907f\u514d\u5730\u9700\u8981\u5728\u7ed3\u6784\u4e0a\u505a\u8bbe\u8ba1\u548c\u4f18\u5316\u3002 \u590d\u6742\u7684\u7f51\u7edc\u7ed3\u6784\u5bfc\u81f4\u975e\u5e38\u96be\u4ee5\u4f7f\u7528\u4f20\u7edf\u7684\u5e95\u5c42\u5e93\u8fdb\u884c\u90e8\u7f72\u3002 \u6700\u521d\uff0cNNoM \u5728 CMSIS-NN \u4e0a\u5c01\u88c5\u4e86\u4e00\u5c42\u7ed3\u6784\u5c42\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u7684\u7f51\u7edc\u8def\u5f84\u5e76\u4e14\u7b80\u5316\u4e86\u53c2\u6570\u8ba1\u7b97\u3002 \u540e\u6765 NNoM \u6709\u4e86\u81ea\u5df1\u7684\u811a\u672c\u53ef\u4ee5\u4e00\u952e\u751f\u6210\u76f8\u5e94\u7684 C \u6587\u4ef6\uff0c\u66f4\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u4f7f\u7528\u6027\u3002 \u65b0\u7684\u7eaf C \u540e\u7aef\u4e5f\u4f7f\u5f97\u4f7f\u7528 NNoM \u540e\uff0c\u7f51\u7edc\u7684\u90e8\u7f72\u51e0\u4e4e\u6ca1\u6709\u5e73\u53f0\u7684\u9650\u5236\u3002","title":"1. \u7b80\u4ecb"},{"location":"rt-thread_guide/#11","text":"nnom \u251c\u2500\u2500\u2500docs \u2502 \u251c\u2500\u2500\u2500figures // \u6587\u6863\u56fe\u7247 \u2502 \u2514\u2500\u2500\u2500*.md // \u6587\u6863 \u251c\u2500\u2500\u2500examples // \u4f8b\u5b50 \u251c\u2500\u2500\u2500inc // \u5934\u6587\u4ef6 \u251c\u2500\u2500\u2500port // \u79fb\u690d\u6587\u4ef6 \u251c\u2500\u2500\u2500scripts // \u811a\u672c\u5de5\u5177\uff0c\u6a21\u578b\u8f6c\u6362\u5de5\u5177 \u251c\u2500\u2500\u2500src // \u6e90\u4ee3\u7801 \u2502 LICENSE // \u8f6f\u4ef6\u5305\u8bb8\u53ef\u8bc1 \u2502 README.md // \u8f6f\u4ef6\u5305\u7b80\u4ecb \u2514\u2500\u2500\u2500SConscript // \u6784\u5efa\u811a\u672c \u5728\u7ebf\u6587\u6863 https://majianjia.github.io/nnom/","title":"1.1 \u76ee\u5f55\u7ed3\u6784"},{"location":"rt-thread_guide/#12","text":"NNoM \u4f7f\u7528 Apache License 2.0 \u8bb8\u53ef\u8bc1\uff0c\u8be6\u89c1LICENSE\u6587\u4ef6\u3002","title":"1.2 \u8bb8\u53ef\u8bc1"},{"location":"rt-thread_guide/#13","text":"RT-Thread 3.0+","title":"1.3 \u4f9d\u8d56"},{"location":"rt-thread_guide/#2","text":"RT-Thread online packages ---> miscellaneous packages ---> [*] NNoM: A Higher-level Nerual Network ... ---> \u63a8\u8350\u9009\u62e9 latest \u7248\u672c\u3002 \u9000\u51fa menuconfig \u540e\uff0c\u9700\u8981\u4f7f\u7528 pkgs --update \u547d\u4ee4\u4e0b\u8f7d\u8f6f\u4ef6\u5305\u3002","title":"2. \u83b7\u53d6\u8f6f\u4ef6\u5305"},{"location":"rt-thread_guide/#3","text":"\u4ece [\u4f8b\u5b50] \u5f00\u59cb\uff0c\u4f8b\u5b50\u4ece\u6d45\u5230\u6df1\uff0c\u9002\u5408\u76f4\u63a5\u786c\u5e72\u7684\u73a9\u5bb6\u3002 \u4ece [\u5728\u7ebf\u6587\u6863] \u5f00\u59cb\uff0c\u9002\u5408\u89c4\u89c4\u77e9\u77e9\u7684\u5927\u4f6c\u3002","title":"3. \u4f7f\u7528\u8f6f\u4ef6\u5305"},{"location":"rt-thread_guide/#31","text":"Python 3 \u73af\u5883\uff0c\u63a8\u8350 Anaconda \u795e\u7ecf\u7f51\u7edc\u548c\u673a\u5668\u5b66\u4e60\u7684\u57fa\u672c\u77e5\u8bc6 \u5728 MCU \u4e0a\u505a\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u4f7f\u7528\u8005\u5177\u5907\u4e00\u4e9b\u7b80\u5355\u7684\u673a\u5668\u5b66\u4e60\u7684\u6982\u5ff5\uff0c\u6bd4\u5982\u5377\u79ef\u5c42\uff0c\u5168\u8fde\u63a5\u5c42\uff0c \u548c Keras \u6216\u662f\u5176\u4ed6\u5de5\u5177\u7684\u521d\u6b65\u6280\u5de7\u3002 NNoM \u53ef\u4ee5\u4f7f\u7528\u81ea\u5e26\u7684 python \u811a\u672c\u5f88\u597d\u5730\u914d\u5408 Keras \u6765\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u5230 MCU\u3002\u6240\u4ee5\u63a8\u8350\u4f7f\u7528 Keras \u6765\u5b66\u4e60\u3002 \u5982\u679c\u4f60\u5bf9\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u5b8c\u5168\u964c\u751f\u4e5f\u6ca1\u6709\u5173\u7cfb\uff0c Keras \u53f7\u79f030\u79d2\u5165\u95e8\u3002","title":"3.1 \u80cc\u666f\u77e5\u8bc6"},{"location":"rt-thread_guide/#32","text":"/examples \u4e0b\u6709\u51e0\u4e2a\u4e0d\u540c\u7684\u4f8b\u5b50\u3002 [mnist-simple] \u624b\u5199\u6570\u5b57\u8bc6\u522b (\u8d85\u7ea7\u7b80\u5355\uff0cMsh \u4ea4\u4e92) [uci-inception] \u4f7f\u75286\u8f74\u8fd0\u52a8\u4f20\u611f\u5668\u8fdb\u884c\u4eba\u4f53\u8fd0\u52a8\u8bc6\u522b(Inception \u7ed3\u6784\uff0c CMSIS-NN\u52a0\u901f\uff0c\u4f7f\u7528Y-modem\u53d1\u9001\u6d4b\u8bd5\u6570\u636e\uff0c\u652f\u6301 Msh\u547d\u4ee4\u884c\u4ea4\u4e92) [mnist-densenet] \u624b\u5199\u6570\u5b57\u8bc6\u522b (DenseNet \u7ed3\u6784) \u66f4\u591a\u4f8b\u5b50\u6b63\u5728\u8def\u4e0a... \u5173\u952e\u8bcd\u8bc6\u522b\u4f8b\u5b50(KeyWord Spotting)","title":"3.2 \u4f8b\u5b50"},{"location":"rt-thread_guide/#33","text":"NNoM \u9ed8\u8ba4\u4f7f\u7528\u7eaf C \u540e\u7aef\uff0c \u540c\u65f6\u652f\u6301 CMSIS-NN/DSP \u540e\u7aef\u3002\u9009\u62e9 CMSIS \u540e\u7aef\u540e\uff0c\u4f1a\u67095\u500d\u5de6\u53f3\u7684\u6027\u80fd\u63d0\u5347. \u5f00\u542f\u4f18\u5316\u7684\u65b9\u5f0f\u53ef\u4ee5\u67e5\u770b Porting and optimising Guide","title":"3.3 \u4f18\u5316"},{"location":"rt-thread_guide/#4","text":"\u963f\u5609 \uff08Jianjia Ma\uff09 majianjia@live.com \u611f\u8c22 Wearable Bio-Robotics Group (WBR), Loughborough University","title":"4. \u8054\u7cfb\u65b9\u5f0f"}]}