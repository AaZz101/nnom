<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Old Readme - NNoM Documentation</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Old Readme";
    var mkdocs_page_input_path = "legacy_README.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-141541198-1', 'majianjia.github.io/nnom');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> NNoM Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Overview</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Guides</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../guide_5_min_to_nnom/">5 min to NNoM</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../guide_development/">Development Guide</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../Porting_and_Optimisation_Guide/">Porting and Optimisation Guide</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">APIs</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../api_nnom_utils/">Utils</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../api_nnom/">Utils (>0.4.0)</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../api_model/">Model</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../api_construction/">Construction Methods</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../api_properties/">Properties</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../api_tensor/">Tensors</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../api_layers/">Core Layers</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../api_pooling/">Pooling Layers</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../api_activations/">Activations</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../api_merge/">Merges Layers</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../api_evaluation/">Evaluation Methods</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Legacy Guide</span></p>
                <ul class="current">
                    <li class="toctree-l1"><a class="reference internal" href="../A_Temporary_Guide_to_NNoM/">A Temporary Guide</a>
                    </li>
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Old Readme</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#dependencies">Dependencies</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#why-nnom">Why NNoM?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#functional-model">Functional Model</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#available-operations">Available Operations</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#memory-requirements">Memory requirements</a>
    </li>
    </ul>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Chinese (中文)</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../rt-thread_guide/">RT-Thread</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../example_mnist_simple_cn/">MNIST-Simple</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">NNoM Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Legacy Guide &raquo;</li>
        
      
    
    <li>Old Readme</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/majianjia/nnom/edit/master/docs/legacy_README.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="neural-network-on-microcontroller-nnom">Neural Network on Microcontroller (NNoM)</h1>
<p><a href="https://travis-ci.org/majianjia/nnom"><img alt="Build Status" src="https://travis-ci.org/majianjia/nnom.svg?branch=master" /></a></p>
<p>NNoM is a higher-level layer-based Neural Network library specifically for microcontrollers. </p>
<p>NNoM is released under LGPL-V3.0, please check the license file for detail. </p>
<p><a href="../A_Temporary_Guide_to_NNoM/">The Temporary Guide</a></p>
<p><a href="docs/Porting_and_Optimisation_Guide.md">Porting and Optimising Guide</a></p>
<h2 id="dependencies">Dependencies</h2>
<p>NNoM now use the local pure C backend implementation by default. Thus, there is no special dependency needed. </p>
<blockquote>
<p>However, you can still select <a href="https://github.com/ARM-software/CMSIS_5/tree/develop/CMSIS/NN">CMSIS-NN/DSP</a> as the backend for about 5x performance with ARM-Cortex-M4/7/33/35P. </p>
<p>Simply <code>#define NNOM_USING_CMSIS_NN</code> in <code>nnom_port.h</code> and include CMSIS-NN in your project. 
Check <a href="../Porting_and_Optimisation_Guide/">Porting and Optimising Guide</a> for detail. </p>
</blockquote>
<h2 id="why-nnom">Why NNoM?</h2>
<p>The aims of NNoM is to provide a light-weight, user-friendly and flexible interface for fast deploying.</p>
<p>Nowadays, neural networks are <strong>wider</strong>, <strong>deeper</strong>, and <strong>denser</strong>.
<img alt="" src="docs/figures/nnom_wdd.png" /></p>
<blockquote>
<p>[1] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... &amp; Rabinovich, A. (2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).</p>
<p>[2] He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</p>
<p>[3] Huang, G., Liu, Z., Van Der Maaten, L., &amp; Weinberger, K. Q. (2017). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4700-4708).</p>
</blockquote>
<p><strong>If you would like to try those more up-to-date, decent and complex structures on MCU</strong> </p>
<p><strong>NNoM can help you to build them with only a few lines of C codes</strong>, same as you did with Python in <a href="https://keras.io/"><strong>Keras</strong></a></p>
<p>Inception example: <a href="../examples/uci-inception">uci-inception</a></p>
<p>DenseNet example: <a href="../examples/mnist-densenet">mnist-densenet</a></p>
<p>A simple example:</p>
<pre><code class="c">#define INPUT_HIGHT 1
#define INPUT_WIDTH 128
#define INPUT_CH 9

new_model(&amp;model);
model.add(&amp;model, Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), qformat(7, 0), input_buf));
model.add(&amp;model, Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &amp;c1_w, &amp;c1_b)); // c1_w, c1_b are weights and bias
model.add(&amp;model, ReLU());
model.add(&amp;model, MaxPool(kernel(1, 4), stride(1, 4), PADDING_VALID));
model.add(&amp;model, Dense(128, &amp;ip1_w, &amp;ip1_b));
model.add(&amp;model, ReLU());
model.add(&amp;model, Dense(6, &amp;ip2_w, &amp;ip2_b));
model.add(&amp;model, Softmax());
model.add(&amp;model, Output(shape(6, 1, 1), qformat(7, 0), output_buf));
sequencial_compile(&amp;model);

while(1){
    feed_input(&amp;input_buf)
    model_run(&amp;model);
}
</code></pre>

<p>It supports both sequential and functional API. </p>
<p>The above codes shows how a sequential model is built, compiled, and ran. </p>
<h2 id="functional-model">Functional Model</h2>
<p>Functional APIs are much more flexible. </p>
<p>It allows developer to build complex structures in MCU, such as <a href="https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf">Inception</a> and <a href="https://arxiv.org/abs/1512.03385">ResNet</a>. </p>
<p>The below codes shows an Inception structures with 3 parallel subpathes.</p>
<pre><code class="c">#define INPUT_HIGHT 1
#define INPUT_WIDTH 128
#define INPUT_CH 9

nnom_layer_t *input_layer, *x, *x1, *x2, *x3;

input_layer = Input(shape(INPUT_HIGHT, INPUT_WIDTH, INPUT_CH), qformat(7, 0), input_buf);

// conv2d
x = model.hook(Conv2D(16, kernel(1, 9), stride(1, 2), PADDING_SAME, &amp;c1_w, &amp;c1_b), input_layer);
x = model.active(act_relu(), x);
x = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x);

// parallel Inception 1 - conv2d 
x1 = model.hook(Conv2D(16, kernel(1, 5), stride(1, 1), PADDING_SAME, &amp;c2_w, &amp;c2_b), x); // hooked to x
x1 = model.active(act_relu(), x1);
x1 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x1);

//  parallel Inception 2 - conv2d 
x2 = model.hook(Conv2D(16, kernel(1, 3), stride(1, 1), PADDING_SAME, &amp;c3_w, &amp;c3_b), x); // hooked to x
x2 = model.active(act_relu(), x2);
x2 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x2);

//  parallel Inception 3 - maxpool 
x3 = model.hook(MaxPool(kernel(1, 2), stride(1, 2), PADDING_VALID), x); // hooked to x

// concatenate 3 parallel. 
x = model.mergex(Concat(-1), 3, x1, x2, x3); // new merge API. 

// flatten &amp; dense
x = model.hook(Flatten(), x);
x = model.hook(Dense(128, &amp;ip1_w, &amp;ip1_b), x);
x = model.active(act_relu(), x);
x = model.hook(Dense(6, &amp;ip2_w, &amp;ip2_b), x);
x = model.hook(Softmax(), x);
x = model.hook(Output(shape(6,1,1), qformat(7, 0), output_buf), x);

// compile and check
model_compile(&amp;model, input_layer, x);

while(1){
    feed_input(&amp;input_buf)
    model_run(&amp;model);
}
</code></pre>

<p>Please check <a href="../A_Temporary_Guide_to_NNoM/">A brief manual</a> for more API details. </p>
<h2 id="available-operations">Available Operations</h2>
<p><strong>Layers</strong></p>
<table>
<thead>
<tr>
<th>Layers</th>
<th>Status</th>
<th>Layer API</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>Convolution</td>
<td>Beta</td>
<td>Conv2D()</td>
<td>Support 1/2D</td>
</tr>
<tr>
<td>Depthwise Conv</td>
<td>Beta</td>
<td>DW_Conv2D()</td>
<td>Support 1/2D</td>
</tr>
<tr>
<td>Fully-connected</td>
<td>Beta</td>
<td>Dense()</td>
<td></td>
</tr>
<tr>
<td>Lambda</td>
<td>Alpha</td>
<td>Lambda()</td>
<td>single input / single output anonymous operation</td>
</tr>
<tr>
<td>Input/Output</td>
<td>Beta</td>
<td>Input()/Output()</td>
<td></td>
</tr>
<tr>
<td>Recurrent NN</td>
<td>Under Dev.</td>
<td>RNN()</td>
<td>Under Developpment</td>
</tr>
<tr>
<td>Simple RNN</td>
<td>Under Dev.</td>
<td>SimpleCell()</td>
<td>Under Developpment</td>
</tr>
<tr>
<td>Gated Recurrent Network (GRU)</td>
<td>Under Dev.</td>
<td>GRUCell()</td>
<td>Under Developpment</td>
</tr>
<tr>
<td>Flatten</td>
<td>Beta</td>
<td>Flatten()</td>
<td></td>
</tr>
<tr>
<td>SoftMax</td>
<td>Beta</td>
<td>SoftMax()</td>
<td>Softmax only has layer API</td>
</tr>
<tr>
<td>Activation</td>
<td>Beta</td>
<td>Activation()</td>
<td>A layer instance for activation</td>
</tr>
</tbody>
</table>
<p><strong>Activations</strong></p>
<p>Activation can be used by itself as layer, or can be attached to the previous layer as <a href="../A_Temporary_Guide_to_NNoM/#addictionlly-activation-apis">"actail"</a> to reduce memory cost.</p>
<table>
<thead>
<tr>
<th>Actrivation</th>
<th>Status</th>
<th>Layer API</th>
<th>Activation API</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReLU</td>
<td>Beta</td>
<td>ReLU()</td>
<td>act_relu()</td>
<td></td>
</tr>
<tr>
<td>TanH</td>
<td>Beta</td>
<td>TanH()</td>
<td>act_tanh()</td>
<td></td>
</tr>
<tr>
<td>Sigmoid</td>
<td>Beta</td>
<td>Sigmoid()</td>
<td>act_sigmoid()</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Pooling Layers</strong></p>
<table>
<thead>
<tr>
<th>Pooling</th>
<th>Status</th>
<th>Layer API</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>Max Pooling</td>
<td>Beta</td>
<td>MaxPool()</td>
<td></td>
</tr>
<tr>
<td>Average Pooling</td>
<td>Beta</td>
<td>AvgPool()</td>
<td></td>
</tr>
<tr>
<td>Sum Pooling</td>
<td>Beta</td>
<td>SumPool()</td>
<td></td>
</tr>
<tr>
<td>Global Max Pooling</td>
<td>Beta</td>
<td>GlobalMaxPool()</td>
<td></td>
</tr>
<tr>
<td>Global Average Pooling</td>
<td>Beta</td>
<td>GlobalAvgPool()</td>
<td></td>
</tr>
<tr>
<td>Global Sum Pooling</td>
<td>Beta</td>
<td>GlobalSumPool()</td>
<td>A better alternative to Global average pooling in MCU before Softmax</td>
</tr>
<tr>
<td>Up Sampling</td>
<td>Beta</td>
<td>UpSample()</td>
<td>or Unpooling</td>
</tr>
</tbody>
</table>
<p><strong>Matrix Operations Layers</strong></p>
<table>
<thead>
<tr>
<th>Matrix</th>
<th>Status</th>
<th>Layer API</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multiple</td>
<td>Beta</td>
<td>Mult()</td>
<td></td>
</tr>
<tr>
<td>Addition</td>
<td>Beta</td>
<td>Add()</td>
<td></td>
</tr>
<tr>
<td>Substraction</td>
<td>Beta</td>
<td>Sub()</td>
<td></td>
</tr>
<tr>
<td>Dot</td>
<td>Under Dev.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="memory-requirements">Memory requirements</h2>
<p>NNoM requires dynamic memory allocating during model building and compiling. </p>
<p>No memory allocating in running the model. </p>
<p>RAM requirement is about 100 to 200 bytes per layer for NNoM instance, plus the maximum data buf cost.</p>
<blockquote>
<p>The sequential example above includes 9 layer instances. So, the memory cost for instances is 130 x 9 = 1170 Bytes.</p>
<p>The maximum data buffer is in the convolutional layer.</p>
<p>It costs 1 x 128 x 9 = 1152 Bytes as input, 1 x 64 x 16 = 1024 Bytes as output, and 576 Bytes as intermedium buffer (img2col). </p>
<p>The total memory cost of the model is around 1170 (instance) + (1152+1024+576)(network) = ~3922 Bytes. </p>
</blockquote>
<p>In NNoM, we dont analysis memory cost manually like above. </p>
<p>Memory analysis will be printed when compiling the model.  </p>
<h1 id="deploying-keras-model-to-nnom">Deploying Keras model to NNoM</h1>
<p>You can now use <a href="scripts/nnom_utils.py#L284">generate_model(model, x_data)</a> to deploy your model to <code>weights.h</code> directly. </p>
<p>Then simply call <code>nnom_model_create()</code> in your <code>main()</code> to compile the model on your platform.</p>
<p>Please check <a href="docs/A_Temporary_Guide_to_NNoM.md">A brief manual</a>
and <a href="examples/mnist-densenet">MNIST-DenseNet</a> example.</p>
<h1 id="porting">Porting</h1>
<p>Simply modify the <a href="port/nnom_port.h">nnom_port.h</a> according to your platform.</p>
<blockquote>
<p>To optimise for ARM chips, it is required to include the <a href="https://github.com/ARM-software/CMSIS_5/tree/develop/CMSIS/NN">CMSIS-NN lib</a> in your projects.
Then define <code>#define NNOM_USE_CMSIS_NN</code> in the <code>nnom_port.h</code></p>
</blockquote>
<p>Please check <a href="../Porting_and_Optimisation_Guide/">Porting and Optimising Guide</a> for detial. </p>
<h1 id="current-critical-limitations">Current Critical Limitations</h1>
<ul>
<li>Support 8-bit quantisation only. </li>
</ul>
<h1 id="todo">TODO</h1>
<ul>
<li>Support RNN types layers.</li>
<li>~~Support mutiple Q-formats~~（Done, by @parai）</li>
<li>~~support memory releasing.~~（Done）</li>
</ul>
<h1 id="contacts">Contacts</h1>
<p>Jianjia Ma</p>
<p>J.Ma2@lboro.ac.uk or majianjia@live.com</p>
<h1 id="citation-required">Citation Required</h1>
<p>Please contact us using above details. </p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../rt-thread_guide/" class="btn btn-neutral float-right" title="RT-Thread">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../A_Temporary_Guide_to_NNoM/" class="btn btn-neutral" title="A Temporary Guide"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/majianjia/nnom/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../A_Temporary_Guide_to_NNoM/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../rt-thread_guide/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
